#+TITLE:index3 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----
* Data Management
** Exemplar Data Management Policies
*** NCEPH
~/Dropbox/projects/DataManagementPolicy/ 
- References/NCEPH-Handbook
*** LTERN/SuperSites
~/projects/asn-ltern.bitbucket.org/data_portal_project_plan/
*** UCRH
~/Dropbox/projects/air_pollution_ucrh/
*** Reproducible Research Pipeline Template
#+begin_src R :session *R* :tangle no :exports none :eval yes
  # This is a script to build a pipeline template, and accompany the github repository
  #setwd("~/tools/disentangle")
  #devtools::load_all()
  #setwd("~/tools/")
  #makeProject::makeProject("ReproducibleResearchPipelineTemplate")
  makeProjectBigger("ReproducibleResearchPipelineTemplate", "~/tools", force = T)
  #setwd(projdir)
  #matrix(dir())
  
#+end_src

#+RESULTS:

** DEPRECATED swish-dmp diagram, see projects/swish-dmp/report_
this is built for the swish ideas
*** COMMENT go
#+name:go
#+begin_src R :session *R* :tangle no :exports none :eval yes
#### name:go ####
system("pdflatex swish-dmp-curves.tex")
system("pdflatex swish-dmp-curves.tex")
browseURL("swish-dmp-curves.pdf")
#+end_src

#+RESULTS: go
: 0

*** COMMENT swish-dmp
**** header
#+name:swish-dmp
#+begin_src latex :tangle swish-dmp.tex :exports none :eval no
  \documentclass{article}
  \usepackage{hyperref}
  \usepackage{tikz}
  \usetikzlibrary{calc}
  
  \usepackage{tikz}
  %------------------%
  \makeatletter
  \newcount\dirtree@lvl
  \newcount\dirtree@plvl
  \newcount\dirtree@clvl
  \def\dirtree@growth{%
    \ifnum\tikznumberofcurrentchild=1\relax
    \global\advance\dirtree@plvl by 1
    \expandafter\xdef\csname dirtree@p@\the\dirtree@plvl\endcsname{\the\dirtree@lvl}
    \fi
    \global\advance\dirtree@lvl by 1\relax
    \dirtree@clvl=\dirtree@lvl
    \advance\dirtree@clvl by -\csname dirtree@p@\the\dirtree@plvl\endcsname
    \pgf@xa=0.5cm\relax % change the length to your needs
    \pgf@ya=-0.75cm\relax % change the length to your needs
    \pgf@ya=\dirtree@clvl\pgf@ya
    \pgftransformshift{\pgfqpoint{\the\pgf@xa}{\the\pgf@ya}}%
    \ifnum\tikznumberofcurrentchild=\tikznumberofchildren
    \global\advance\dirtree@plvl by -1
    \fi
  }
  \tikzset{ %definition of a new style "dirtree"
    dirtree/.style={
      growth function=\dirtree@growth,
      every node/.style={anchor=north},
      every child node/.style={anchor=west},
      edge from parent path={(\tikzparentnode\tikzparentanchor) |- (\tikzchildnode\tikzchildanchor)}
    }
  }
  \makeatother
  
  
  \begin{document}
  \tikzset{
      hyperlink node/.style={
          alias=sourcenode,
          append after command={
              let     \p1 = (sourcenode.north west),
                  \p2=(sourcenode.south east),
                  \n1={\x2-\x1},
                  \n2={\y1-\y2} in
              node [inner sep=0pt, outer sep=0pt,anchor=north west,at=(\p1)] {\hyperlink{#1}{\phantom{\rule{\n1}{\n2}}}}
                      %xelatex needs \XeTeXLinkBox, won't create a link unless it
                      %finds text --- rules don't work without \XeTeXLinkBox.
                      %Still builds correctly with pdflatex and lualatex
          }
      }
  }
  
  %\hypertarget{pageone}{Page One}
  %}
  
  
  %\tikz \node [draw, inner sep=2ex,hyperlink node=pagetwo] {Go to Page Two};
  %\tikz \node (author) at (-2.5,4.1) [draw=black!50,dashed,rectangle,fill=green!20,hyperlink node=pagetwo]{Author}; 
  
  %\tikz \node (reader) at (-2.5, -2.0) [draw=black!50,dashed,rectangle,fill=green!20,hyperlink node=pagetwo] {Go to Page Three};
  
  %     \makebox[.4\textwidth][r]{
  
  %    \makebox[.4\textwidth][l]{
#+end_src
**** COMMENT DEPRECATED main computer

#+begin_src latex :tangle swish-dmp.tex :exports none :eval no  
  \begin{tikzpicture}[dirtree] % it's what we defined above
  
  \node [draw=black!50,dashed,rectangle,fill=green!20]{\hyperref[dmp]{* main computer} }
      child { node {\hyperref[dmp]{Data Management Plan} }}
      child { node {\hyperref[datinv]{Data Inventory} }}
      child { node {\hyperref[install]{** Project1} }
          child { node {\hyperref[linux]{README}} }
          child { node {\hyperref[proj]{project management}} }
          child { node {\hyperref[mac]{*** dataset1}} 
              child { node {\hyperref[mac2]{**** workplan and protocol}} }            
              child { node {\hyperref[mac3]{worklog}} }
              child { node {\hyperref[workflow]{workflow}} }
              child { node {\hyperref[dataprov]{**** entities1 data provided}} }
              child { node {\hyperref[dataprov]{**** entities2 data derived}} 
                child { node {\hyperref[mac]{files}}} 
                child { node {\hyperref[mac2]{versions}}}                
                 }                                      
              child { node {\hyperref[mac2]{**** results}} 
                child { node {YYYY-MM-DD}}
                 }            
              child { node {\hyperref[mac3]{reports}} }
  }
          child { node {\hyperref[win]{dataset2}} }
      }
      child {node {\hyperref[trbl-shoot]{Project2}}
          child {node {\hyperref[caseX]{dataset3}}}
          child {node {\hyperref[caseY]{dataset4}}}
      }
      child {node {\hyperref[start]{working\_user}}
          child { node {\hyperref[caseA]{do A}} }
          child { node {\hyperref[caseB]{do B}} }
      };
      % I've put the external resources to the end:
%      child {node {Versions}
%          child { node {\href{file:sanitize_bib_table.pdf}{Backups}} }% works only, if "manual.pdf" is in
                                                         % the same directory as the compiled
                                                         % version of this document
  %        child { node {\href{http://www.google.com/}{Version Control}} }
%      };
  \end{tikzpicture}
  \hspace{0.1cm}
#+end_src
**** COMMENT auxiliary resources
#+begin_src latex :tangle swish-dmp.tex :exports none :eval no  :padline no
  \begin{tikzpicture}[
                outpt/.style={->,blue!80!black,very thick},
                >=stealth,
             every node/.append style={align=center}]
                  \node (aux) at (0,18) [draw=black!50,dashed,rectangle,fill=green!30,hyperlink node=pagetwo]{Auxiliary resources}; 
                  \node (aux) at (0,17) [draw=black!50,dashed,rectangle,fill=yellow!30,hyperlink node=pagetwo]{Dropbox}; 
    
                  \node (measdata) at (-2.4,9) [draw=black!50,dashed,rectangle,fill=orange!30,hyperlink node=proj]{Distributed data}; 
                  \node (hypothesis) at (2,9) [draw=black!50,dashed,rectangle,fill=red!30,hyperlink node=pagethree]{Permissions \\ + citations}; 
                \node (anadata) at (0,7.5) [draw=black!50,dashed,rectangle,fill=orange!30] {\begin{tabular}{@{}c}feedback \end{tabular}};
                \node (anadata3) at (0,0) [draw=black!50,dashed,rectangle,fill=orange!30] {\begin{tabular}{@{}c}Version control\end{tabular}};
  
                \draw[outpt](anadata)--(measdata);
                \draw[outpt](measdata)--(hypothesis);
                \draw[outpt](hypothesis)--(anadata);
  
    
  \end{tikzpicture}
  %}
#+end_src
**** COMMENT main computer
#+begin_src latex :tangle swish-dmp.tex :exports none :eval no  :padline no
  
  \clearpage
  \tikz \node [draw, inner sep=2ex,hyperlink node=pageone] {Main Computer};
  
  \hypertarget{pagetwo}{Page Two}
  \clearpage
  \hypertarget{pagethree}{Page Three}
  
  \clearpage
  \section*{Installation}\label{install}
  
  \subsection*{Data Management Plan}\label{dmp}
  
  \subsection*{Linux}\label{linux}
  Some content.
  
  \subsection*{Mac}\label{mac}
  Some content.
  
  \subsection*{Windows}\label{win}
  Some content.
  \clearpage
  \section*{Get started}\label{start}
  \subsection*{First: Do A}\label{caseA}
  Some content.
  
  \subsection*{Second: Do B}\label{caseB}
   Some content.
  \clearpage
  \section*{Trouble shooting}\label{trbl-shoot}
  \subsection*{If X happens:}\label{caseX}
  Some content.
  
  \subsection*{If Y happens:}\label{caseY}
   Some content.
  
  \subsection*{Data Inventory}\label{datinv}
  \subsection*{Worklog2}\label{mac2}
  
#+end_src
**** worklog
#+begin_src latex :tangle swish-dmp.tex :exports none :eval no  :padline no
  
\subsection*{Worklog}\label{mac3}

Conventions used for writing these entries are:
\begin{quote}
- Names follow this structure [**] [date in ISO 8601] [meeting/notes/results] [from UserName] [Re: topic shortname]
- 'meetings' are for both agenda preparation and also notes of discussion
- 'notes' are such things as emailed information or ad hoc Discovery
- 'results' are entries related to a section of the 'results' folder. 
  That is, this kind of entry is in parallel to the results entry,
  however the log contains a prose description of the experiment,
  whereas the results folder contains scripts etc of all the gory
  details.  
\end{quote}
#+end_src
**** end
#+begin_src latex :tangle swish-dmp.tex :exports none :eval no  :padline no

  \subsection*{Workflow}\label{workflow}
  
  \subsection*{Data Provided}\label{dataprov}
#+end_src
**** proj
#+begin_src latex :tangle swish-dmp.tex :exports none :eval no  :padline no
\clearpage
\subsection*{Project Management}\label{proj}
\hypertarget{proj}{Project Management stuff}
\begin{tikzpicture}[dirtree] % it's what we defined above
  
\node [draw=black!50,dashed,rectangle,fill=green!20]{{project plan} }
      child { node {{proposal} }
          child { node {{approved version: this is the master plan}} }
      }
      child { node {{meetings} }
          child { node {{meeting1}} }
      };

\end{tikzpicture}
#+end_src
**** end
#+begin_src latex :tangle swish-dmp.tex :exports none :eval no  :padline no
  \end{document}  
#+end_src
*** versions
**** this one exlcludes working and versions
\documentclass{article}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{calc}

\usepackage{tikz}
%------------------%
\makeatletter
\newcount\dirtree@lvl
\newcount\dirtree@plvl
\newcount\dirtree@clvl
\def\dirtree@growth{%
  \ifnum\tikznumberofcurrentchild=1\relax
  \global\advance\dirtree@plvl by 1
  \expandafter\xdef\csname dirtree@p@\the\dirtree@plvl\endcsname{\the\dirtree@lvl}
  \fi
  \global\advance\dirtree@lvl by 1\relax
  \dirtree@clvl=\dirtree@lvl
  \advance\dirtree@clvl by -\csname dirtree@p@\the\dirtree@plvl\endcsname
  \pgf@xa=0.5cm\relax % change the length to your needs
  \pgf@ya=-0.75cm\relax % change the length to your needs
  \pgf@ya=\dirtree@clvl\pgf@ya
  \pgftransformshift{\pgfqpoint{\the\pgf@xa}{\the\pgf@ya}}%
  \ifnum\tikznumberofcurrentchild=\tikznumberofchildren
  \global\advance\dirtree@plvl by -1
  \fi
}
\tikzset{ %definition of a new style "dirtree"
  dirtree/.style={
    growth function=\dirtree@growth,
    every node/.style={anchor=north},
    every child node/.style={anchor=west},
    edge from parent path={(\tikzparentnode\tikzparentanchor) |- (\tikzchildnode\tikzchildanchor)}
  }
}
\makeatother


\begin{document}
\tikzset{
    hyperlink node/.style={
        alias=sourcenode,
        append after command={
            let     \p1 = (sourcenode.north west),
                \p2=(sourcenode.south east),
                \n1={\x2-\x1},
                \n2={\y1-\y2} in
            node [inner sep=0pt, outer sep=0pt,anchor=north west,at=(\p1)] {\hyperlink{#1}{\phantom{\rule{\n1}{\n2}}}}
                    %xelatex needs \XeTeXLinkBox, won't create a link unless it
                    %finds text --- rules don't work without \XeTeXLinkBox.
                    %Still builds correctly with pdflatex and lualatex
        }
    }
}

%\hypertarget{pageone}{Page One}
%}


%\tikz \node [draw, inner sep=2ex,hyperlink node=pagetwo] {Go to Page Two};
%\tikz \node (author) at (-2.5,4.1) [draw=black!50,dashed,rectangle,fill=green!20,hyperlink node=pagetwo]{Author}; 

%\tikz \node (reader) at (-2.5, -2.0) [draw=black!50,dashed,rectangle,fill=green!20,hyperlink node=pagetwo] {Go to Page Three};

%     \makebox[.4\textwidth][r]{

%    \makebox[.4\textwidth][l]{
        \resizebox {.65\columnwidth} {!} {
\begin{tikzpicture}[dirtree] % it's what we defined above
  
  \node [draw=black!50,dashed,rectangle,fill=green!20]{\hyperref[dmp]{* main computer} }
      child { node {\hyperref[dmp]{Data Management Plan} }}
      child { node {\hyperref[datinv]{Data Inventory} }}
      child { node {\hyperref[install]{** Project1} }
          child { node {\hyperref[linux]{README}} }
          child { node {\hyperref[proj]{project management}} }
          child { node {\hyperref[mac]{*** dataset1}} 
              child { node {\hyperref[mac2]{**** workplan and protocol}} }            
              child { node {\hyperref[mac3]{worklog}} }
              child { node {\hyperref[workflow]{workflow}} }
              child { node {\hyperref[dataprov]{**** entities1 data provided}} }
              child { node {\hyperref[dataprov]{**** entities2 data derived}} 
                child { node {\hyperref[mac]{files}}} 
                child { node {\hyperref[mac2]{versions}}}                
                 }                                      
              child { node {\hyperref[mac2]{**** results}} 
                child { node {YYYY-MM-DD}}
                 }            
              child { node {\hyperref[mac3]{reports}} }
  }
          child { node {\hyperref[win]{dataset2}} }
      }
      child {node {\hyperref[trbl-shoot]{Project2}}
          child {node {\hyperref[caseX]{dataset3}}}
%          child {node {\hyperref[caseY]{dataset4}}}
      };
%      child {node {\hyperref[start]{working\_user}}
%          child { node {\hyperref[caseA]{do A}} }
%          child { node {\hyperref[caseB]{do B}} }
%      }
      % I've put the external resources to the end:
%      child {node {Versions}
%          child { node {\href{file:sanitize_bib_table.pdf}{Backups}} }% works only, if "manual.pdf" is in
                                                         % the same directory as the compiled
                                                         % version of this document
  %        child { node {\href{http://www.google.com/}{Version Control}} }
%      };

  \end{tikzpicture}
}
  \hspace{0.1cm}
\resizebox {.35\columnwidth} {!} {
\begin{tikzpicture}[
              outpt/.style={->,blue!80!black,very thick},
              >=stealth,
           every node/.append style={align=center}]
                \node (aux) at (0,18) [draw=black!50,dashed,rectangle,fill=green!30,hyperlink node=pagetwo]{Auxiliary resources}; 
                \node (aux) at (0,17) [draw=black!50,dashed,rectangle,fill=yellow!30,hyperlink node=pagetwo]{Dropbox}; 
  
                \node (measdata) at (-2.4,9) [draw=black!50,dashed,rectangle,fill=orange!30,hyperlink node=pagetwo]{Distributed data}; 
                \node (hypothesis) at (2,9) [draw=black!50,dashed,rectangle,fill=red!30,hyperlink node=pagethree]{Permissions \\ + citations}; 
              \node (anadata) at (0,7.5) [draw=black!50,dashed,rectangle,fill=orange!30] {\begin{tabular}{@{}c}feedback \end{tabular}};
              \node (anadata3) at (0,0) [draw=black!50,dashed,rectangle,fill=orange!30] {\begin{tabular}{@{}c}Version control\end{tabular}};

              \draw[outpt](anadata)--(measdata);
              \draw[outpt](measdata)--(hypothesis);
              \draw[outpt](hypothesis)--(anadata);

  
\end{tikzpicture}
}
%}
\clearpage
\tikz \node [draw, inner sep=2ex,hyperlink node=pageone] {Main Computer};

\hypertarget{pagetwo}{Page Two}
\clearpage
\hypertarget{pagethree}{Page Three}

\clearpage
\section*{Installation}\label{install}

\subsection*{Data Management Plan}\label{dmp}

\subsection*{Linux}\label{linux}
Some content.

\subsection*{Mac}\label{mac}
Some content.

\subsection*{Windows}\label{win}
Some content.
\clearpage
\section*{Get started}\label{start}
\subsection*{First: Do A}\label{caseA}
Some content.

\subsection*{Second: Do B}\label{caseB}
 Some content.
\clearpage
\section*{Trouble shooting}\label{trbl-shoot}
\subsection*{If X happens:}\label{caseX}
Some content.

\subsection*{If Y happens:}\label{caseY}
 Some content.

\subsection*{Data Inventory}\label{datinv}
\subsection*{Worklog2}\label{mac2}
\subsection*{Worklog}\label{mac3}

Conventions used for writing these entries are:
\begin{quote}
- Names follow this structure [**] [date in ISO 8601] [meeting/notes/results] [from UserName] [Re: topic shortname]
- 'meetings' are for both agenda preparation and also notes of discussion
- 'notes' are such things as emailed information or ad hoc Discovery
- 'results' are entries related to a section of the 'results' folder. 
  That is, this kind of entry is in parallel to the results entry,
  however the log contains a prose description of the experiment,
  whereas the results folder contains scripts etc of all the gory
  details.  
\end{quote}
\subsection*{Workflow}\label{workflow}

\subsection*{Data Provided}\label{dataprov}
\clearpage
\subsection*{Project Management}\label{proj}

\begin{tikzpicture}[dirtree] % it's what we defined above
  
\node [draw=black!50,dashed,rectangle,fill=green!20]{{project plan} }
      child { node {{proposal} }
          child { node {{approved version: this is the master plan}} }
      }
      child { node {{meetings} }
          child { node {{meeting1}} }
      };

\end{tikzpicture}
\end{document}

*** version with bezier curves
#+begin_src latex :tangle swish-dmp-curves.tex :exports none :eval no  
\documentclass{article}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{calc}

 
\usepackage{tikz}
%------------------%
\makeatletter
\newcount\dirtree@lvl
\newcount\dirtree@plvl
\newcount\dirtree@clvl
\def\dirtree@growth{%
  \ifnum\tikznumberofcurrentchild=1\relax
  \global\advance\dirtree@plvl by 1
  \expandafter\xdef\csname dirtree@p@\the\dirtree@plvl\endcsname{\the\dirtree@lvl}
  \fi
  \global\advance\dirtree@lvl by 1\relax
  \dirtree@clvl=\dirtree@lvl
  \advance\dirtree@clvl by -\csname dirtree@p@\the\dirtree@plvl\endcsname
  \pgf@xa=0.5cm\relax % change the length to your needs
  \pgf@ya=-0.75cm\relax % change the length to your needs
  \pgf@ya=\dirtree@clvl\pgf@ya
  \pgftransformshift{\pgfqpoint{\the\pgf@xa}{\the\pgf@ya}}%
  \ifnum\tikznumberofcurrentchild=\tikznumberofchildren
  \global\advance\dirtree@plvl by -1
  \fi
}
\tikzset{ %definition of a new style "dirtree"
  dirtree/.style={
    growth function=\dirtree@growth,
    every node/.style={anchor=north},
    every child node/.style={anchor=west},
    edge from parent path={(\tikzparentnode\tikzparentanchor) |- (\tikzchildnode\tikzchildanchor)}
  }
}
\makeatother


\begin{document}
\tikzstyle{every picture}+=[remember picture]
\tikzset{
    hyperlink node/.style={
        alias=sourcenode,
        append after command={
            let     \p1 = (sourcenode.north west),
                \p2=(sourcenode.south east),
                \n1={\x2-\x1},
                \n2={\y1-\y2} in
            node [inner sep=0pt, outer sep=0pt,anchor=north west,at=(\p1)] {\hyperlink{#1}{\phantom{\rule{\n1}{\n2}}}}
                    %xelatex needs \XeTeXLinkBox, won't create a link unless it
                    %finds text --- rules don't work without \XeTeXLinkBox.
                    %Still builds correctly with pdflatex and lualatex
        }
    }
}

%\hypertarget{pageone}{Page One}
%}


%\tikz \node [draw, inner sep=2ex,hyperlink node=pagetwo] {Go to Page Two};
%\tikz \node (author) at (-2.5,4.1) [draw=black!50,dashed,rectangle,fill=green!20,hyperlink node=pagetwo]{Author}; 

%\tikz \node (reader) at (-2.5, -2.0) [draw=black!50,dashed,rectangle,fill=green!20,hyperlink node=pagetwo] {Go to Page Three};

%     \makebox[.4\textwidth][r]{

%    \makebox[.4\textwidth][l]{

\begin{tikzpicture}[dirtree] % it's what we defined above
  
  \node [draw=black!50,dashed,rectangle,fill=green!20]{\hyperref[dmp]{* main computer} }
      child { node (dmp1) {\hyperref[dmp]{Data Management Plan} }}
      child { node {\hyperref[datinv]{Data Inventory} }}
      child { node {\hyperref[install]{** Project1} }
          child { node {\hyperref[linux]{README}} }
          child { node {\hyperref[proj]{project management}} }
          child { node {\hyperref[mac]{*** dataset1}} 
              child { node {\hyperref[mac2]{**** workplan and protocol}} }            
              child { node {\hyperref[mac3]{worklog}} }
              child { node {\hyperref[workflow]{workflow}} }
              child { node {\hyperref[dataprov]{**** entities1 data provided}} }
              child { node {\hyperref[dataprov]{**** entities2 data derived}} 
                child { node {\hyperref[mac]{files}}} 
                child { node {\hyperref[mac2]{versions}}}                
                 }                                      
              child { node (res1) {\hyperref[mac2]{**** results}} 
                child { node {YYYY-MM-DD}}
                 }            
              child { node {\hyperref[mac3]{reports}} }
  }
          child { node {\hyperref[win]{dataset2}} }
      }
      child {node {\hyperref[trbl-shoot]{Project2}}
          child {node {\hyperref[caseX]{dataset3}}}
          child {node {\hyperref[caseY]{dataset4}}}
      }
      child {node {\hyperref[start]{working\_user}}
          child { node {\hyperref[caseA]{do A}} }
          child { node {\hyperref[caseB]{do B}} }
      };
      % I've put the external resources to the end:
%      child {node {Versions}
%          child { node {\href{file:sanitize_bib_table.pdf}{Backups}} }% works only, if "manual.pdf" is in
                                                         % the same directory as the compiled
                                                         % version of this document
  %        child { node {\href{http://www.google.com/}{Version Control}} }
%      };
  \end{tikzpicture}
  \hspace{0.1cm}
\begin{tikzpicture}[
              outpt/.style={->,blue!80!black,very thick},
              >=stealth,
           every node/.append style={align=center}]
                \node (aux) at (0,18) [draw=black!50,dashed,rectangle,fill=green!30,hyperlink node=pagetwo]{Auxiliary resources}; 
                \node (aux1) at (0,17) [draw=black!50,dashed,rectangle,fill=yellow!30,hyperlink node=pagetwo]{Dropbox}; 
  
                \node (measdata) at (-2.4,9) [draw=black!50,dashed,rectangle,fill=orange!30,hyperlink node=proj]{Distributed data}; 
                \node (hypothesis) at (2,9) [draw=black!50,dashed,rectangle,fill=red!30,hyperlink node=pagethree]{Permissions \\ + citations}; 
              \node (anadata) at (0,7.5) [draw=black!50,dashed,rectangle,fill=orange!30] {\begin{tabular}{@{}c}feedback \end{tabular}};
              \node (anadata3) at (0,0) [draw=black!50,dashed,rectangle,fill=orange!30] {\begin{tabular}{@{}c}Version control\end{tabular}};

              \draw[outpt](anadata)--(measdata);
              \draw[outpt](measdata)--(hypothesis);
              \draw[outpt](hypothesis)--(anadata);

  
\end{tikzpicture}
%}
\begin{tikzpicture}[overlay]
	\draw (aux1) -- (dmp1);
	\draw (anadata) .. controls (10,7) .. (res1);

\end{tikzpicture}
\clearpage
\tikz \node [draw, inner sep=2ex,hyperlink node=pageone] {Main Computer};

\hypertarget{pagetwo}{Page Two}
\clearpage
\hypertarget{pagethree}{Page Three}

\clearpage
\section*{Installation}\label{install}

\subsection*{Data Management Plan}\label{dmp}

\subsection*{Linux}\label{linux}
Some content.

\subsection*{Mac}\label{mac}
Some content.

\subsection*{Windows}\label{win}
Some content.
\clearpage
\section*{Get started}\label{start}
\subsection*{First: Do A}\label{caseA}
Some content.

\subsection*{Second: Do B}\label{caseB}
 Some content.
\clearpage
\section*{Trouble shooting}\label{trbl-shoot}
\subsection*{If X happens:}\label{caseX}
Some content.

\subsection*{If Y happens:}\label{caseY}
 Some content.

\subsection*{Data Inventory}\label{datinv}
\subsection*{Worklog2}\label{mac2}
\subsection*{Worklog}\label{mac3}

Conventions used for writing these entries are:
\begin{quote}
- Names follow this structure [**] [date in ISO 8601] [meeting/notes/results] [from UserName] [Re: topic shortname]
- 'meetings' are for both agenda preparation and also notes of discussion
- 'notes' are such things as emailed information or ad hoc Discovery
- 'results' are entries related to a section of the 'results' folder. 
  That is, this kind of entry is in parallel to the results entry,
  however the log contains a prose description of the experiment,
  whereas the results folder contains scripts etc of all the gory
  details.  
\end{quote}
\subsection*{Workflow}\label{workflow}

\subsection*{Data Provided}\label{dataprov}
\clearpage
\subsection*{Project Management}\label{proj}
\hypertarget{proj}{Project Management stuff}
\begin{tikzpicture}[dirtree] % it's what we defined above
  
\node [draw=black!50,dashed,rectangle,fill=green!20]{{project plan} }
      child { node {{proposal} }
          child { node {{approved version: this is the master plan}} }
      }
      child { node {{meetings} }
          child { node {{meeting1}} }
      };

\end{tikzpicture}
\end{document}
#+end_src
** 2015-11-29-notes-from-dr-climate
-re-data-reference-syntax-models-for-file-organisation-and-naming
#+name:notes-from-dr-climate-re-data-reference-syntax-models-for-file-organisation-and-naming-header
#+begin_src markdown :tangle no :exports none :eval no :padline no
---
name: notes-from-dr-climate-re-data-reference-syntax-models-for-file-organisation-and-naming
layout: post
title: Notes from Dr Climate Re data reference syntax models for file organisation and naming
date: 2015-11-29
categories:
- disentangle
tags:
- data management
---

- This is an excellent explanation of the Australian Integrated Marine Observing System (IMOS) Data Reference Syntax by Damien Irving on the Dr Climate blog  [https://drclimate.wordpress.com/2015/09/04/managing-your-data/](https://drclimate.wordpress.com/2015/09/04/managing-your-data/)
- A Data Reference Syntax (DRS) – a convention for naming your files

```
<computer>/<project>/<organisation>/<collection>/<facility>/<data-type>/<site-code>/<year>/

The data type has a sub-DRS of its own, which tells us that the data
represents the 1-hourly average surface current for a single month
(October 2012), and that it is archived on a regularly spaced spatial
grid and has not been quality controlled.

Just in case the file gets separated from this informative directory
structure, much of the information is repeated in the file name
itself, along with some more detailed information about the start and
end time of the data, and the last time the file was modified:

<project>_<facility>_V_<time-start>_<site-code>_FV00_<data-type>_<time-end>_<modified>.nc.gz

In the first instance this level of detail seems like a bit of
overkill... 

Since the data are so well labelled,
locating all monthly timescale ACORN data from the Turquoise Coast and
Rottnest Shelf sites (which represents hundreds of files) would be as
simple as typing the following at the command line:

$ ls */ACORN/monthly_*/{TURQ,ROT}/*/*.nc

```
<p></p>

## Damien's personalised DRS

- It is worthwhile thinking through these ideas and incorporating them in ones data management system as early as possible
- Damien has also helpfully openly shared his own DRS at [https://github.com/DamienIrving/climate-analysis/blob/master/data_reference_syntax.md](https://github.com/DamienIrving/climate-analysis/blob/master/data_reference_syntax.md)
- Here is a summary of some key items I'm going to implement versions of for my own work

```
Basic data files

<var>_<dataset>_<level>_<time>_<spatial>.nc

Sub-categories:  

,* <time>: <tstep>-<aggregation>-<season>
,* <spatial>: <grid>-<region>-<bounds>-<np>

Where:  

,* <tstep>: daily, monthly
,* <aggregation>: 030day-runmean, anom-wrt-1979-2011, anom-wrt-all
,* <season>: JJA, MJJASO
,* <grid>: native or something like y181x360, which describes the number of latitude (181) and longitude (360) points (in this case it is a 1 by 1 degree horizontal grid).
,* <region>: Region names are defined in netcdf_io.py
,* <bounds>: e.g. lon225E335E-lat10S10N or mermax, zonal-anom 
,* <np>: North pole location, e.g. np20N260E

Examples include:  
psl_Merra_surface_daily_y181x360.nc 

More complex file names

<inside>_<filters>_<prev-var>_<dataset>_<level>_<time>_<spatial>.nc 

Sub-categories:

,* <inside>: The variable inside the file. e.g. tas-composite, datelist
,* <filters>: e.g. samgt90pct (gt and lt and used for greater and less than, pct for percentile)
,* <prev-var>: if it's not obvious what variable <inside> was created from, include the previous variable/s

Examples:  
tas-composite_pwigt90pct_ERAInterim_500hPa_030day-runmean-anom-wrt-all_native-sh.png
```
<p></p>

### Principles of Tidy Data

In the words of Hadley Wickham the order that data should be
arranged in follows some generic principles:

```
'A good ordering makes it easier to scan the raw values. One way of
organizing variables is by their role in the analysis: are values
fixed by the design of the data collection, or are they measured
during the course of the experiment? Fixed variables describe the
experimental design and are known in advance. Computer scientists
often call fixed variables dimensions, and statisticians usually
denote them with subscripts on random variables. Measured variables
are what we actually measure in the study. Fixed variables should come
first, followed by measured variables, each ordered so that related
variables are contiguous. Rows can then be ordered by the first
variable, breaking ties with the second and subsequent (fixed)
variables.'
```
<p></p>
### An exemplar

In my last project the protocol we developed (for an ecology and biodiversity database) had a naming convention which relied heavily on a sequence of information being used to order the names of folders, subfolders and files.  This is:

1. The project name (and optional sub-project name)
1. Data type (such as experimental unit, observational unit, and/or measurement methods)
1. Geographic location (locality name, State, Country)
1. Temporal frequency and coverage (such as annual or seasonal tranches).

### The concepts of slow moving dimensions and fast moving variables

The concept of dimensions and variables can be useful here, and especially for deciding on filenames.  Dimensions are fixed or change slowly while variables change more quickly.  By 'change', this  means that there are more of them. For example the project name is 'fixed', that is it does not change across the files, but the sub-project name does change, just more slowly (say there may be 2-3 different sub-projects within a project). Then there may be a set of data types, and these 'change' more quickly than the sub-project name.  Then the geographic and temporal variables might change quickest of all.

So a general rule for the order of things can be stated. The fixed and slowly changing variables should come first (those things that don't change, or don't change much), 
followed by the more fluid variables (or things that change more across the project). 
List elements can then be ordered so that the groups of things that are similar will always be contiguous, and vary sequentially within clusters.

So the only thing I disagree with Damien about is his decision to put space after time:

`<var>_<dataset>_<level>_<time>_<spatial>.nc`

<p></p>

This is  because I think that the geography is more stable than the time period for a data collection, and as most of my studies look at changes of variables measured at a location over time I generally want to compare the same spot at multiple times.  There are pros and cons of each approach such as if the analyst wants to make maps of a variable measured at several locations at a single point in time then having the data arranged by time first and then location may make that job simpler.

I also notice however that the IMOS syntax puts the site spatial location before the year.




    
#+end_src

** reporting from metacat XMLs
~/Dropbox/data/ltern_data_inventory/errata_and_addenda/metadata_checking

<<<<<<< HEAD
** 2015-12-22-my-framework-of-scientific-workflow
-and-integration-software-for-holistic-data-analysis


*** header
#+name:my-framework-of-scientific-workflow-and-integration-software-for-holistic-data-analysis-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-12-22-my-framework-of-scientific-workflow-and-integration-software-for-holistic-data-analysis.md :exports none :eval no :padline no
---
name: my-framework-of-scientific-workflow-and-integration-software-for-holistic-data-analysis
layout: post
title: My framework of scientific workflow and integration software for holistic data analysis
date: 2015-12-22
categories:
- data management
- swish
---

Scientific workflow and integration software for holistic data analysis (SWISH) is a 
title I have given to describe the area of my research that focuses on the tools and techniques 
of reproducible data analysis.

Reproducibility is the ability to recompute the results of a data
analysis with the original data.  It is possible to have analyses that
are reproducible with varying degrees of difficulty. A data
analysis might be reproducible but require thousands of hours of work to
piece together the datasets, transformations, manipulations, calculations and interpretations of computational results.
A primary challenge to reproducible data analysis is to make analyses
that are _easy_ to reproduce.

To achieve this, a guiding principle is that analysts should
effectively implement 'pipelines' of method steps and tools.  Data
analysts should employ standardised and evidence-based methods based
on conventions developed from many data analysts approaching the
problems in a similar way, rather than each analyst configuring 
pipelines to suit particular individual or domain-specific
preferences.

## Planning and implementing a pipeline

It can be much easier to conceptualise a complicated data analysis
method than to implement this as a reproducible research pipeline. The
most effective way to implement a pipeline is by methodically tracking
each of the steps taken, the data inputs needed and all the outputs of
the step.  If done in a disciplined way then the analyst or some other
person could 'audit' the procedure easily and access the details of
the pipeline they need to scrutinise.

### Toward a standardised data analysis pipeline framework

In my own work I have tried a diverse variety of configurations based on 
things I have read and discussions I have had.  Coming to the end of 
my PhD project I have reflected on the framework that I have arrived at and 
present this below as a schematic overview.
#+end_src
*** schematic
#+name:my-framework-of-scientific-workflow-and-integration-software-for-holistic-data-analysis-header
#+begin_src R :session *R* :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-12-22-my-framework-of-scientific-workflow-and-integration-software-for-holistic-data-analysis.md :exports none :eval no :padline yes
      
  ```
  ,*   /home/
  ,**    /overview.org 
             - summary data_inventory
             - DMP
  ,**    /worklog.org    
             - YYYY-MM-DD
  ,*   /projects/
  ,**    /project1_data_analysis_project_health_research
  ,***       /dataset1_merged_health_outcomes_and_exposures
               - index.org
               - git (local private, gitignore all subfolders)
               - workplan
               - worklog
               - workflow
               - main.Rmd
  ,****         /data1_provided
  ,****         /data2_derived
  ,*****            - workflow script
  ,****         /code
  ,****         /results/  (this has all the pathways explored)
  ,*****           - README.md
                   - git (public Github)
                   /YYYY-MM-DD-shortname (i.e. EDA, prelim, model-selection, sensitivity)
                       /main.Rmd
                       /code/
                       /data/
  ,****         /report/
                     /manuscript.Rmd
                       - main results recomputed in production/publication quality
                       - supporting_information (but also can refer to github/results)
                   /figures_and_tables/
                       - png
                       - csv
  ,*****           /journal_submission/
                       - cover letter
                       - approval signatures
                       - submitted manuscript
  ,*****           /journal_revision/
                       - response.org
  ,**    /project2_data_analysis_project_exposure_assessment
             - index.org
             - git
  ,***       /dataset2.1_monitored_data
                - workplan
                - worklog
                - workflow
  ,****         /data1_provided
  ,****         /data2_derived 
                   - stored here or
                   - web2py crud or
                   - geoserver
                /data1_and_data2_backups
                /reports/
                   - manuscript.Rmd -> publish with the data somehow
                /tools (R package)
                   - git/master -> Github
  ,****      /dataset2.2_GIS_layers 
  ,**    /methods_or_literature_review_project
  ,*  /tools/
           /web2py
               /applications
                   /data_inventory
                       - holdings
                       - prospective
                   /database_crud
            /disentangle (R package)
            /pipeline_templates
  ,**   /data/
           /postgis_hanigan
           /postgis_anu_gislibrary
           /geoserver_anu_gislibrary
  ,**   /references/
           - mendeley
           - bib
           - PDFs annotated
  ,**   /KeplerData/workflows/MyWorkflows/
  ,***      /data_analysis_workflow_using_kepler (implemented as an R package)
  ,****         /inst/doc/A01_load.R
  ,***      /data_analysis_workflow_using_kepler (implemented as an R LCFD workflow)
               - main.Rmd (raw R version)
               - main.xml (this is kepler)
  ,****         /data/
                   - file1.csv
                   - file2.csv
  ,****         /code/
                   - load.R
  ```
  
#+end_src
* Data documentation
** Scales and units
***  Morpho’s definitions

The concept of a measurement scale as defined by Stevens is useful for
classifying data despite the weaknesses of the approach that have been
pointed out by several practitioners. In particular, the
classification allows us to determine some of the mathematical
operations that are appropriate for a given set of data, and allows us
to determine which types of metadata are needed for a given set of
data. For example, categorical data never have a "unit" of
measurement.  Here is a brief overview of the measurement scales we
have employed in EML. They are based on Steven's original typology,
with the addition of "Date-Time" for purely pragmatic reasons (we need
to distinguish date time values in order to collect certain essential
metadata about date and time representation).

**** NOMINAL
- The nominal scale places values into named categories. The
 different values within a set are unordered. Some examples of
 nominal scales include gender (Male/Female) and marital status
 (single/married/divorced). Text fields should be classified as
 nominal.
**** ORDINAL
- The ordinal scale places values in a set order. All ordinal values
 are also nominal. Ordinal data show a particular value's position
 relative to other values, such as "low, medium, high, etc." The
 ordinal scale doesn't indicate the distance between each item.
**** INTERVAL
- The interval scale uses equal-sized units of measurement on a
 scale between values. It therefore allows the comparison of the
 differences between two values on the scale. With interval data,
 the allowable values start from an arbitrary point (not a
 meaningful zero), and so there is no concept of 'zero' of the
 measured quantity. Consequently, ratios of interval values are not
 meaningful. For example, one can not infer that someone with a
 value of 80 on an ecology test knows twice as much ecology as
 someone who scores 40 on the test, or that an object at 40 degrees
 C has twice the kinetic energy as an object at 20 degrees C. All
 interval values are also ordered and therefore are ordinal scale
 values as well.
**** RATIO
- The ratio scale is an interval scale with a meaningful zero
 point. The ratio scale begins at a true zero point that represents
 an absolute lack of the quality being measured. Thus, ratios of
 values are meaningful. For example, an object that is at elevation
 of 100 meters above sea level is twice as high as an object that
 is at an elevation of 50 meters above sea level (where sea level
 is the zero point). Also, an object at 300 degrees Kelvin has
 three times the kinetic energy of an object at 100 degrees Kelvin
 (where absolute zero (no motion) defines the zero point of the
 Kelvin scale). Interval values can often be converted to ratio
 values in order to make ratio comparisons legitimate. For example,
 an object at 40 degrees C is 313.15 degrees Kelvin, an object at
 20 degrees C is 293.15 degrees Kelvin, and so the first object has
 approximately 1.07 times more kinetic energy (note the wrong
 answer you would have gotten had you taken the ratio of the values
 in Celsius).
**** DATE-TIME
- Date and time values in the Gregorian calendar are very strange to
 use in calculations in that they have properties of both interval
 and ratio scales. They also have some properties that do not
 conform to the interval scale because of the adjustments that are
 made to time to account for the variations in the period of the
 Earth around the sun. While the Gregorian calendar has a
 meaningful zero point, it would be difficult to say that a value
 taken on midnight January 1, 1000 is twice as old as a value taken
 on midnight January 1 2000 because the scale has many
 irregularities in length in practice. However, over short
 intervals the scale has equidistant points based on the SI second,
 and so can be considered interval for some purposes, especially
 with respect to measuring the timing of short-term ecological
 events. Date and time values can be represented using several
 distinct notations, and so we have distinct metadata needs in
 terms of specifying the format of the value
 representation. Because of these pragmatic issues, we separated
 Date-time into its own measurement scale. Examples of date-time
 values are '2003-05-05', '1999/10/10', and
 '2001-10-10T14:23:20.3'.

*** Advice for best practices can be found:
https://im.lternet.edu/sites/im.lternet.edu/files/LTERunitBestPractices_V13.pdf
*** Rainfall
 
On the basis of Morpho's recommendations rainfall data is interval data – then 0 just another
value in “interval data” terms.  The confusion arises because there are
two different versions of “meaningful” here – 0 is certainly a
meaningful (& sensible) rainfall value – in terms of observations and
physical processes (no rain) – but this is not the sort of
“meaningful” they are talking about.
 
Their “meaningful zero” is precisely a value that CANNOT be observed
since all the values are by definition positive (so that valid ratios
can be formed).
 
The two “meaningfuls” don’t mesh at all well.  Especially since, as
mentioned, it is common practice to refer to ratios of rainfall (when
they are not zero).


*** Counts
For DATA “counts” use the following:

- Category = Relative (interval)
- Standard unit = dimensionless -> number
- Number type = whole or natural

*** Altitude

- Category = Relative (interval)
- Standard unit = length -> metre (or other length unit)
- Number type = whole or real
* Exploratory Data Analysis
** 2015-10-26-show-missingness-in-large-dataframes
*** post
#+name:show-missingness-in-large-dataframes-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-28-show-missingness-in-large-dataframes-v2.md :exports none :eval no :padline no
---
name: show-missingness-in-large-dataframes
layout: post
title: Show missingness in large dataframes, version 2
date: 2015-10-28
categories:
- disentangle
- Exploratory Data Analysis
---

- UPDATE: the other day I blogged this but I needed to tweak things, so this is a re-post with extra
- UPDATE 2: Today an R blogger has posted a new solution [/2015/12/show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger](/2015/12/show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger)


## The old post

- Sometime ago I saw this example of a method for assessing missing data in a large data frame [http://flowingdata.com/2014/08/14/csv-fingerprint-spot-errors-in-your-data-at-a-glance/](http://flowingdata.com/2014/08/14/csv-fingerprint-spot-errors-in-your-data-at-a-glance/)
- I asked my colleague Grant about doing this in R and he whipped up the following code to generate such an image:

![/images/bankstown_traffic_counts_full_listing_june_2014.svg](/images/bankstown_traffic_counts_full_listing_june_2014.svg)

#### Code
    misstable <- function(atable){
     op <- par(bg = "white")
     plot(c(0, 400), c(0, 1000), type = "n", xlab="", ylab="",
         main = "Missing Data Table")
    
    
     pmin=000
     pmax=400
     stre=pmax-pmin
     lnames=length(atable)
     cstep = (stre/lnames)
     for(titles in 1:lnames){
     text((titles-1) * cstep+pmin+cstep/2,1000,colnames(atable)[titles])
     }
    
     gmax=900
     gmin=0
     gstre=gmax-gmin
     rvec = as.vector(atable[ [ 1 ] ])
     dnames=length(rvec)
     step = gstre / dnames
     for(rows in 1:dnames){
     text(30,gmax - (rows-1)*step-step/2,rvec[rows])
     ymax=gmax - (rows-1)*step
     ymin=gmax - (rows)*step
     for(col in 2:lnames-1){
     if(atable[rows,col+1] == F){
     tcolor = "red"
     }
     if(atable[rows,col+1] == T){
     tcolor = "white"
     }
     rect((col) * (stre/lnames)+pmin, ymin, (col+1) * (stre/lnames)+pmin,
     ymax,col=tcolor,lty="blank")
     }
     }
    }
<p></p>

- Now things to note are that the function expects the data to be TRUE if Not NA and  FALSE if is NA
- so might need to massage things a bit first
- here is the small test Grant supplied

#### Code
    require(grDevices)
       
    # Make a quick dataframe with true/false representing data availability
    locs=c("Australia","India","New Zealand","Sri Lanka","Uruguay","Somalia")
    f1=c(T,F,T,T,F,F)
    f2=c(F,F,F,T,F,F)
    f3=c(F,T,T,T,F,T)
    atable=data.frame(locs,f1,f2,f3)
    atable
    #Draw the table.
    misstable(atable)
    
<p></p>

- here is the one I worked on today

#### Code
    # having defined the input dir and input file tried reading the excel sheet (without head 3 rows)
    #dat <- readxl::read_excel(file.path(indir, infile), skip =3)
    # got lots of warnings()
    ## 50: In read_xlsx_(path, sheet, col_names = col_names, col_types = col_types,  ... :
    ##   [1278, 4]: expecting date: got '[NULL]'
    # I always worry about using excel connections so open in excel (in windows) 
    # and save as to convert to CSV
    dat <- read.csv(file.path(indir, gsub(".xlsx", ".csv", infile)), skip =3, stringsAsFactor = F)
    str(dat)
    # 'data.frame':	1396 obs. of  167 variables:
    # but most of the cols and a third of the rows are empty!
    # check missings
    dat2 <- data.frame(id = 1:nrow(dat), dat)
    str(dat2)
    # first if they are empty strings
    dat2[dat2 == ""] <- NA
    # now if NA
    dat2[,2:ncol(dat2)] <- !is.na(dat2[,2:ncol(dat2)])
    
    # Truncate the hundreds of empty cols
    str(dat2[,1:18])
    tail(dat2[,1:18])
    svg(file.path(outdir, gsub(".csv", ".svg", outfile))    )
    misstable(dat2[,1:18])
    dev.off()
    browseURL(file.path(outdir, gsub(".csv", ".svg", outfile))    )
    
    # cool, that is an effective way to look at the data
    
#+end_src

*** COMMENT misstable-code
#+name:misstable
#+begin_src R :session *R* :tangle R/misstable.R :exports none :eval no
  #### name:misstable ####
    
  #Plot Function
  misstable <- function(atable){
   op <- par(bg = "white")
   plot(c(0, 400), c(0, 1000), type = "n", xlab="", ylab="",
       main = "Missing Data Table")
  
  
   pmin=000
   pmax=400
   stre=pmax-pmin
   lnames=length(atable)
   cstep = (stre/lnames)
   for(titles in 1:lnames){
   text((titles-1) * cstep+pmin+cstep/2,1000,colnames(atable)[titles])
   }
  
   gmax=900
   gmin=0
   gstre=gmax-gmin
   rvec = as.vector(atable[[1]])
   dnames=length(rvec)
   step = gstre / dnames
   for(rows in 1:dnames){
   text(30,gmax - (rows-1)*step-step/2,rvec[rows])
   ymax=gmax - (rows-1)*step
   ymin=gmax - (rows)*step
   for(col in 2:lnames-1){
   if(atable[rows,col+1] == F){
   tcolor = "red"
   }
   if(atable[rows,col+1] == T){
   tcolor = "white"
   }
   rect((col) * (stre/lnames)+pmin, ymin, (col+1) * (stre/lnames)+pmin,
   ymax,col=tcolor,lty="blank")
   }
   }
  }
  
    
#+end_src

#+RESULTS: misstable

*** COMMENT test-code
#+name:test
#+begin_src R :session *R* :tangle test.R :exports none :eval no
  #### name:test ####
    
    
   require(grDevices)
      
   # Make a quick dataframe with true/false representing data availability
   locs=c("Australia","India","New Zealand","Sri Lanka","Uruguay","Somalia")
   f1=c(T,F,T,T,F,F)
   f2=c(F,F,F,T,F,F)
   f3=c(F,T,T,T,F,T)
   atable=data.frame(locs,f1,f2,f3)
   atable
   #Draw the table.
   misstable(atable)
  
  dat <- read.csv("~/data/LTERN/kwrt_woodland_restoration/kwrt_birds_spring_2013_p12/kwrt_birds_spring_p12t312.csv")
  dat[,2:ncol(dat)] <-  is.na(dat[,2:ncol(dat)])
  dat[1:10,1:4]
  names(dat) <- paste("V", 1:ncol(dat), sep = "")
  nrow(dat)
  str(dat)
  png("misstable.png", height=1800, width = 3000, res = 200)
  misstable(dat[1:100,])
  dev.off()
  browseURL("misstable.png")
#+end_src


** 2015-12-02-show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger
#+name:show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-12-02-show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger.md :exports none :eval no :padline no
---
name: show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger
layout: post
title: show-missingness-in-large-dataframes-with-ggplot-thanks-to-r-blogger
date: 2015-12-02
categories:
- disentangle
tags:
- exploratory data analysis
---

- This is a revision of my post [/2015/10/show-missingness-in-large-dataframes-v2](/2015/10/show-missingness-in-large-dataframes-v2)
- This guy posted [http://www.njtierney.com/r/missing%20data/rbloggers/2015/12/01/ggplot-missing-data/](http://www.njtierney.com/r/missing%20data/rbloggers/2015/12/01/ggplot-missing-data/)

### Let's try it out!

```
library(devtools)
# depends
install.packages("gbm")
install_github("tierneyn/neato")
library(neato)
# small eg
locs=c("Australia","India","New Zealand","Sri Lanka","Uruguay","Somalia")
f1=c(T,F,T,T,F,F)
f2=c(F,F,F,T,F,F)
f3=c(F,T,T,T,F,T)
atable=data.frame(locs,f1,f2,f3)
atable[atable == FALSE] <- NA
atable
png("ggplotmissing.png")
ggplot_missing(atable)
dev.off()

```
<p></p>

![/images/ggplotmissing.png](/images/ggplotmissing.png)

- The one I had problems with because too large is:

```
# Cool but what about a big one?
dat <- read.csv("~/path/to/file.csv")
str(dat)
png("ggplotmissing2.png", height=1800, width = 3000, res = 200)
ggplot_missing(dat)
dev.off()

```

![/images/ggplotmissing2.png](/images/ggplotmissing2.png)

#+end_src
*** COMMENT code
#+name:code
#+begin_src R :session *R* :tangle code.R :exports none :eval no
  #### name:code ####
  library(devtools)
  # depends
  install.packages("gbm")
  install_github("tierneyn/neato")
  library(neato)
  
  locs=c("Australia","India","New Zealand","Sri Lanka","Uruguay","Somalia")
  f1=c(T,F,T,T,F,F)
  f2=c(F,F,F,T,F,F)
  f3=c(F,T,T,T,F,T)
  atable=data.frame(locs,f1,f2,f3)
  atable[atable == FALSE] <- NA
  atable
  png("ggplotmissing.png")
  ggplot_missing(atable)
  dev.off()
  
  
  # Cool but what about a big one?
  dat <- read.csv("~/data/LTERN/kwrt_woodland_restoration/kwrt_birds_spring_2013_p12/kwrt_birds_spring_p12t312.csv")
  dat[,2:ncol(dat)] <-  is.na(dat[,2:ncol(dat)])
  dat[1:10,1:4]
  names(dat) <- paste("V", 1:ncol(dat), sep = "")
  nrow(dat)
  dat[dat == FALSE] <- NA
  str(dat)
  png("ggplotmissing2.png", height=1800, width = 3000, res = 200)
  ggplot_missing(dat)
  dev.off()
  browseURL("ggplotmissing2.png")
  
#+end_src

** recursive queries with SQL
http://blog.revolutionanalytics.com/2015/12/exploring-recursive-ctes-with-sqldf.html
* Statistical modelling
** causal influence diagrams with tikz
*** COMMENT go
#+name:go
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:go ####
  dir()
  system("pdflatex causes.tex")
  #browseURL("causes.pdf")
#+end_src

*** header
#+name:swish-dmp
#+begin_src latex :tangle causes.tex :exports none :eval no
  
  \documentclass{article}
  \usepackage{hyperref}
  \usepackage{tikz}
  \usetikzlibrary{calc}
  
  \usepackage{tikz}
    \begin{document}
  
  \begin{tikzpicture}[
    outpt/.style={->,blue!80!black,very thick},
    >=stealth,
    every node/.append style={align=center}]
    \node (aux) at (0,18) [draw=black!50,dashed,rectangle,fill=green!30]{Auxiliary resources}; 
    \node (aux) at (0,17) [draw=black!50,dashed,rectangle,fill=yellow!30]{Dropbox}; 
    
    \node (measdata) at (-2.4,9) [draw=black!50,dashed,rectangle,fill=orange!30]{Distributed data}; 
    \node (hypothesis) at (2,9) [draw=black!50,dashed,rectangle,fill=red!30]{Permissions \\ + citations}; 
    \node (anadata) at (0,7.5) [draw=black!50,dashed,rectangle,fill=orange!30] {\begin{tabular}{@{}c}feedback \end{tabular}};
    \node (anadata3) at (0,0) [draw=black!50,dashed,rectangle,fill=orange!30] {\begin{tabular}{@{}c}Version control\end{tabular}};
  
    \draw[outpt](anadata)--(measdata);
    \draw[outpt](measdata)--(hypothesis);
    \draw[outpt](hypothesis)--(anadata);
    
  \end{tikzpicture}
  \end{document}
  
  
#+end_src

** graphical models
http://jmbh.github.io//Estimation-of-mixed-graphical-models/
** Confounding definition
- TODO Confounding is defined as a distortion in an 'effect measure introduced by an extraneous variate'. Rothman, K. J. (1976). Causes. Journal of Epidemiology, 104(6), 587–592.
- a confounder, W is associated with both exposure (tempera- ture) and outcome (mortality) and provides an unblocked backdoor path between mortal- ity and temperature, in the language of DAGs (Greenland et al. 1999) p34 2nd col (from \cite{Reid2012})
** causal diagrams
Greenland, S., Pearl, J., & Robins, J. M. (1999). Causal diagrams for epidemiologic research. Epidemiology (Cambridge, Mass.), 10(1), 37–48. doi:10.1097/00001648-199901000-00008
* bibliometrics and literature reviewing
** TODO author indices
*** scholar metrics
http://datascienceplus.com/hindex-gindex-pubmed-rismed/

*** text mining
http://tuxette.nathalievilla.org/?p=1682

** 2015-12-17-using-scholar-rankings-to-provide-weights-in-systematic-literature-reviews-part-1


#+name:using-scholar-rankings-to-provide-weights-in-systematic-literature-reviews-part-1-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-12-17-using-scholar-rankings-to-provide-weights-in-systematic-literature-reviews-part-1.md :exports none :eval no :padline no
---
name: using-scholar-rankings-to-provide-weights-in-systematic-literature-reviews-part-1
layout: post
title: Using scholar rankings to provide weights in systematic literature reviews part 1
date: 2015-12-17
categories:
- disentangle
---


- I've been thinking alot recently about an approach used in this recent systematic review 

```
Vins, H., Bell, J., Saha, S., & Hess, J. (2015). The mental health
outcomes of drought: A systematic review and causal process
diagram. International Journal of Environmental Research and Public
Health, 12(10), 13251–13275. doi:10.3390/ijerph121013251
```
<p></p>

- They identify causal pathways from papers and ascribe the supporting evidentiary weight based on the number of papers published with findings that support this cause-effect pathway
- The raw number of papers is probably not a good metric, prone to bias so I was thinking of ways to ascribe weight based on quality of journal or authors 
- This is not supposed to replace the need to actually read the papers, but purely as an additional source of information
- This recent post on scholar metrics provided some impetus via h-indices
[http://datascienceplus.com/hindex-gindex-pubmed-rismed/](http://datascienceplus.com/hindex-gindex-pubmed-rismed/)
- I also think this approach of text mining the abstracts could be useful [http://tuxette.nathalievilla.org/?p=1682](http://tuxette.nathalievilla.org/?p=1682)

### Sanity check of the two options using myself as guinea pig

```   
library(RISmed)
x <- "hanigan+ic[Author]"
res <- EUtilsSummary(x, type="esearch", db="pubmed", datetype='pdat', mindate=1900, 
  maxdate=2015, retmax=500)
str(res)
citations1 <- Cited(res)
citations <- as.data.frame(citations1)
citations <- citations[order(citations$citations,decreasing=TRUE),]
citations <- as.data.frame(citations)
str(citations)
citations <- cbind(id=rownames(citations),citations)
citations$id<- as.character(citations$id)
citations$id<- as.numeric(citations$id)
hindex <- max(which(citations$id<=citations$citations))

hindex
# 5

library(scholar)
myid <- "cGN1P0wAAAAJ"
y <- scholar::get_publications(myid)
str(y)
y[,c("author", "cites")]
y$id <- as.numeric(row.names(y))
hindex2 <- max(which(y$id<=y$cites))
hindex2
# 15

```

### Clearly the pubmed and google scholar search engines makes a big difference to my score!

#+end_src

* Diagrams
** newgraph
*** R-newgraph
#+name:newgraph
#+begin_src R :session *R* :tangle R/newgraph.r :exports none :eval yes
    
  newgraph <- function(
    indat2  = nodes
    ,
    in_col = "causes"
    ,
    out_col  = "effect"
    ,
    colour_col = "colour"
    ,
    pos_col = "pos"
    ,
    label_col = TRUE
  ){
  nodes2 <-as.data.frame(matrix(ncol = 2, nrow = 0))
  names(nodes2)  <- c("inputs", "outputs")
  nameslist <- character(0)
  colourslist <- character(0)
  poslist <- character(0)
  labellist <- character(0)
  for(i in 1:nrow(indat2)){
  #  i <- 1
    #i
    indat2[i,]
    inputs <- unlist(lapply(strsplit(indat2[i,in_col], ","), str_trim))
    outputs <- unlist(lapply(strsplit(indat2[i,out_col], ","), str_trim))
    if(length(inputs) > 0){
      nodes2 <- rbind(nodes2, cbind(inputs, outputs))
    }
    nameslist <- c(nameslist, outputs)
    labellist <- c(labellist, indat2[i, label_col])
    colourslist <- c(colourslist, indat2[i, colour_col]) 
    poslist <- c(poslist, indat2[i, pos_col])
  }
  ## nodes2
  ## nameslist
  ## colourslist
  ## poslist
  #labellist
  edges_outcome <- create_edges(from = nodes2$inputs,
                          to =   nodes2$outputs
                          )
  if(label_col == TRUE){
    label2 <- TRUE
  } else {
    label2 <- labellist
  }
  #label2
  nodes_outcome <- create_nodes(nodes = nameslist,
                          label = label2,
                          color = colourslist, pos = poslist)
  #nodes_outcome
  graph_outcome <- create_graph(nodes_df = nodes_outcome,
                         edges_df = edges_outcome)
  return(graph_outcome)
  }
  
#+end_src

#+RESULTS: newgraph

*** test-newgraph
#+name:newgraph
#+begin_src R :session *R* :tangle tests/test-newgraph.r :exports none :eval yes
  # name:newgraph
  library(DiagrammeR)
  library(stringr)
  nodes  <- read.csv(textConnection('causes,         effect, colour, pos, label
                  , Rainfall deficit,                           , "-1,3!", and this is a very\\llong line
  Rainfall deficit, Drought,                                 indianred, "-0.5,2!", b
                 , Insular society, ,                             "1.5,3!", c
  Insular society, Anomie, ,                                      "1,2!", d
  "Drought, Anomie", Altered social structures and dynamics, gray,  "0,0!", e
  Altered social structures and dynamics, Depression, lightblue,         "4,0!", f
  Depression,                             Suicide   , lightgreen,        "7,0!", g
  '), stringsAsFactors = F, strip.white = T)
  nodes
  
  dotty <- newgraph(
    indat2  = nodes
    ,
    in_col = "causes"
    ,
    out_col  = "effect"
    ,
    colour_col = "colour"
    ,
    pos_col = "pos"
    ,
    label_col = "label"
    )
  cat(dotty$dot_code)
  
  # just test this out
  render_graph(dotty)
  #render_graph(dotty, output = "visNetwork")
  
  # actual control
  dotty2 <- gsub("digraph \\{",
  "digraph \\{
  graph [layout = neato]
  node [fontname = Helvetica,
       style = filled]
  edge [color = gray20,
       arrowsize = 1,
       fontname = Helvetica]",
  dotty$dot_code)
  #cat(dotty2)
  grViz(dotty2)
  
#+end_src

#+RESULTS: code

** boxes-arrows-and-curves
*** COMMENT code
#+name:code
#+begin_src tex :session *shell* :tangle no :exports none :eval no
%https://hstuart.dk/2007/02/21/drawing-trees-in-latex/
\documentclass{article}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{positioning}
\usetikzlibrary{fit}
\usetikzlibrary{backgrounds}
\usetikzlibrary{calc}
\usetikzlibrary{shapes}
\usetikzlibrary{mindmap}
\usetikzlibrary{decorations.text}

\begin{document}
 \begin{tikzpicture} [
            outpt/.style={->,blue!80!black,very thick},
            >=stealth,
         every node/.append style={align=center}]
\tikzstyle{every node}=[draw,rectangle] 
\node (root) at (0,0) {UCRH Data Warehouse }
%  child { 
node (CC)[above left=of root]{Collaborator 1} 
%} 
%  child { 
node (JH)[left=of root]{Collaborator 2}
%} 
%  child { 
node (rightmost)[below left=of root] {Collaborator 3}  
%}
 ; 
\tikzstyle{every node}=[] 
\draw[-latex,color=red] (root) .. controls +(east:6cm) and +(right:8cm) .. node[near end,above right,color=black] {Re-distribute complete DB (v1, v2 etc)} (rightmost); 
\draw [outpt] (CC)--(root);
\draw [outpt] (JH)--(root);
\draw [outpt] (rightmost)--(root);
\end{tikzpicture} 
\end{document}
#+end_src

* Reproducible Research Pipelines
** manuscript_template
- An Rmarkdown manuscript template, inspired by https://github.com/jhollist/manuscriptPackage
- I reviewed that work but I think it takes it to an uneccessary level of complicated-ness
- Mine is the same but built like an emacs orgmode file not an R package

** RRR template simple
*** headers
#+begin_src R :session *R* :tangle main.Rmd :exports none :eval no :padline no
  ---
  title: "Rmarkdown LaTeX tests"
  author: Ivan C. Hanigan
  header-includes:
    - \usepackage{graphicx}
    - \usepackage{longtable}
  output:
    html_document:
      toc: true
      theme: united
      number_sections: yes    
    pdf_document:
      toc: true
      toc_depth: 3
      highlight: zenburn
      keep_tex: true
      number_sections: no        
  documentclass: article
  classoption: a4paper
  csl: mee.csl
  bibliography: references.bib
  ---
  
  ```{r echo = F, eval=F, results="hide"}
#+end_src
*** run-able R
#+begin_src R :session *R* :tangle main.Rmd :exports none :eval yes :padline no
  dir()
  library(rmarkdown)
  library(knitr)
  library(knitcitations)
  library(bibtex)
  cleanbib()
  cite_options(citation_format = "pandoc", check.entries=FALSE) 
  rmarkdown::render("main.Rmd", "pdf_document")
  #browseURL("main.pdf")
  rmarkdown::render("main.Rmd", "html_document")
  #browseURL("main.html")
  # to get the R code only 
  knitr::knit("main.Rmd", tangle=T)
  ```
#+end_src

#+RESULTS:
: /home/ivan_hanigan/tools/disentangle/main.html

*** Deprecated?  Not needed anymore? to tangle out pure R
#+begin_src R :session *shell* :tangle no none :eval no :padline no

  ```{r echo = F, eval=F, results="hide"}  
  # http://stackoverflow.com/a/26066411
  # to tangle chunks even when eval = F use this (with eval=F)
  library(knitr)
  knit_hooks$set(purl = function(before, options) {
    if (before) return()
    input  = current_input()  # filename of input document
    output = paste(tools::file_path_sans_ext(input), 'R', sep = '.')
    if (knitr:::isFALSE(knitr:::.knitEnv$tangle.start)) {
      assign('tangle.start', TRUE, knitr:::.knitEnv)
      unlink(output)
    }
    cat(options$code, file = output, sep = '\n', append = TRUE)
  })
  
  ```  
#+end_src
*** bib
#+begin_src R :session *shell* :tangle main.Rmd :exports none :eval no :padline no
  
  ```{r, echo = F, results = 'hide'}
  # load
  if(!exists("bib")){
  bib <- read.bibtex("~/references/library.bib")
  }
  ```
  
#+end_src

*** Introduction
**** intro
#+name:intro
#+begin_src R :session *shell* :tangle main.Rmd :exports none :eval no :padline yes 
# Intro
This is the thing `r citep(     bib[[ "Hsiang2011" ]])`.
This is another thing `r citep( bib[[ "Hanigan2012" ]])`.

## subsection

This is an example with a footnote [^com1].

#+end_src


*** fig
#+begin_src R :session *shell* :tangle main.Rmd :exports none :eval no
  ## figure
  
  shown in figure \ref{fig:fig}
  
  ```{r, eval = F, echo = F}
  png("fig.png", res = 150, height = 1000, width = 1000)
  pairs(airquality)
  dev.off()
  ```
  
  \begin{figure}[!h]
  \centering
  \includegraphics[width=.5\textwidth]{fig.png}
  \caption{fig}
  \label{fig:fig}
  \end{figure}
  \clearpage
  
#+end_src
*** table
#+name:main.Rmd
#+begin_src R :session *shell* :tangle main.Rmd :exports none :eval no
  
  ## a table
  
  ```{r, eval = F, echo = F}
  library(xtable)
  library(disentangle)
  library(nycflights13)
  dd <- data_dictionary(flights)
  
  sink('tab1.tex')
  print(xtable(dd, caption = "Table of nominal variables.", lab = "tab:table1"),
  tabular.environment='longtable',
  floating=FALSE,
  caption.placement = "top", add.to.row = list(pos = list(0),
  command = "\\hline \\endhead "),  include.rownames = F)
  sink()
  ```
  
  Shown in Table \ref{tab:table1} is ... 
  
  \input{tab1.tex}
  \clearpage
  
#+end_src

*** COMMENT bib-code
#+name:bib
#+begin_src R :session *shell* :tangle main.Rmd :exports none :eval no

  **References**

  ```{r, echo=FALSE, message=FALSE, eval = T}
  write.bibtex(file="references.bib")
  ```

  [^com1]: more info on footnotes at [http://nancym.tumblr.com/post/59594358553/links-footnotes-and-abbreviations-in-markdown](http://nancym.tumblr.com/post/59594358553/links-footnotes-and-abbreviations-in-markdown)
#+end_src

*** footnotes in markdown
http://nancym.tumblr.com/post/59594358553/links-footnotes-and-abbreviations-in-markdown
This is needed to  provide the trust that a data  analysis was appropriately conducted and avoided errors of execution,  or methodogical design [^com1].

[^com1]: the footnote

** 2015-12-20-this-is-an-open-notebook-but-selected-content-delayed
*** blog
#+name:this-is-an-open-notebook-but-selected-content-delayed-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-12-20-this-is-an-open-notebook-but-selected-content-delayed.md :exports none :eval no :padline no
---
name: this-is-an-open-notebook-but-selected-content-delayed
layout: post
title: This is an open notebook but selected content delayed
date: 2015-12-20
categories:
- disentangle
---

#### Open Notebook Science, Selected Content, Delayed (ONS-SCD)

I am trying to juggle my work in a dual Open-And-Closed way.  

To explain: I try to keep an electronic ‘Open Notebook’ that aligns
with the principles of the Open Notebook Science (ONS) movement’s
‘Selected Content – Delayed’ category (ONS-SCD).  Back in 2012 when I started my notebook I looked 
around for models of what style of publication I wanted.  I knew that some of my work was owned
by the university I work at, and I am not allowed to publish this openly.
Then there is other stuff I owned as part of my PhD, but that I might 
not want to release all the details of my work.  So I settled on a 'Selected Content - Delayed' category and got the logo shown here from the (now-defunct) website [http://onsclaims.wikispaces.com/](http://onsclaims.wikispaces.com/).  The ONS movement
is still described on Wikipedia though [https://en.wikipedia.org/wiki/Open_notebook_science](https://en.wikipedia.org/wiki/Open_notebook_science).

![/images/ONS-SCD.png](/images/ONS-SCD.png)

In this publication model I
make publicly available the content of my research notebook (like a
blog), in which I write reports of the details of the data, code and
documents related to my research. I selectively make material open on
github, and I sometimes delay publication of the material that I keep
in my private research notebook.  That work is kept private either
because it includes unpublished work that I wish to keep embargoed
until after publication, or because it is all the gory details of
process of writing code to create or analyse data that is not
appropriate for open publication.


In previous work I have either paid for additional private repos on
github, and made the repo open once the paper is published, or
alternately used bitbucket with unlimited free private repos for
university students and then just put together a public repo for
sharing 'polished' outputs.

The upshot is that I use this part open / part closed approach during the data exploration, cleaning, analysis and writing. In my opinion as long as the final workflow is clearly and openly documented and reproducible, that's  the most important thing.


#### The motivation stems back to the Climategate scandal and infamous 'Harry Readme' file

My supervisors over the years have all been really supportive of working in an open way and I have flirted with the idea of being completely open.  However, I got a little worried about the implications of working too openly when malicious people might dig though my work for vexatious reasons, such as looking for errors or embarrassing comments I might inadvertently make that, when taken out of context, might make me sound foolish.

This sounds far fetched, but as an example of this, a few years ago there was a fair amount of heat generated by a lot of
emails and other documents from the University of East Anglia Climate
Research University.  I was particularly interested because I was
struggling to make sense of a lot of weird and wonderful databases and
I felt a lot of sympathy for 'Harry', someone who as far as I could
tell was doing a pretty good job of exploring, cleaning and
documenting their work.

Here is one journalists summary of this issue [http://blogs.telegraph.co.uk/technology/iandouglas/100004334/harry_read_me-txt-the-climategate-gun-that-does-not-smoke/](http://blogs.telegraph.co.uk/technology/iandouglas/100004334/harry_read_me-txt-the-climategate-gun-that-does-not-smoke/):

``` 

the contents of the harry_read_me.txt file, apparently leaked from the
University of East Anglia and now becoming a totem for climate change
sceptics to gather around as though it were a piece of the true cross.

This file – thousands of lines of annotations kept on the process of
re-developing a computer model of the climate form figures submitted
by weather stations around the world and other historical data sets –
holds a personal commentary written by an un-named developer (let's
call him Harry), frustrated and often tied up in knots, working late
into the night and the weekend trying to squeeze differently-formatted
numbers into a consistent narrative.  

``` 

<p></p>

#### Using git and Github in an ONS-SCD model

- Recall Noble's framework?  The results folder is what I want to publish
- Noble recommended the following folder and file structures [http://dx.doi.org/10.1371/journal.pcbi.1000424.g001](http://dx.doi.org/10.1371/journal.pcbi.1000424.g001)
- I revised his conceptual diagram, and I blogged about this at [/2015/10/a-quick-review-of-a-quick-guide-to-organizing-computational-biology-projects/](/2015/10/a-quick-review-of-a-quick-guide-to-organizing-computational-biology-projects/)

```
/projectname (eg msms)/
    /doc/
        /ms-analysis.html 
        /paper/
            /msms.tex
            /msms.pdf
    /data/
        /YYYY-MM-DD/
            /yeast/
                /README
                /yeast.sqt
            /worm/
                /README
                /worm.sqt
    /src/
        /ms-analysis.c
    /bin/
        /parse-sqt.py
    /results/
        /notebook.html 
        /YYYY-MM-DD-1/
            /runall
            /split1/
            /split2/
        /YYYY-MM-DD-2/
            /runall
```

<p></p>

#### I want to publish my results, rather than my process

- I had the realisation that [/2015/10/how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology/](/2015/10/how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology/)


```

    the 'Experiment Results' level is about work you might do on a 
       single day, or over a week

    Workflow scripts: At this level each 'experiment' is written up in
    chronological order, as entries to the Worklog at the meso level

    Noble recommends 'create either a README file, in which I store
    every command line that I used while performing the experi- ment,
    or a driver script (I usually call this runall) that carries out
    the entire experiment automatically'...

    and 'you should end up with a file that is parallel to the lab
    notebook entry. The lab notebook contains a prose description of
    the exper- iment, whereas the driver script contains all the gory
    details.'

    this is the level I usually think of managing the distribution
    side of things. I will want to pack up the results and email to my
    collaborators, or decide on the one set of tables and figures to
    write into the manuscript for submission to a journal. If this is
    accepted for publication, this is the one combined package of
    'analytical data and code' that I would consider putting up online
    (to github) as supporting information for the paper.

```

<p></p>

#### Public Github repo within a private local `overview` git repo: My setup 

- I mostly use one single Emacs orgmode file to run the whole project, using tangle to send chunks of code to scripts, after testing them out using the library of babel
- To keep this version controlled I created a git repo for this
- To test out  have created a fake-data-analysis-project and this includes a local git repository
- in the `.gitignore` file I added the commend `*` to ignore all subfolders and files
- If I want to add files to this I need to use `git add -f thefile`
- Then I create a public github repo in the results folder (I named the repo: `THE-PROJECT-NAME-results`

```

$ cd ~/projects/fake-data-analysis-project
$ mkdir results
$ cd results/
/results$ git init
Initialized empty Git repository in /home/ivan_hanigan/tools/ReproducibleResearchPipelineTemplate/results/.git/
/results$ mkdir 2015-12-20-eda
/results$ git remote add origin git@github.com:ivanhanigan/ReproducibleResearchPipelineTemplate-results.git
$ git push -u origin master
```
<p></p>

#### The Result

- An example of these results are now published at [https://github.com/ivanhanigan/ReproducibleResearchPipelineTemplate-results](https://github.com/ivanhanigan/ReproducibleResearchPipelineTemplate-results)
- But the rest of my work is privately held, and version controlled.

#+end_src



** 2016-01-03-exemplars-of-distributing-data-and-code-ropensci
#+name:exemplars-of-distributing-data-and-code-ropensci-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2016-01-03-exemplars-of-distributing-data-and-code-ropensci.md :exports none :eval no :padline no
---
name: exemplars-of-distributing-data-and-code-ropensci
layout: post
title: Exemplars of distributing data and code - rOpenSci
date: 2016-01-03
categories:
- disentangle
- reproducible research pipelines
---

The practice of distributing data and code has a wide variety of possible approaches.  There are many resources available to be used to post data and code to the internet for dissemination, and it is very easy to access these resources.  It is more difficult to find exemplars of how data and code are easily and effectively distributed.  I am conducting a review of some of the resources that describe procedures for this and present exemplars in this and following notes.

A paper that describes the rOpenSci project's approach is [http://dx.doi.org/10.5334/jors.bu](http://dx.doi.org/10.5334/jors.bu):

``` 
Boettiger, C., Chamberlain, S., Hart, E., & Ram,
K. (2015). Building Software, Building Community: Lessons from the
rOpenSci Project. Journal of Open Research Software,
3(1). 
```

This paper is focused on the way the community development and capacity building part of the project was conducted.  The diagram shown below is introduced as an example of the style that rOpenSci recommend a data analysis workflow be constructed.

![/images/workflow-ropensci.png](/images/workflow-ropensci.png)

The focus on publishing data to a public repository so early in the project (prior to final analysis and manuscript) seems premature to me. But then, I do feel that I am somewhat more concerned with vexatious activity by climate skeptics than the rOpenSci team.

#+end_src    


** 2016-01-dd-exemplars-of-distributing-data-and-code-openair
*** head
#+name:exemplars-of-distributing-data-and-code-openair-header
#+begin_src markdown :tangle no :exports none :eval no :padline no
---
name: exemplars-of-distributing-data-and-code-openair
layout: post
title: Exemplars of distributing data and code: openair
date: 2016-01-dd
categories:
- disentangle
- reproducible research pipelines
---

#### Code:exemplars-of-distributing-data-and-code-openair
    
#+end_src
*** COMMENT import
#+name:import
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:import ####
  library(openair)
  marylebone  <-  importAURN(site = 'my1', year = 2000:2002)
  ## view first few lines
  head(marylebone)
  str(marylebone)
#+end_src

** 2016-01-12-we-have-a-statistically-rigorous-and-scientifically-meaningful-definition-of-replication
*** post
#+name:we-have-a-statistically rigorous and scientifically meaningful definition of replication-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2016-01-12-we-have-a-statistically-rigorous-and-scientifically-meaningful-definition-of-replication.md :exports none :eval no :padline no
---
name: we-have-a-statistically rigorous and scientifically meaningful definition of replication
layout: post
title: We have a statistically rigorous and scientifically meaningful definition of replication. Let's use it
date: 2016-01-12
categories:
- disentangle
- reproducible research
---



Researchers writing about the 'Reproducibility Crisis' often conflate
the terms reproducibility, repeatability and replicability, but it is
quite important to distinguish these.  There is a great discussion of
the distinction in the SimplyStatistics blog post titled: [We need a statistically rigorous and
scientifically meaningful definition of
replication](http://simplystatistics.org/2015/10/20/we-need-a-statistically-rigorous-and-scientifically-meaningful-definition-of-replication).
But I actually now think _we DO have that definition!_ It is the
that repeatability is the same
as replication and involves a new sample with new measurement errors
while reproducibility uses the same data to recalculate the result.

I think it is vital that we labour the point so that the
distinction between repeatability and reproducibility is made clear.

I follow the definition that 'reproducible' is exact re-computation
whereas repeatable/replicable is a new analysis, of a new sample,
yielding a new result (plus or minus some variance from measurement
error), and if the original study is replicated then the same
conclusions are reached from the new analysis.

I used this paragraph and recent reference in my thesis: 

```
Reproducibility is defined as ‘the ability to recompute data analytic
results given an observed dataset and knowledge of the data analysis
pipeline’ (Leek & Peng 2015). This definition distinguishes
reproducibility from replicability which is ‘the chance that an
independent experiment targeting the same scientific question will
produce a consistent result’ (Leek & Peng 2015).  

Leek, J.T. & Peng, R.D. (2015). Opinion: Reproducible research can
still be wrong: Adopting a prevention approach. Proceedings of the
National Academy of Sciences of the United States of America, 112(6),
1645–1646.  
```

<p></p>


But I am also a big fan of the the definitions in Peng 2011 and Cassey 2006.

- Peng 2011:

```
With replication, independent investigators address a scientific
hypothesis and build up evidence for or against it...

Reproducibility calls for the data and computer code used to analyze
the data be made available to others. This standard falls short of
full replication because the same data are analysed again, rather than
analysing independently collected data.

Peng, R. D. (2011). Reproducible research in computational
science. Science, 334(6060), 1226–1227. doi:10.1126/science.1213847

```

<p></p>

- Cassey 2006:

```
[For a repeatable study] a third party must be able to perform a study
using identical methodological proto- cols and analyze the resulting
data in an identical manner... [Further] a published result
must be presented in a manner that allows for a quantitative
comparison in a later study...

We consider a study reproducible if, from the information presented in
the study, a third party could replicate the re- ported results
identically. 

CASSEY, P., & BLACKBURN, T. M. (2006). Reproducibility and
Repeatability in Ecology. BioScience,
56(12), 958. doi:10.1641/0006-3568
```

<p></p>

To fully endorse any scientific claims, the experimental findings should be completely repeatable by many independent investigators who ‘address areplicated hypothesis and build up evidence for or against it’ (Peng, 2011). It is important to note that the exact results need not be computed in a repeatable study. This is because experimentation involves probability, and if performed again, with a different sample and new set of measurement errors some variance between experiments is to be expected. 

#+end_src


   



*** notes
Reproducibility versus repeatability
[I FOLLOW THE DEFINITIONS OF PENG 2011 AND CASSEY 2006 WHERE REPRODUCIBLE IS EXACT RE-COMPUTATION VS REPEATABLE/REPLICABLE WHICH IS A NEW SAMPLE, NEW RESULT
(+/- SOME VARIANCE), SAME CONCLUSION.  WE NEED TO AVOID THE CONFUSION MANY AUTHORS HAVE BETWEEN THESE. FOR EXAMPLE FREEDMAN BELOW DEFINES
IRREPRODUCIBILITY AS ‘ERRORS AND OMISSIONS THAT PREVENT REPLICATION/REPEATABILITY’.  
THEREFORE IMPLICITLY DEFINING REPRODUCIBILITY AS REPEATABILITY 
SEE
http://simplystatistics.org/2015/10/20/we-need-a-statistically-rigorous-and-scientifically-meaningful-definition-of-replication/]
Reproducibility is defined here as the case where a published result can be re-computed from the original datasets.  According to Cassey and Blackburn (2006), this is the case when ‘from the information presented in the study, a third party could replicate the reported results identically’. This definition distinguishes reproducibility from repeatability (also known as ‘replicability’) which is when ‘a third party must be able to perform a study using identical methodological protocols and analyze the resulting data in an identical manner’ (Cassey and Blackburn, 2006).



## Conflated and confused

Some researchers assert thatBut, often this is not the case, according to several studies only 10-30% of the published science articles are repeatable reproducible (Pritsker, 2012 [THIS IS A BLOG POST SUMMARISING THREE SECOND HAND/THIRD HAND REPORTS.  NOT A GREAT SOURCE. CAN WE FIND A GOOD PRIMARY REPORT?]).

Pritsker, M., 2012. Studies show only 10% of published science articles are reproducible. What is happening? http://www.jove.com/blog/2012/05/03/studies-show-only-10-of-published-science-articles-are-reproducible-what-is-happening [Accessed 12 Jan. 2016] (I DON’T THINK BLOG POSTS ARE GENERALLY A GOOD SOURCE, AND THIS IS A VERY LIGHTWEIGHT PIECE REALLY. I RECOMMEND REPLACING WITH AN ARTICLE IF WE CAN FIND ONE)


From the economic perspective, iIt is estimated US $28 billion per year is spent on preclinical research that is not repeatable reproducible and it is also reported that non-repeatability of publicationsirreproducibility ranges from 51% to 89% (Freedman et al., 2015).  In the same study, Freedman et al., estimated that the contribution of data analysis towards repeatability problemsirreproducibility is 26%.

* Writing
** environmental health risk assessment (simple)
- define the range of health effects and environmental exposures
- characterise the main health outcomes and exposure pathways
- discuss multiple interacting environmental and social factors
- the magnitude of public health impacts related to the issue (also the level of public concern)
- The epidemiological evidence of health effects related to the exposures of interest (in general terms) 
- Specific exposed populations and sensitive subgroups
- uncertainties or limitations of the literature are noted.
** reviewing lessons
#### Introduction
This post is an attempt to put together a standard framework for reviewing lessons.  I've been to numerous workshops, master classes, tutorials and lectures and always take ad hoc notes that I generally re-write and re-organise afterwards.  I hope to develop a clearer methodology for reviewing what I learnt in each lesson.

#### Context

- When:
Either just the date and time or include more context like the broader event such as a conference, season, public holidays.  

- Who:
The presenter will give a bio at the start so make notes, especially to identify what disciplinary perspective they come from.

- Where:
Includes the address, city, lecture theatre.  These are cues for memory.

- Why:
Both why is the presenter here talking and also why am I hear listening.  It is important to critically reflect on what I want to get out of this lesson.

- What:
What is this lesson all about?  This might start with a synopsis overview and will probably move on to a sequence of notes as the presenter's narrative unfolds, and my thoughts on the topic evolve.  

#### Three Stages: Thesis, Antithesis, Synthesis
A guiding principle I use for writing the 'What' section is the three stages: thesis, antithesis, synthesis.  I am not a philosopher so I don't know the proper use of these concepts in that discipline, but for me they are useful to structure my notes as I go through the process of a lesson.  Here is how I think of these stages:

- Thesis: 
This is where I might pick out the key topics that are being presented, and write down my prior knowledge and preconceptions about the topic.

- Antithesis:
What's the main message(s) of the presenter?  What are their priorities?  What secondary (surrogate) topics emerge around the main points?  If I bring questions to the lesson are they answered by the presenter?  If not why?

- Synthesis:
My new understanding of the topic.  

#### Other tools
Other things I use are:

- Mind maps: a central topic with a spiderweb of links extending out in a circle.
- TODO

* Blogging
** Jekyll
http://statistics.rainandrhino.org/2015/12/15/jekyll-r-blogger-knitr-hyde.html
