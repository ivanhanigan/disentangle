#+TITLE:Disentangle Things (overflow)
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----

* Data Documentation
** 2015-03-19-r-eml-to-mitigate-risks-in-morpho-metacat-data-publishing


#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-03-19-r-eml-to-mitigate-risks-in-morpho-metacat-data-publishing.md :exports none :eval no :padline no
---
name: r-eml-to-mitigate-risks-in-morpho-metacat-data-publishing
layout: post
title: Using the R EML software to mitigate risks in Morpho and Metacat data publishing
date: 2015-03-19
categories:
- morpho
- data documentation
---

# Introduction

- Over the last few months I have used software called Metacat as a Data Portal and Repository.  Metacat is server software which has been
developed by the Knowledge Network for Biocomplexity (KNB). 
- Metacat
conforms to the Ecological Metadata Language (EML) Standard ([https://knb.ecoinformatics.org/#external//emlparser/docs/index.html](https://knb.ecoinformatics.org/#external//emlparser/docs/index.html)).  
- KNB also develop another software package called Morpho to be used by Ecologists to document their data ([https://knb.ecoinformatics.org/#tools/morpho](https://knb.ecoinformatics.org/#tools)).
- Morpho can be used to send the data and metadata documents to be published on a Metacat portal. 
- KNB’s software is used internationally by the Data
Observation Network for Earth (DataONE) nodes, the United States Long
Term Ecological Research (US LTER) network and the International Long
Term Ecological Research (ILTER) network. 
- Additionally, the Australian Long Term Ecological Research Network 
Data Portal ([www.ltern.org.au/knb/](http://www.ltern.org.au/knb/)), Australian SuperSites Network and
Australian Centre for Ecological Analysis and Synthesis used
the same underlying technology to publish data packages.
- The Metacat system is great for a data repository but unfortunately (in my experience) the Morpho software package has repeatedly hampered data processing and increased risks of inadvertently publishing data with errors. 
- My colleagues and I workaround these problems using a lot of different 'fixes' for the different problems.
- Fortunately there is an alternative to Morpho in the R statistical software environment called the R-EML package ([https://github.com/ropensci/EML](https://github.com/ropensci/EML)).  This provides a library of functions used in the R language to generate and parse EML files.
- This new workflow mitigates some of the risks of the Morpho software by ensuring the data related steps of the workflow are conducted in the R environment for statical computing.
- However, some Issues remain in that this requires a fairly specialised computing environment with various Linux libraries configured appropriately


# Results
 
- I generate EML metadata using REML in the workflow shown in the figure below.

![altext](/images/workflow-rmd-md.png)

<p></p>

- Image adapted from [http://kieranhealy.org/blog/archives/2014/01/23/plain-text/](http://kieranhealy.org/blog/archives/2014/01/23/plain-text/)


#+end_src

** TODO xxx-adding-value-labels-to-reml-boilerplate
*** adding-value-labels-to-reml-boilerplate-header
#+begin_src R :tangle no :exports none :eval no :padline no
  ---
  name: adding-value-labels-to-reml-boilerplate
  layout: post
  title: adding-value-labels-to-reml-boilerplate
  date: 2014-08-22
  categories:
  -
  ---
  
  #### Code:adding-value-labels-to-reml-boilerplate
  #For sake of argument, imagine a data.frame looking something like this:
  
  
  dat = data.frame(river=c("SAC", "SAC", "AM"),
                   spp = c("king", "king", "ccho"),
                   stg = c("smolt", "parr", "smolt"),
                   ct =  c(293, 410, 210L))
  
  xtable::xtable(dat)
  
  
  
  #In case our column header abbreviations are not obvious to others (or our future selves), we take a moment to define them:
  
  
  col_metadata = c(river = "http://dbpedia.org/ontology/River",
                   spp = "http://dbpedia.org/ontology/Species",
                   stg = "Life history stage",
                   ct = "count")
  
  
  
  # Define the units used in each column.  In the case of factors, we define the abbreviations in a named string.
  
  
  unit_metadata =
    list(river = c(SAC = "The Sacramento River", AM = "The American River"),
         spp = c(king = "King Salmon", ccho = "Coho Salmon"),
         stg = c(parr = "third life stage", smolt = "fourth life stage"),
         ct = "number")
  
  # automated?
  #dat <- dat[,-4]
  dat
  
#+end_src

*** COMMENT reml_boilerplate1-code
#+name:reml_boilerplate
# begin_src R :session *R* :tangle R/reml_boilerplate.r :exports reports :eval no
#+begin_src R :session *R* :tangle no :exports reports :eval no

  ################################################################
  # name:reml_boilerplate
   
  # func
  ## if(!require(EML)) {
  ##   require(devtools)
  ##   install_github("EML", "ropensci")
  ##   } 
  ## require(EML)
  
  reml_boilerplate <- function(data_set, outfile = NA, created_by = "Ivan Hanigan <ivanhanigan@gmail.com>", data_dir = getwd(), titl = NA)
  {
  
    # next create a list from the data
    unit_defs <- list()
    for(i in 1:ncol(data_set))
      {
        # i = 4
        if(is.numeric(data_set[,i])){
          unit_defs[[i]] <- "number"
        } else {
          unit_defs[[i]] <- names(data_set)[i]
        }
      }
  
  # print helpful comments
  cat(
  sprintf('
  # you just got a cheater\'s unit_defs
  # we can get the col names easily
  col_defs <- names(dat)
  # then create a dataset with metadata
  ds <- data.set(dat,
                 col.defs = col_defs,
                 unit.defs = unit_defs
                 )
  # now write EML metadata file
  eml_config(creator="%s")
  eml_write(ds,
            file = "%s",
            title = "%s"
            )
  
  # now your metadata has been created
  # if you want to add this to morpho and metacat it will needs something like
  </dataFormat>
    <distribution scope="document">
      <online>
        <url function="download">ecogrid://knb/hanigan.34.1</url>
      </online>
    </distribution>
  </physical>', created_by, outfile, titl)
  )
  
  
    return(unit_defs)
  
   }
#+end_src

*** COMMENT reml_boilerplate2-code
#+name:get_vals
# begin_src R :session *R* :tangle R/reml_boilerplate.r :exports none :eval no
#+begin_src R :session *R* :tangle no :exports none :eval no
  reml_boilerplate <- function(.dataframe){
  strng <- list()
  for(i in 1:ncol(.dataframe)){
    # i = 6
    .variable <- names(.dataframe)[i]
    #.dataframe[,.variable]
      if(is.character(.dataframe[ ,.variable])){
        .dataframe[,.variable]  <- factor(.dataframe[,.variable])
      }
  
    if(is.factor(.dataframe[,.variable])){
      x <- .dataframe[,.variable]
      vals <-  names(table(x))
      # symbols may pollute the string to parse
      vals <- make.names(vals)
      vals <- tolower(vals)  
      vals <- gsub("\\.","_",vals)
      vals <- gsub("_+","_",vals)    
      v <- .variable
      #v
      strng[[.variable]] <- paste(
      v, ' = c(',
      paste(vals, sep = '', collapse = ' = "TBA",')
      ,' = "TBA")', sep = '')
    } else if(is.numeric(.dataframe[,.variable])){
      v <- .variable
      strng[[.variable]] <- paste(v,' = "number"',sep='')
  #    strng[[.variable]] <- '"number"'
      
    } else if(
      !all(is.na(as.Date(as.character(na.omit(.dataframe[,.variable])), origin = "1970-01-01")))
      ){
      v <- .variable    
      strng[[.variable]] <- paste(v,' = "YYYY-MM-DD"',sep='')
  #    strng[[.variable]] <- '"YYYY-MM-DD"'
  
    } else if (all(is.na(.dataframe[ ,.variable]))){
      v <- .variable        
      strng[[.variable]] <- paste(v,' = "', names(.dataframe)[i], '"', sep='')
    }
  }  
  #strng
  strng2 <- ""
  for(n in 1:(length(strng)-1)){
    strng2 <- paste(strng2, strng[[n]], ",\n")
  }
  strng2 <- paste(strng2, strng[[length(strng)]], "\n")
  #cat(strng2)
  strng3 <- paste("
  unit_metadata =
    list(",strng2,")", sep = "")
  #cat(strng3)
  eval(parse(text = strng3))
  #unit_metadata
  return(unit_metadata)
  }
  
  #u1 <- get_vals(analyte)
  #u1
      
#+end_src
*** COMMENT reml_boilerplate3-code
#+name:get_vals
#+begin_src R :session *R* :tangle R/reml_boilerplate.r :exports none :eval no
  
  reml_boilerplate <- function(.dataframe, enumerated = NA){
  strng <- list()
  for(i in 1:ncol(.dataframe)){
  # i = 1
    .variable <- names(.dataframe)[i]
    #.dataframe[,.variable]
      if(is.character(.dataframe[ ,.variable])){
        .dataframe[,.variable]  <- factor(.dataframe[,.variable])
      }
  
    if(is.factor(.dataframe[,.variable])  & i %in% enumerated){
      x <- .dataframe[,.variable]
      vals <-  names(table(x))
      # symbols may pollute the string to parse
      vals <- make.names(vals)
      vals <- tolower(vals)  
      vals <- gsub("\\.","_",vals)
      vals <- gsub("_+","_",vals)    
      v <- .variable
      #v
      strng[[.variable]] <- paste(
      v, ' = c(',
      paste(vals, sep = '', collapse = ' = "TBA",')
      ,' = "TBA")', sep = '')
    } else if(is.factor(.dataframe[,.variable])){
      
      strng[[.variable]] <- paste(
        .variable, ' = "TBA"', sep = ''
        )
  
    } else if(is.numeric(.dataframe[,.variable])){
      v <- .variable
      strng[[.variable]] <- paste(v,' = "number"',sep='')
  #    strng[[.variable]] <- '"number"'
      
    } else if(
      !all(is.na(as.Date(as.character(na.omit(.dataframe[,.variable])), origin = "1970-01-01")))
      ){
      v <- .variable    
      strng[[.variable]] <- paste(v,' = "YYYY-MM-DD"',sep='')
  #    strng[[.variable]] <- '"YYYY-MM-DD"'
  
    } else if (all(is.na(.dataframe[ ,.variable]))){
      v <- .variable        
      strng[[.variable]] <- paste(v,' = "', names(.dataframe)[i], '"', sep='')
    }
  }  
  #strng
  strng2 <- ""
  for(n in 1:(length(strng)-1)){
    strng2 <- paste(strng2, strng[[n]], ",\n")
  }
  strng2 <- paste(strng2, strng[[length(strng)]], "\n")
  #cat(strng2)
  strng3 <- paste("
  unit_metadata =
    list(",strng2,")", sep = "")
  #cat(strng3)
  eval(parse(text = strng3))
  #unit_metadata
  return(unit_metadata)
  }
  
  #u1 <- get_vals(analyte)
  #u1
      
#+end_src


*** reml_boilerplate-test-code
#+name:reml_boilerplate-test
#+begin_src R :session *R* :tangle no :exports reports :eval no
  ################################################################
  # name:reml_boilerplate-test
  library(EML)
  require(devtools)
  load_all()
  #require(disentangle)
  fpath <- system.file("extdata/civst_gend_sector_additional_variables.csv", package = "disentangle")
  analyte <- read.csv(fpath, colClasses=c(ordinal_variable = "character"))
  analyte <- read.csv(fpath, stringsAsFactor = F)
  
  str(analyte)
  analyte$datevar <- as.Date(analyte$datevar)
  analyte$ordinal_variable <- as.character(analyte$ordinal_variable)
  analyte$fractions <- sample(rnorm(1000,0.1,0.1), nrow(analyte))
  
  str(analyte)
  head(analyte)
  unit_defs <- reml_boilerplate(
    data_set = analyte,
    created_by = "Ivan Hanigan <ivanhanigan@gmail.com>",
    titl = "civst_gend_sector_full",
    outfile = 'testing1234'
    )
  unit_defs
  analyte2 <- analyte
  names(analyte)
  #analyte <- analyte2[,-c(6)]
  unit_defs <- reml_boilerplate2(analyte)
  unit_defs
  
  unit_defs <- reml_boilerplate3(analyte, enumerated = c(1,2,3,8))
  unit_defs
  
  
  
  
    # we can get the col names easily
    col_defs <- names(analyte)
    # then create a dataset with metadata
    ds <- data.set(analyte,
                   col.defs = col_defs,
                   unit.defs = unit_defs
                   )
    # now write EML metadata file
    eml_config(creator="TBA <fakeaddress@gmail.com>")
  outfile <- "testingAbcd12234.csv"
  str(ds)
    ## eml_write(ds,
    ##           file = gsub(".csv", "_eml_skeleton.xml", outfile),
    ##           title = gsub(".csv", "", outfile)
    ##           )
  ## [1] "testingAbcd12234_eml_skeleton.xml"
  ## Warning message:
  ## In `[[<-.data.frame`(`*tmp*`, i, value = c("2013-09-09/01/13", "2013-09-09/01/13",  :
  ##   Setting class(x) to NULL;   result will no longer be an S4 object
  ## >
  eml_write(analyte,
            col.defs = col_defs,
            unit.defs = unit_defs,
            creator="TBA <fakeaddress@gmail.com>",
            file = gsub(".csv", "_eml_skeleton.xml", outfile)
            )
  
  tempfile <- dir(pattern="^data_table_")
  tempfile
  file.rename(tempfile, outfile)
  # rename the CSV file.
  
  # dir("data")
#+end_src

*** COMMENT reml_boilerplate new test-code
#+name:reml_boilerplate new test
#+begin_src R :session *R* :tangle no :exports none :eval no
#### name:reml_boilerplate new test####
install.packages("dlnm")
library(dlnm)
library(disentangle)
library(EML)

data(chicagoNMMAPS)
str(chicagoNMMAPS)
?chicagoNMMAPS
data_dictionary(chicagoNMMAPS)
unit_defs <- reml_boilerplate(chicagoNMMAPS)
col_defs <- names(chicagoNMMAPS)
ds <- eml_dataTable(chicagoNMMAPS,
              col.defs = col_defs,
              unit.defs = unit_defs,
              description = "Metadata documentation for chic.csv", 
              filename = "chic.csv")
# now write EML metadata file
eml_config(creator="Antonio Gasparrini
<antonio.gasparrini@lshtm.ac.uk>")
eml_write(ds,
          file = "chic.xml",
          title = "chicagoNMMAPS"
)

# now your metadata has been created

#+end_src

* Data Operation
*** COMMENT aggregate-list-of-dfs
#+name:aggregate
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:aggregate####
  # stoopid hack for lu
  # desc
  "I have a do.call question that perhaps you can answer in 5 seconds... [my brain is leaking... I can't see the solution].
  
  I have a list of data.frames imagine I wanted to sum them up, like d1+d2+... N. If I do that manually the output is a data.frame, but when I use do.call I get one number, and I need a data.frame instead.
  
  Another issues is that some columns have a factor, so I'm planning to wrap my operation into a function with a condition to do the sum if the cell contains a number. Other to skip it.
  so I though of:
  
  foo = function (x) { ifelse(is.numeric(x), sum(x), NA)} #but it doesn't work
  
  This example addressed the issues.
  
  d1 = mtcars
  d2 = d1*2
  
  str(mtcars)
  l1 = list(d1,d2)
  str(l1)
  
  d3 = do.call('sum', l1)
  #> d3
  #[1] 41826.61 # I don't want one number, but a data.frame with the sums.
  
  foo = function (x) { ifelse(is.numeric(x), sum(x), NA)}
  d4 = do.call('foo', l1)
  
  Any help or tip will be welcome!"
  
  # I feel like it should be simple because R can add dataframes
  # but maybe these need to be matrices (because a data.frame is really
  # a list)
  d1 = as.matrix(mtcars)
  d1
  d2 = as.matrix(d1*2)
  d2
  d1 + d2
  # lets make more dfs
  d3 = as.matrix(d2*3)
  d4 = as.matrix(d3*2)
  d1+d2+d3+d4
  # I note that do.call can add two df fine
  l1  <- list(d1, d2)
  out  <- do.call("+", l1)
  str(out)
  out
  # but with multiple no good
  l1 = list(d1,d2, d3, d4)
  str(l1)
  out  <- do.call("+", l1)
  # when these are dataframes the error is
  ## Error in `+`(list(mpg = c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4,  :
  ##   unused arguments (list(mpg = c(... blah blah
  # when matrices the error is
  ## Error in `+`(c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2,  :
  ##   operator needs one or two arguments
  
  
  # But the lists content are able to be combined if done individually
  out1 <- l1[[1]] + l1[[2]] + l1[[3]] + l1[[4]]
  str(out1)
  out1
  
  # make things interesting
  l1 <- list()
  for(i in 1:1000){
    #i = 1
    di  <- iris
    di[sapply(iris, is.numeric)]  <- iris[sapply(iris, is.numeric)] * sample(rnorm(100,1,2), 1)
    l1[[i]]  <- di
  }
  str(l1)
  # so we could figure out a way to iterate over the dataframes with a
  # loop like
  ## for(i in 1:length(l1)){
  ##   if(i == 1){
  ##     out2 <- l1[[i]]
  ##     next
  ##   } else {
  ##     out2  <- out2 + l1[[i]]
  ##   }
  ## }
  # doesn't like factor
  #out2
  # or 
  ## d1 <- iris
  ## str(d1) # Note pesky factor column
  ## d2 <- d1
  ## # Only multiply cells that are numeric
  ## str(d2[sapply(d2, is.numeric)])
  ## d2[sapply(d2, is.numeric)] <- d2[sapply(d2, is.numeric)] * 2
  
  ## # Add another data.frame to make it interesting
  ## d3 <- d1
  ## d3[sapply(d3, is.numeric)] <- d3[sapply(d3, is.numeric)] * 3
  
  
  ## l1 = list(d1,d2, d3)
  str(l1)
  
  # Inititalise output dataframe to zero
  
  # Sum data frames
  summarise_list_dfs3 <- function(listed){
    d.results <- d1
    d.results[sapply(d.results, is.numeric)] <- 0 
    for (i in seq_along(l1)){
      d.results[sapply(d.results, is.numeric)] <-
        d.results[sapply(d.results, is.numeric)] +
        l1[[i]][sapply(l1[[i]], is.numeric)]
    }
    return(d.results)
  }
  system.time(d.results  <- summarise_list_dfs3(l1))
  str(d.results)
  
  # but we want an elegant solution that will be
  # able to give any number of dataframes, and also have
  # the issue of some variables being factor so not to be used
  
  # let's write some func
  # I like SQL for it's clarity
  library(sqldf)
  # we are basically grouping the values of each row in each df, so add
  # an id
  
  nam <- function(x){
    x$row_names <- 1:nrow(x)
    return(x)
  }
  l_df2 <- lapply(l1, nam)
  str(l_df2)
  # now construct some sql and run it.  let's make it flexible for
  # different summarising functions like sum, mean, median, stdev etc
  summarise_list_of_dfs <- function(
    list_of_dfs = l1
    ,
    summarise_fun = 'sum'
    ,
    id = 'row_names'
    ){
    if(!is.data.frame(list_of_dfs[[1]])) list_of_dfs <- lapply(list_of_dfs, as.data.frame)
    l_df2 <- lapply(list_of_dfs, nam)
    x = do.call('rbind.data.frame', l_df2)
  names(x)<-gsub("\\.", "_", names(x))
    todo <- sapply(x, 'is.numeric')
    todo <- names(x)[todo]
    todo <- todo[-which(todo == "row_names")]
    oper  <- sprintf('), %s(',summarise_fun)
    sql<-paste("sum(",paste(todo, sep = '', collapse = oper),")", sep = "")
  #sql
    out<-sqldf(
    #cat(
    sprintf("select row_names, %s
    from x
    group by row_names
    order by row_names
    ", sql)
    )
    names(x)<-gsub("_", "\\.", names(x))
    return(out)
  }
  
  # do
  #l1 <- lapply(l1, as.data.frame)
  
  system.time(qc  <- summarise_list_of_dfs(l1))
  
  str(qc)
  str(out1)
  out1  <- as.data.frame(out1)
  names(qc)<-names(out1)
  # same?
  identical(qc, out1)
  # not identical
  all(qc == out1)
  # all values are equal tho
  
  # compare to base r
  summarise_list_dfs2 <- function(listed){
    listed  <- l1
    d5=do.call('rbind',listed)
  #  str(d5)
    d5.rnames =rep(rownames(listed[[1]]),times=length(listed))
    d5$reg=d5.rnames
    d6 = aggregate(d5[sapply(d5, is.numeric)], by=list(d5$reg), FUN=sum) # I should use an
                                            # ifelse, to do this only to
                                            # numeric columns.
    return(d6)
  }
  system.time(d6 <- summarise_list_dfs2(l1))
  
  str(d6)
  row.names(d6) <- d6$Group.1
  # same?
  identical(d6, out1[sort(row.names(out1)),])
  # not identical
  all(
    d6[,-c(1)] == out1[sort(row.names(out1)),]
    )
  # looks like the integers are equal but not the doubles.
  # but a diff of the two looks same
  
  # now does it work for  mean?
  qc  <- summarise_list_of_dfs(l1, 'mean')
  # whoops, sql uses 'avg'
  qc  <- summarise_list_of_dfs(l1, 'avg')
  str(qc)
  # and stdev
  qc  <- summarise_list_of_dfs(l1, 'stdev')
  str(qc)
  # this did multiple dataframes, now try with factor variables
  str(l1)
  d1[,3] <- as.factor(d1[,3])
  d2[,3] <- as.factor(d2[,3])
  d3[,3] <- as.factor(d3[,3])
  d4[,3] <- as.factor(d4[,3])
  
  l1 = list(d1,d2, d3, d4)
  str(l1)
  qc  <- summarise_list_of_dfs(l1)
  str(qc)
  # it has just skipped that variable so this should be good to go
  # NB if the combination of all the dataframes is too big for RAM then
  # R will fail. in this case I would
  # put into a PostgreSQL database as this will use the disk rather than
  # RAM
  # this can also be used to add indexes and clustering functions to
  # speed up the calculations.
  # HTH, let me know if there is a more efficient R solution?
  
  install.packages("data.table")
  library(data.table)
  sum_ldf4 <- function(
    listed=l1
    ,
    summarise_fun  = 'sum'
                       ){
    d5=do.call('rbind',listed)
    d5.rnames =rep(rownames(listed[[1]]),times=length(listed))
    d5 <- d5[sapply(d5, is.numeric)]
    d5$reg=d5.rnames
    oper  <- sprintf('), %s(',summarise_fun)
    todo <- sapply(d5, 'is.numeric')
    todo <- names(d5)[todo] 
    sql<-paste("sum(",paste(todo, sep = '', collapse = oper),")", sep = "")
    #sql
      
    DT = data.table(d5)
    
    out <-  eval(parse(
        text = sprintf("DT[,list(%s),by=list(d5$reg)]", sql)
        ))
      
  
    return(out)
  }
  
  
  # sqldf
  o1 <- system.time(qc  <- summarise_list_of_dfs(l1))
  str(qc)
  # aggregate
  o2 <- system.time(d6 <- summarise_list_dfs2(l1))
  str(d6)
  # loop
  o3 <- system.time(d.results  <- summarise_list_dfs3(l1))
  str(d.results)
  # data.table
  o4 <- system.time(datab  <- sum_ldf4(l1))
  str(datab)
  
  # http://zvfak.blogspot.com.au/2011/03/applying-functions-on-groups-sqldf-plyr.html
  library(gplots)
  x<-c(o1[3],o2[3],o3[3],o4[3])
  balloonplot( rep("time.elapsed",5),c("sqldf","aggregate","loop", "data.table"),round(x,1), ylab ="Method", xlab="",sorted=F,dotcolor=rev(heat.colors(5)),main="time.elapsed for different methods") 
#+end_src

*** COMMENT aggregate-list-of-dfs-asRNW-code
#+name:aggregate-list-of-dfs-asRNW
#+begin_src R :session *R* :tangle no :exports none :eval no
#### name:aggregate-list-of-dfs-asRNW####
---
title: "aggregate list of dfs"
author: "Ivan C Hanigan"
date: "7/2/2015"
output: html_document
---

```{r}
#### name:aggregate####
# stoopid hack for lu
# desc
"I have a do.call question that perhaps you can answer in 5 seconds... [my brain is leaking... I can't see the solution].

I have a list of data.frames imagine I wanted to sum them up, like d1+d2+... N. If I do that manually the output is a data.frame, but when I use do.call I get one number, and I need a data.frame instead.

Another issues is that some columns have a factor, so I'm planning to wrap my operation into a function with a condition to do the sum if the cell contains a number. Other to skip it.
so I though of:

foo = function (x) { ifelse(is.numeric(x), sum(x), NA)} #but it doesn't work

This example addressed the issues.

d1 = mtcars
d2 = d1*2

str(mtcars)
l1 = list(d1,d2)
str(l1)

d3 = do.call('sum', l1)
#> d3
#[1] 41826.61 # I don't want one number, but a data.frame with the sums.

foo = function (x) { ifelse(is.numeric(x), sum(x), NA)}
d4 = do.call('foo', l1)

Any help or tip will be welcome!"

# I feel like it should be simple because R can add dataframes
# but maybe these need to be matrices (because a data.frame is really
# a list)
d1 = as.matrix(mtcars)
d1
d2 = as.matrix(d1*2)
d2
d1 + d2
# lets make more dfs
d3 = as.matrix(d2*3)
d4 = as.matrix(d3*2)
d1+d2+d3+d4
# I note that do.call can add two df fine
l1  <- list(d1, d2)
out  <- do.call("+", l1)
str(out)
out
# but with multiple no good
l1 = list(d1,d2, d3, d4)
str(l1)
#out  <- do.call("+", l1)
# when these are dataframes the error is
## Error in `+`(list(mpg = c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4,  :
##   unused arguments (list(mpg = c(... blah blah
# when matrices the error is
## Error in `+`(c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2,  :
##   operator needs one or two arguments


# But the lists content are able to be combined if done individually
out1 <- l1[[1]] + l1[[2]] + l1[[3]] + l1[[4]]
str(out1)
out1

# make things interesting
l1 <- list()
for(i in 1:1000){
  #i = 1
  di  <- iris
  di[sapply(iris, is.numeric)]  <- iris[sapply(iris, is.numeric)] * sample(rnorm(100,1,2), 1)
  l1[[i]]  <- di
}
str(l1)
# so we could figure out a way to iterate over the dataframes with a
# loop like
## for(i in 1:length(l1)){
##   if(i == 1){
##     out2 <- l1[[i]]
##     next
##   } else {
##     out2  <- out2 + l1[[i]]
##   }
## }
# doesn't like factor
#out2
# or 
## d1 <- iris
## str(d1) # Note pesky factor column
## d2 <- d1
## # Only multiply cells that are numeric
## str(d2[sapply(d2, is.numeric)])
## d2[sapply(d2, is.numeric)] <- d2[sapply(d2, is.numeric)] * 2

## # Add another data.frame to make it interesting
## d3 <- d1
## d3[sapply(d3, is.numeric)] <- d3[sapply(d3, is.numeric)] * 3


## l1 = list(d1,d2, d3)
str(l1)

# Inititalise output dataframe to zero

# Sum data frames
summarise_list_dfs3 <- function(
  listed=l1
  ){
  d.results <- listed[[1]]
  d.results[sapply(d.results, is.numeric)] <- 0 
  for (i in seq_along(listed)){
    d.results[sapply(d.results, is.numeric)] <-
      d.results[sapply(d.results, is.numeric)] +
      listed[[i]][sapply(listed[[i]], is.numeric)]
  }
  return(d.results)
}
system.time(d.results  <- summarise_list_dfs3(l1))
str(d.results)

# but we want an elegant solution that will be
# able to give any number of dataframes, and also have
# the issue of some variables being factor so not to be used

# let's write some func
# I like SQL for it's clarity
library(sqldf)
# we are basically grouping the values of each row in each df, so add
# an id

nam <- function(x){
  x$row_names <- 1:nrow(x)
  return(x)
}
l_df2 <- lapply(l1, nam)
str(l_df2)
# now construct some sql and run it.  let's make it flexible for
# different summarising functions like sum, mean, median, stdev etc
summarise_list_of_dfs <- function(
  list_of_dfs = l1
  ,
  summarise_fun = 'sum'
  ,
  id = 'row_names'
  ){
  if(!is.data.frame(list_of_dfs[[1]])) list_of_dfs <- lapply(list_of_dfs, as.data.frame)
  l_df2 <- lapply(list_of_dfs, nam)
  x = do.call('rbind.data.frame', l_df2)
names(x)<-gsub("\\.", "_", names(x))
  todo <- sapply(x, 'is.numeric')
  todo <- names(x)[todo]
  todo <- todo[-which(todo == "row_names")]
  oper  <- sprintf('), %s(',summarise_fun)
  sql<-paste("sum(",paste(todo, sep = '', collapse = oper),")", sep = "")
#sql
  out<-sqldf(
  #cat(
  sprintf("select row_names, %s
  from x
  group by row_names
  order by row_names
  ", sql)
  )
  names(x)<-gsub("_", "\\.", names(x))
  return(out)
}

# do
#l1 <- lapply(l1, as.data.frame)

system.time(qc  <- summarise_list_of_dfs(l1))

str(qc)
str(out1)
out1  <- as.data.frame(out1)
#names(qc)<-names(out1)
# same?
#identical(qc, out1)
# not identical
#all(qc == out1)
# all values are equal tho

# compare to base r
summarise_list_dfs2 <- function(listed){
  #listed  <- l1
  d5=do.call('rbind',listed)
#  str(d5)
  d5.rnames =rep(rownames(listed[[1]]),times=length(listed))
  d5$reg=d5.rnames
  d6 = aggregate(d5[sapply(d5, is.numeric)], by=list(d5$reg), FUN=sum) # I should use an
                                          # ifelse, to do this only to
                                          # numeric columns.
  return(d6)
}
system.time(d6 <- summarise_list_dfs2(l1))

str(d6)
row.names(d6) <- d6$Group.1
# same?
#identical(d6, out1[sort(row.names(out1)),])
# not identical
#all(
#  d6[,-c(1)] == out1[sort(row.names(out1)),]
#  )
# looks like the integers are equal but not the doubles.
# but a diff of the two looks same

# now does it work for  mean?
#qc  <- summarise_list_of_dfs(l1, 'mean')
# whoops, sql uses 'avg'
qc  <- summarise_list_of_dfs(l1, 'avg')
str(qc)
# and stdev
qc  <- summarise_list_of_dfs(l1, 'stdev')
str(qc)
# this did multiple dataframes, now try with factor variables
str(l1)
d1[,3] <- as.factor(d1[,3])
d2[,3] <- as.factor(d2[,3])
d3[,3] <- as.factor(d3[,3])
d4[,3] <- as.factor(d4[,3])

l1 = list(d1,d2, d3, d4)
str(l1)
qc  <- summarise_list_of_dfs(l1)
str(qc)
# it has just skipped that variable so this should be good to go
# NB if the combination of all the dataframes is too big for RAM then
# R will fail. in this case I would
# put into a PostgreSQL database as this will use the disk rather than
# RAM
# this can also be used to add indexes and clustering functions to
# speed up the calculations.
# HTH, let me know if there is a more efficient R solution?

#install.packages("data.table")
library(data.table)
sum_ldf4 <- function(
  listed=l1
  ,
  summarise_fun  = 'sum'
                     ){
  d5=do.call('rbind',listed)
  d5.rnames =rep(rownames(listed[[1]]),times=length(listed))
  d5 <- d5[sapply(d5, is.numeric)]
  d5$reg=d5.rnames
  oper  <- sprintf('), %s(',summarise_fun)
  todo <- sapply(d5, 'is.numeric')
  todo <- names(d5)[todo] 
  sql<-paste("sum(",paste(todo, sep = '', collapse = oper),")", sep = "")
  #sql
    
  DT = data.table(d5)
  
  out <-  eval(parse(
      text = sprintf("DT[,list(%s),by=list(d5$reg)]", sql)
      ))
    

  return(out)
}


# sqldf
o1 <- system.time(qc  <- summarise_list_of_dfs(l1))
str(qc)
# aggregate
o2 <- system.time(d6 <- summarise_list_dfs2(l1))
str(d6)
# loop
o3 <- system.time(d.results  <- summarise_list_dfs3(l1))
str(d.results)
# data.table
o4 <- system.time(datab  <- sum_ldf4(l1))
str(datab)

# http://zvfak.blogspot.com.au/2011/03/applying-functions-on-groups-sqldf-plyr.html
library(gplots)
x<-c(o1[3],o2[3],o3[3],o4[3])
balloonplot( rep("time.elapsed",5),c("sqldf","aggregate","loop", "data.table"),round(x,1), ylab ="Method", xlab="",sorted=F,dotcolor=rev(heat.colors(5)),main="time.elapsed for different methods") 

sessionInfo()
```



#+end_src

* Graphical User Interfaces
** web2py


*** TODO using the appadmin interface

query can be db.dataset.contact.like('Lach%')

* Project Management
** 2015-03-15-tuftes-gantt-alternative-for-detail-within-context

*** blog
  
  #### Blocker:
      property which allows you to state that a task depends on either
      a previous sibling ("previous-sibling") or
      any other task by stating the task_id property of the predecessor

#+begin_src R :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-03-15-tuftes-gantt-alternative-for-detail-within-context.md :exports none :eval no :padline no
  ---
  name: tuftes-gantt-alternative-for-detail-within-context 
  layout: post
  title: tuftes-gantt-alternative-for-detail-within-context 
  date: 2015-03-15
  categories:
  - project management
  ---
  
  - During the end of 2014 I found that the Gantt Chart by TaskJuggler was a struggle to really achieve any decent task management with (fine for higher level overviews though).
  -   I had been following the approach described at [this link](http://orgmode.org/worg/org-tutorials/org-taskjuggler.html)
  - I decided to code up an alternative based on the theory explained on [this link](http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=000076) 
  
  
  #### Project Management Graphics (or Gantt Charts), by Edward Tufte
      Computer screens are generally too small for an overview of big
      serious projects. Horizontal and vertical scrolling are necessary to
      see more than about 40 horizontal time lines for a reasonable period
      of time. Thus, for large projects, print out the sequence on a big
      roll of paper and put it up on a wall.
       
      The chart might be retrospective as well as prospective. That is, the
      chart should show actualdates of achieved goals, evidence which will
      continuously reinforce a reality principle on the mythical future
      dates of goal achievement.
       
      Most of the Gantt charts are analytically thin, too simple, and lack
      substantive detail. The charts should be more intense. At a minimum,
      the charts should be annotated--for example, with to-do lists at
      particular points on the grid. Costs might also be included in
      appropriate cells of the table.
       
      About half the charts show their thin data in heavy grid prisons. For
      these charts the main visual statement is the administrative grid
      prison, not the actual tasks contained by the grid. No explicitly
      expressed grid is necessary--or use the ghost-grid graph
      paper. Degrid!
  
  #### The Results:
  
  I used the example for a fictional Journal Paper submission from my favourite reference for anything to do with Project Management:

      Aragon, T., Mier, H. M., Payauys, T., & Siador, C. (2012). 
      Project Management for Health Professionals.
    [http://www.academia.edu/1746564/Project_Management_for_Health_Professionals](http://www.academia.edu/1746564/Project_Management_for_Health_Professionals)    


  <p></p>

  With the following results (PS SVG format allows you to zoom in).

  ![alttext2](/images/gantt_tufte_test.svg)
  
  #### The codes: 
      library(disentangle)
      library(sqldf)
      library(lubridate)
      
      datin  <- read.csv(
      textConnection("
      container_task_title  , task_id                      , allocated , fte , blocker               ,       start_date , effort , status , notes 
      01 Start              , Start                        , ivan      ,   1 , NA                    ,       2015-03-15 ,     1d , DONE   , NA    
      02 Update Lit Review  , Repeat MEDLINE search        , ivan      ,   1 , Start                 ,       2015-03-16 ,     5d , DONE   , NA    
      02 Update Lit Review  , Retrieve articles            , ivan      ,   1 , Repeat MEDLINE search ,               NA ,     5d , DONE   , NA    
      02 Update Lit Review  , Read articles                , ivan      ,   1 ,                       ,       2015-03-26 ,    11d , DONE   ,       
      02 Update Lit Review  , Summarize articles           , ivan      ,   1 ,                       ,       2015-04-06 ,     9d , TODO   ,       
      03 Write Draft        , Write introduction           , ivan      ,   1 ,                       ,       2015-04-09 ,     6d , TODO   ,       
      03 Write Draft        , Write methods                , ivan      ,   1 , Start                 ,                  ,    15d , TODO   ,       
      03 Write Draft        , Write results                , ivan      ,   1 ,                       ,       2015-03-30 ,    10d , TODO   ,       
      03 Write Draft        , Write discussion             , ivan      ,   1 ,                       ,       2015-04-15 ,    10d , TODO   ,       
      04 Internal Review    , Send to co-author for review , ivan      ,   1 , Write discussion      ,                  ,     2d , TODO   ,        
      04 Internal Review    , Revise draft 1               , ivan      ,   1 ,                       ,       2015-04-19 ,    10d , TODO   ,       
      05 Peer Review        , Submit article 1             , ivan      ,   1 , Revise draft 1        ,                  ,     5d , TODO   ,       
      06 Revise and Resubmit, Revise draft 2               , ivan      ,   1 ,                       ,       2015-04-30 ,    10d , TODO   ,       
      06 Revise and Resubmit, Submit article 2             , ivan      ,   1 , Revise draft 2        ,                  ,     5d , TODO   ,       
      07 End                , Accepted                     , ivan      ,   1 ,                       ,       2015-05-15 ,     1d , TODO   ,       
      "),
      stringsAsFactor = F, strip.white = T)
      # or 
      # datin <- get_gantt_data("gantt_todo", test_data = T) # need to
      # adjust min_context_xrange to 2015-01-01 or something
      datin$start_date  <- as.Date(datin$start_date)
      str(datin)
      datin
      
      dat_out <- gantt_data_prep(dat_in = datin)
      str(dat_out)
      dat_out
      svg("tests/gantt_tufte_test.svg",height=10,width=8)
      gantt_tufte(dat_out, focal_date = "2015-04-13", time_box = 3*7,
                  min_context_xrange = "2015-03-16",
                  cex_context_ylab = 0.65, cex_context_xlab = .7,
                  cex_detail_ylab = 0.9,  cex_detail_xlab = .4,
                  show_today = F)
      dev.off()
      
  
  
  
      
#+end_src

** gantt_tufte
*** COMMENT R-gantt_tufte_test_data
#+name:gantt_tufte
#+begin_src R :session *R* :tangle no :exports none :eval yes
  # func
  library(sqldf)
  library(lubridate)
  library(swishdbtools)
  ch <- connect2postgres('localhost','gantt_tufte2', 'w2p_user', p='xpassword')
  pgListTables(ch, "public")
  
  # load
  datin  <- read.csv(textConnection("container_task_title, task_id, allocated, fte, blocker, start_date, effort
  Container 1, task 0, jim,   1,   ,     2014-12-01, 1m
  Container 1, task 1, jim,   1,   ,     2014-12-20, 1m
  Container 1, task 2, bob,   1, task 1,           , 10d 
  Container 2, task 3, sue,   1,   ,     2014-12-01, 2w
  Container 2, task 4, jim,   1, task 3,           , 2d
  Container 3, task 5, jimmy, 1, task 3,           , 10d
  Container 3, task 6, jimmy, 1,       , 2015-01-01, 10d
  Container 4, task 7, jimmy, 1, task 3,           , 10d
  "),
  stringsAsFactor = F, strip.white = T)
  datin$start_date  <- as.Date(datin$start_date)
  str(datin)
  datin
  
  cnt  <- sqldf("select container_task from datin group by container_task", drv = "SQLite")
  cnt$key_contact  <- NA
  cnt$abstract  <- NA
  cnt
  dbWriteTable(ch, "container_task", cnt, append = T)
  cnt  <- dbReadTable(ch, "container_task")
  cnt
  
  paste(  names(datin), sep = "", collapse = ", ")
  datin2  <- sqldf("select id as container_id, task_id, allocated, fte, blocker, start_date, effort
  from cnt
  join datin
  on cnt.container_task_title = datin.container_task", drv = "SQLite")
  datin2
  datin2$notes_issues  <- NA
  dbWriteTable(ch, "work_package", datin2, append = T)
  
  # psql got munteded, so revert to sqlite, tried swapping to sqlite, noto
  
  ## drv <- dbDriver("SQLite")
  ## tfile <- tempfile()
  ## con <- dbConnect(drv, dbname = "~/tools/web2py/applications/gantt_tufte/databases/storage.sqlite")
  ## dbListTables(con)
  ## datin2 <- dbGetQuery(con , "select * from work_package")
  ## dbWriteTable(ch, "work_package", datin2, append = T)
  
  
  # ended up deleteing from the applications folder
  
   
#+end_src

#+RESULTS: gantt_tufte
=1
==1
==1
==2
==2
==2
==2
==2
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==2
==1
==1
==1
==1
==1
==2
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==2
==2
==1
==1
==2
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==2
==1
==1
==2
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==2
==1
==1
==1
==TRUE
==1
==1
==1
==1
==1
==1
==1
==1
==1

*** COMMENT R-gantt_tufte_preprocessing
#+name:gantt_tufte
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################ 
  gantt_tufte_preprocessing  <- function(
    indat = datin
    ){
    # self join to collect the dependencies
    # paste(names(datint), sep = "", collapse = ", ")
    library(sqldf)
    library(lubridate)
    indat
    #indat$indat_id <- paste(indat$container_task, indat$task_id, sep = "_")
    # self join to return dependents
    indat2 <- sqldf("
    select t1.container_task,
    t1.task_id as predecessor,
    t2.task_id, t2.efforti,
    t1.end
    from indat t1
    left join
    indat t2
    on t1.task_id = t2.blocker
    
    ", drv = 'SQLite')
    #where t2.task_id is not null 
    indat2
    # get any other containers... not sure this helps
    indat2_1 <- sqldf("select t1.container_task, t1.predecessor, t2.predecessor as task_id,
    t2.efforti,
    t2.end
    from indat2 t1
    join
    indat2 t2
    where t1.predecessor = t2.task_id")
    indat2_1
    indat2$start  <- indat2$end 
    indat2$end  <- indat2$start + indat2$efforti
    indat2_1$start  <- indat2_1$end 
    indat2_1$end  <- indat2_1$start + indat2_1$efforti
    indat2  <- indat2[!is.na(indat2$start) & !is.na(indat2$end) ,]
    indat2
    indat2_1
    indat2 <- rbind(indat2, indat2_1)
    
    indat2 <- unique(indat2)
    # now you know the start of the dependents
    
    # now get other independent tasks
    indat3 <- sqldf("select container_task,
    task_id as predecessor,
    task_id,
    efforti,
    end, start
    from indat
    where start is not null
    ")
    # TODO at this point need to figure out how to get proper locs
    #indat3$loc <- nrow(indat3):1
    indat3
    indat2 
    # add loc of siblings
    ## indatx <- sqldf("select t1.*, t2.loc
    ## from indat2 t1
    ## left join
    ## indat3 t2
    ## where (t1.predecessor = t2.task_id)
    ## and t1.task_id is not null
    ## ")
    #indatx
    
    indat4 <- rbind(indat2, indat3)
    indat4 <- indat4[order(indat4$start),]
    indat4[order(indat4$container_task),]
    indat4 
    return(indat4)
  }
  datin2 <- indat4
  #datin2 <- gantt_tufte_preprocessing(datin)
  #str(datin2)
    
#+end_src

*** R timebox

#+name:timebox
#+begin_src R :session *R* :tangle R/timebox.R :exports none :eval yes
  #### name:timebox####
  # func to calculate time boxes
  timebox <- function(dat_in){
    # dat_in  <- datin
    if(
      !exists("dat_in$end_date")
      ) dat_in$end_date <- NA
    # str(dat_in)
    nameslist <- names(dat_in)
    dat_in$effortt <- as.numeric(gsub("[^\\d]+", "", dat_in$effort, perl=TRUE))
    dat_in$effortd <- gsub("d", 1, gsub("[[:digit:]]+", "", dat_in$effort, perl=TRUE))
    dat_in$effortd <- gsub("w", 7, dat_in$effortd)
    dat_in$effortd <- gsub("m", 30.5, dat_in$effortd)
    dat_in$effortd <- as.numeric(dat_in$effortd)
    dat_in$efforti <- dat_in$effortt * dat_in$effortd
    dat_in[is.na(dat_in$end_date),"end_date"] <- dat_in[is.na(dat_in$end_date),"start_date"] + dat_in[is.na(dat_in$end_date),"efforti"]
    dat_in$end_date  <- as.Date(dat_in$end_date, '1970-01-01')
    #   str(dat_in)
    dat_in <- dat_in[,c(nameslist, "efforti")]
    return(dat_in)
  }
  
#+end_src

#+RESULTS: timebox

*** get_gantt_data-code ETL
#+name:get_test_data
#+begin_src R :session *R* :tangle R/get_gantt_data.R :exports none :eval no
  
  library(sqldf)
  library(lubridate)
  library(swishdbtools)
  
  
  get_gantt_data <- function(
    dbname = 'gantt_todo'
    ,
    test_data = T
    ){
  if(test_data != TRUE){
  #### name:get_test_data####
  if(exists("ch"))  dbDisconnect(ch)
  ch <- connect2postgres2(dbname)
  
  datin  <- dbGetQuery(ch,
  "
  select t1.container_task_title, 
  t2.*
  from container_task t1
  join work_package t2
  on t1.id = t2.container_id
  where t2.status != 'DONTSHOW'
  order by container_task_title"
  )
  str(datin)
  datin_done  <- dbGetQuery(ch,
  "
  select t1.container_task_title, 
  t2.*
  from container_task_done t1
  join work_package_done t2
  on t1.id = t2.container_id
  where t2.status != 'DONTSHOW'
  "
  )
  str(datin_done)
  datin  <- rbind(datin, datin_done)
  } else {
  # or simpler
  datin  <- read.csv(textConnection("container_task_title, task_id, allocated, fte, blocker, start_date, effort, status, notes
    Container 1, task 0, jim,   1,   ,     2015-01-01, 1m  , DONE,  
    Container 1, task 1, jim,   1,   ,     2015-01-20, 1m  , DONE,  
    Container 1, task 2, bob,   1, task 1,           , 10d , TODO, This is a note 
    Container 2, task 3, sue,   1,   ,     2015-01-01, 2w  , TODO,  
    Container 2, task 4, jim,   1, task 3,           , 2d  , TODO,  
    Container 3, task 5, jimmy, 1, task 3,           , 10d , TODO,  
    Container 3, task 6, jimmy, 1,       , 2015-02-01, 10d , TODO,  
    Container 4, task 7, jimmy, 1, task 0,           , 10d , TODO,  
    Container 5, task 8, sue,   1,       , 2015-01-14, 5d  , TODO,  
    Container 5, task 9, sue,   1, task 8, , 2d            , TODO,  
    Container 5, task 10, sue,   1, task 9, , 2d           , TODO,  
    Container 5, task 11, sue,   1, task 10, , 2d          , TODO,  
    Container 5, task 12, sue,   1, task 11, , 2d          , TODO,  
    Container 5, task 13, sue,   1, task 12, , 2d          , TODO,  
    Container 5, task 14, sue,   1, task 13, , 2d          , TODO,  
    "),
    stringsAsFactor = F, strip.white = T)
    datin$start_date  <- as.Date(datin$start_date)
    str(datin)
    datin[datin$blocker == "","blocker"] <- NA
  # datin
  }
  
  return(datin)
  }

#+end_src

*** gantt_data_prep-code ETL
#+name:get_test_data
#+begin_src R :session *R* :tangle R/gantt_data_prep.R :exports none :eval no
  
  gantt_data_prep <- function(
    dat_in = datin
    ){
    dat_in <- timebox(dat_in)
    dat_in[1:5,c("task_id","start_date","end_date", "efforti")]
    str(dat_in)
    dat_in  <- dat_in[,c('container_task_title','task_id','allocated','fte','start_date','efforti','notes','status','blocker','end_date')]
    t(dat_in[1,])
    #dat_in
    # dbSendQuery(ch, "drop table indat")
    # dbWriteTable(ch, "indat", dat_in)
    
    indat <- dat_in
    dat_in_depends <- sqldf("
    select tab1.container_task_title, tab1.task_id, 
    'depends on ' || tab1.blocker || ' from Container ' || tab2.container_task_title as depends_on,
    tab2.end_date as start_date, 
    tab1.efforti, tab1.status
    from
    (
      select t1.container_task_title,
      t1.task_id, t1.blocker,
      t1.start_date,
      t1.end_date,
      t1.efforti, t1.status
      from indat t1
      where t1.blocker is not null
      ) tab1
    join
    indat tab2
    on tab1.blocker = tab2.task_id
    ", drv = "SQLite")
    # cast(tab2.end_date + (tab1.efforti || ' day')::INTERVAL as date) as
    # end_date
    dat_in_depends[1,]
    #dat_in_depends
    dat_in_depends$end_date  <- dat_in_depends$start_date + dat_in_depends$tab1.efforti
    names(dat_in_depends) <- gsub('tab1.', '', names(dat_in_depends))
    
    dat_in <- sqldf("
      select t1.container_task_title,
      t1.task_id, 
      t1.task_id as depends_on,  
      t1.start_date,
      t1.efforti,
      t1.status,
      t1.end_date
      from indat t1
      where t1.blocker is null or t1.blocker = ''
      order by container_task_title
    ", drv = 'SQLite')
    dat_in[,1]
    dat_in <- rbind(dat_in, dat_in_depends)
    dat_in[1,]
    #dat_in
    loc  <- sqldf("select container_task_title from dat_in group by container_task_title", drv = "SQLite")
    loc$loc  <- nrow(loc):1
    loc
    dat_in <- merge(loc, dat_in)
    str(dat_in)
    loc
    dat_out <- as.data.frame(matrix(NA, nrow = 0, ncol = ncol(dat_in) + 1))
    #names(qc) <- c(names(dat_in),"loc2")
    for(loci in loc$loc){
    # loci = loc$loc[1]
    qc <- dat_in[dat_in$loc == loci,]
    qc <- qc[order(qc$start_date),]
    loc2 <- seq(qc$loc[1]-1, qc$loc[1],  1/(length(qc$loc)))
    qc$loc2  <- loc2[(length(loc2)):2] 
    
    dat_out  <- rbind(dat_out, qc)
    
    }
    str(dat_out)
    return(dat_out)
    }
      
#+end_src

#+RESULTS: get_test_data

*** R-gantt_tufte PLOT
#+name:gantt_tufte
#+begin_src R :session *R* :tangle R/gantt_tufte.r :exports none :eval yes
  ################################################################
  # plot 
  
  gantt_tufte <- function(
    indat = dat_out
    ,
    smidge_lab = .15
    ,
    focal_date = '2015-01-18' # Sys.Date()
    , 
    show_today = TRUE
    ,
    time_box = 7 * 2.5
    ,
    end_task_ticks = F
    ,
    cex_context_ylab = 0.2
    ,
    cex_context_xlab = 0.5
    ,
    cex_context_points = 0.5
    ,
    min_context_xrange =  NA
    , 
    max_context_xrange = NA
    ,
    cex_detail_ylab = 0.7
    ,
    cex_detail_xlab = 1
    ,
    cex_detail_points = 0.7
    ,
    cex_detail_labels = 0.7
    ){
    focal_date <- as.Date(focal_date)
    m <- matrix(c(1,2), 2, 1)
    layout(m, widths=c(1), heights=c(.75,4))
    par(mar = c(3,16,2,1))
    # layout.show(2)
  
  
    yrange <- c((min(indat$loc2) - smidge_lab), (max(indat$loc2) + smidge_lab))
    if(!is.na(min_context_xrange)){
    xmin <- as.Date(min_context_xrange)    
    } else {
    xmin <- min(indat$start_date, na.rm = T)
    }
    if(!is.na(max_context_xrange)){
    xmax <- as.Date(max_context_xrange)    
    } else {
    xmax <- max(indat$start_date, na.rm = T)
    }
  
    xrange  <- c(xmin,xmax)
    
    # xrange
    #### context ####
    
    plot(xrange, yrange, type = 'n', xlab = "", ylab = "", axes = F )
    indat_lab  <- sqldf("select container_task_title, loc from indat group by container_task_title, loc", drv = "SQLite")
    mtext(c(indat_lab$container_task_title), 2, las =1, at = indat_lab$loc, cex = cex_context_ylab)
  
    polygon(c(focal_date, focal_date + time_box, focal_date + time_box, focal_date), c(rep(yrange[1],2), rep(yrange[2],2)), col = 'lightyellow', border = 'lightyellow')
  # DONE is grey
  indat_done <- indat[indat$status == 'DONE',]
    points(indat_done$start_date, indat_done$loc2, pch = 16, cex = cex_context_points, col = 'grey')
    #text(indat_done$start_date, indat_done$loc2 - smidge_lab, labels = indat_done$task_id, pos = 4)
    js <- indat_done$loc2
    for(i in 1:nrow(indat_done)){
    # = 1
      segments(indat_done$start_date[i] , js[i] , indat_done$start_date[i] , max(indat_done$loc2) + 1 , lty = 3, col = 'grey')
      segments(indat_done$start_date[i] , js[i] , indat_done$end_date[i] , js[i], col = 'grey')
    }
  # indat todo is black
  indat_todo <- indat[indat$status == 'TODO',]
    points(indat_todo$start_date, indat_todo$loc2, pch = 16, cex = cex_context_points)
    #text(indat_todo$start_date, indat_todo$loc2 - smidge_lab, labels = indat_todo$task_id, pos = 4)
    js <- indat_todo$loc2
    for(i in 1:nrow(indat_todo)){
    # = 1
      segments(indat_todo$start_date[i] , js[i] , indat_todo$start_date[i] , max(indat_todo$loc2) + 1 , lty = 3)
      segments(indat_todo$start_date[i] , js[i] , indat_todo$end_date[i] , js[i] )
    }  
    #segments(focal_date, yrange[1], focal_date, yrange[2], 'red')
    xstart_date <- ifelse(wday(xrange[1]) != 1, xrange[1] - (wday(xrange[1]) - 2), xrange[1])
    xend <- ifelse(wday(xrange[2]) != 7, xrange[2] + (5-wday(xrange[2])), xrange[2] )
    at_dates  <- seq(xstart_date, xend, 7)
    label_dates  <-
      paste(month(as.Date(at_dates, "1970-01-01"), label = T),
      day(as.Date(at_dates, "1970-01-01")),
      sep = "-")
  
    axis(1, at = at_dates, labels = label_dates, cex.axis = cex_context_xlab)
    #axis(3)
    if(show_today) segments(Sys.Date(), min(js), Sys.Date(), max(js), lty = 2, col = 'blue')
    
    #### detail ####
    js <- indat$loc2
    # todo
    plot(c(focal_date, focal_date + time_box), yrange, type = 'n', xlab = "", ylab = "", axes = F)
         
    mtext(c(indat_lab$container_task_title), 2, las =1, at = indat_lab$loc, cex = cex_detail_ylab)
    points(indat$start_date, indat$loc2, pch = 16, cex = cex_detail_points)
    text(indat$start_date, indat$loc2 - smidge_lab, labels = indat$task_id, pos = 4,
         cex = cex_detail_labels)
    for(i in 1:nrow(indat)){
    # = 1
      segments(indat$start_date[i] , js[i] , indat$start_date[i] , max(indat$loc2) + 1 , lty = 3,
        col = ifelse(indat$status[i] == "DONE", "grey","black"))
      segments(indat$start_date[i] , js[i] , indat$end_date[i] , js[i],
        col = ifelse(indat$status[i] == "DONE", "grey","black"))
    }
    # done
    indat_done  <- indat[indat$status == "DONE",]
    points(indat_done$start_date, indat_done$loc2, pch = 16, cex = cex_detail_points, col = "darkgrey")
    text(indat_done$start_date, indat_done$loc2 - smidge_lab, labels = indat_done$task_id, pos = 4,
         cex = cex_detail_labels, col = "darkgrey")  
    for(i in 1:nrow(indat_done)){
    # = 1
      segments(indat_done$start_date[i] , indat_done$loc2[i] , indat_done$start_date[i] , max(indat_done$loc2) + 1 , lty = 3, col = 'darkgrey')
      segments(indat_done$start_date[i] , indat_done$loc2[i] , indat_done$end_date[i] , indat_done$loc2[i], col = 'darkgrey' )
    }
  
    # continuing
  
    bumped_up <- indat[indat$start_date < focal_date & indat$status != 'DONE',]
    if(nrow(bumped_up) > 0){
    text(focal_date, bumped_up$loc2 - smidge_lab, labels = bumped_up$task_id, pos = 4,
         cex = cex_detail_labels, col = 'darkred')
    }

    bumped_up2 <- indat[indat$start_date < focal_date & indat$status == 'DONE' & indat$end_date >= focal_date,]
    if(nrow(bumped_up2) > 0){
    text(focal_date, bumped_up2$loc2 - smidge_lab, labels = bumped_up2$task_id, pos = 4,
         cex = cex_detail_labels, col = 'grey')
    }
    
    # overdue
    ## bumped_up <- indat[indat$end_date < focal_date & indat$status != 'DONE',]
    ## text(focal_date, bumped_up$loc2 - smidge_lab, labels = bumped_up$task_id, pos = 4,
    ##      cex = cex_detail_labels, col = 'darkorange')
    
    #segments(focal_date, yrange[1], focal_date, yrange[2], 'red')
    xstart_date <- ifelse(wday(focal_date) != 1, focal_date - (wday(focal_date) - 2), focal_date)
    xend <- ifelse(wday(focal_date + time_box) != 7, (focal_date + time_box) + (5-wday(focal_date + time_box)), (focal_date + time_box))
    at_dates  <- seq(xstart_date, xend, 1)
    at_dates2  <- seq(xstart_date, xend, 7)
    
    label_dates  <-
      paste(month(as.Date(at_dates2, "1970-01-01"), label = T),
      day(as.Date(at_dates2, "1970-01-01")),
      sep = "-")
  
    axis(1, at = at_dates, labels = F)
    axis(1, at = at_dates2, labels = label_dates,  cex = cex_detail_xlab)
    #segments(min(xrange), min(yrange) - .09, max(xrange), min(yrange) - .09)
    axis(3, at = at_dates, labels = F)
    axis(3, at = at_dates2, labels = label_dates)
    #segments(min(xrange), max(yrange) + .09, max(xrange), max(yrange) + .09)  
    if(show_today) segments(Sys.Date(), min(js), Sys.Date(), max(js) + 1, lty = 2, col = 'blue')
    
  }
  #ls()
  
#+end_src
*** man-gantt_tufte
#+name:gantt_tufte
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # arguments: gantt_tufte
  # this is a ploting function, depends on timebox and preprocessing 

  # args
  ## indat = datin4
  ## smidge_lab = .15
  ## focal_date = Sys.Date()
  ## time_box = 21
  ## end_task_ticks = F # this is the little tick marking the end of the tasks

#+end_src


      
*** test go
**** COMMENT test1-code

#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test2####
  source("R/timebox.R")
  source("R/get_gantt_data.R")
  source("R/gantt_data_prep.R")
  source("R/gantt_tufte.r")
  dat_in <- get_gantt_data(test_data = F)
  dat_out <- gantt_data_prep(dat_in)
  str(dat_out)
    svg("tests/AAPL.svg",height=26,width=18)
    gantt_tufte(dat_out, focal_date = "2015-01-10", cex_context_ylab = 0.45,
     min_context_xrange = "2015-01-01", time_box = 7 * 3.5, cex_detail_xlab = .4)
    dev.off()
  
  #### name:tat####
  #library(devtools)
  #install_github("ivanhanigan/disentangle")
  setwd("tests")
  require(knitr)
  require(markdown)
  opts_chunk$set(fig.align=”left”)
  knit2html("gantt_tufte_test.Rmd", options = c("toc", markdown::markdownHTMLOptions(TRUE)), stylesheet = "custom.css")
  setwd("..")
#+end_src

#+RESULTS:
: /home/ivan_hanigan/tools/disentangle/tests

: 
*** COMMENT test RMD
<section>
    <img style="float: left" src="AAPL.svg">
  </section>

#+name:make_html
#+begin_src R :session *R* :tangle tests/gantt_tufte_test.Rmd :exports none :eval yes
  Overview of Gantt Chart
  ===
  
  ivan.hanigan@anu.edu.au
  
  ```{r echo = F, eval=F, results="hide"}
  setwd("tests")
  require(knitr)
  require(markdown)
  opts_chunk$set(fig.align=”left”)
  knit2html("gantt_tufte_test.Rmd", options = c("toc", markdown::markdownHTMLOptions(TRUE)), stylesheet = "custom.css")
  ```
  
  ```{r}
  print(Sys.Date())
  ```
  
  Introduction
  ---
  
  This is a report of the TODO list broken down by LTERN Data Team member.
  
  
  ![aa](AAPL.svg)  
  
  ```{r}
  print(cat("\n"))
  ```
  
    
  ```{r echo = F, results = "hide", eval = T}
  #### name:test2####
  source("../R/timebox.R")
  source("../R/get_gantt_data.R")
  source("../R/gantt_tufte.r")
  dat <- get_gantt_data(test_data = F)
  # str(dat)
  
  
  datin  <- dbGetQuery(ch,
  "
  select t1.container_task_title, 
  t2.*
  from container_task t1
  join work_package t2
  on t1.id = t2.container_id
  where t2.status != 'DONTSHOW'
  order by container_task_title"
  )
  #str(datin)
  datin_done  <- dbGetQuery(ch,
  "
  select t1.container_task_title, 
  t2.*
  from container_task_done t1
  join work_package_done t2
  on t1.id = t2.container_id
  where t2.status != 'DONTSHOW'
  "
  )
  #str(datin_done)
  datin  <- rbind(datin, datin_done)
   str(datin)
  str(dat)
  as.data.frame(table(datin$name))
  
  
  ```
  
  ```{r echo = F, results = "asis", eval = T}
  library("xtable")
  
  for(i in names(table(datin$name))){
   #i <-names(table(datin$name))[1]
  
   cat(i)
   cat("\n")
   cat("===\nTODO\n---\n")
   # str(dat)
   xdat <- sqldf(
   sprintf("select name, t1.container_task_narrow as task_group , t1.task_id,
   t2.start_date, t2.end_date, t2.efforti as effort_days, t1.notes, t2.status,
   case when t2.depends_on = t1.task_id then '' else t2.depends_on end as depends_on
   from datin t1
   left join
   dat t2
   on t1.task_id = t2.task_id
   where t1.name = '%s'
   order by t2.start_date
   ", i),
   drv = "SQLite")
    xdat$start_date <- as.character(format(xdat$start_date, "%A, %d-%b-%Y"))
    xdat$end_date <- as.character(format(xdat$end_date, "%A, %d-%b-%Y"))
   xdat1 <- subset(xdat, status == 'TODO')
   print(xtable(xdat1), type = "html", include.rownames = F)
  
   xdat2 <- subset(xdat, status == 'DONE')
   xdat2 <- xdat2[which(as.Date(xdat2$end_date, format = "%A, %d-%b-%Y") > Sys.Date() - 7),]
   # xdat2
   if(nrow(xdat2) > 0){
     cat("DONE\n---\n")  
     cat("\n")
     print(xtable(xdat2), type = "html", include.rownames = F)
     }
   }
  
  ```
  
    
#+end_src

#+RESULTS: make_html
*** COMMENT test2-code

| container_task_title | task_id                      | allocated | fte | blocker               |       start_date | effort | status | notes |
| no 1                 | Start                        | ivan      |   1 | NA                    |       2015-03-15 |     1d | DONE   | NA    |
| no 2                 | Repeat MEDLINE search        | ivan      |   1 | Start                 |       2015-03-16 |     5d | TODO   | NA    |
| no 3                 | Retrieve articles            | ivan      |   1 | Repeat MEDLINE search |               NA |     5d | TODO   | NA    |
| no 4                 | Read articles                | ivan      |   1 |                       |       2015-03-26 |    10d | TODO   |       |
| no 5                 | Summarize articles           | ivan      |   1 |                       |       2015-04-06 |     5d | TODO   |       |
| no 6                 | Write introduction           | ivan      |   1 |                       |       2015-04-11 |     5d | TODO   |       |
| no 7                 | Write methods                | ivan      |   1 | Start                 |                  |    10d | TODO   |       |
| no 8                 | Write results                | ivan      |   1 | Start                 |                  |    10d | TODO   |       |
| no 9                 | Write discussion             | ivan      |   1 | Write results         |                  |    10d | TODO   |       |
| no 10                | Send to co-author for review | ivan      |   1 | Write discussion      |                  |     2d | TODO   |       |
| no 11                | Revise draft 1               | ivan      |   1 |                       |       2015-04-30 |    10d | TODO   |       |
| no 12                | Submit article 1             | ivan      |   1 |                       |   Revise draft 1 |     5d | TODO   |       |
| no 13                | Revise draft 2               | ivan      |   1 |                       |       2015-05-30 |    10d | TODO   |       |
| no 14                | Submit article 2             | ivan      |   1 |                       |   Revise Draft 2 |     5d | TODO   |       |
| no 15                | Accepted                     | ivan      |   1 |                       | Submit article 2 |     1d | TODO   |       |

*** code name:test2
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test2####
  library(disentangle)
  library(sqldf)
  library(lubridate)
  
  datin  <- read.csv(
  textConnection("
  container_task_title  , task_id                      , allocated , fte , blocker               ,       start_date , effort , status , notes 
  01 Start              , Start                        , ivan      ,   1 , NA                    ,       2015-03-15 ,     1d , DONE   , NA    
  02 Update Lit Review  , Repeat MEDLINE search        , ivan      ,   1 , Start                 ,       2015-03-16 ,     5d , DONE   , NA    
  02 Update Lit Review  , Retrieve articles            , ivan      ,   1 , Repeat MEDLINE search ,               NA ,     5d , DONE   , NA    
  02 Update Lit Review  , Read articles                , ivan      ,   1 ,                       ,       2015-03-26 ,    11d , DONE   ,       
  02 Update Lit Review  , Summarize articles           , ivan      ,   1 ,                       ,       2015-04-06 ,     9d , TODO   ,       
  03 Write Draft        , Write introduction           , ivan      ,   1 ,                       ,       2015-04-09 ,     6d , TODO   ,       
  03 Write Draft        , Write methods                , ivan      ,   1 , Start                 ,                  ,    15d , TODO   ,       
  03 Write Draft        , Write results                , ivan      ,   1 ,                       ,       2015-03-30 ,    10d , TODO   ,       
  03 Write Draft        , Write discussion             , ivan      ,   1 ,                       ,       2015-04-15 ,    10d , TODO   ,       
  04 Internal Review    , Send to co-author for review , ivan      ,   1 , Write discussion      ,                  ,     2d , TODO   ,        
  04 Internal Review    , Revise draft 1               , ivan      ,   1 ,                       ,       2015-04-19 ,    10d , TODO   ,       
  05 Peer Review        , Submit article 1             , ivan      ,   1 , Revise draft 1        ,                  ,     5d , TODO   ,       
  06 Revise and Resubmit, Revise draft 2               , ivan      ,   1 ,                       ,       2015-04-30 ,    10d , TODO   ,       
  06 Revise and Resubmit, Submit article 2             , ivan      ,   1 , Revise draft 2        ,                  ,     5d , TODO   ,       
  07 End                , Accepted                     , ivan      ,   1 ,                       ,       2015-05-15 ,     1d , TODO   ,       
  "),
  stringsAsFactor = F, strip.white = T)
  # or 
  # datin <- get_gantt_data("gantt_todo", test_data = T) # need to
  # adjust min_context_xrange to 2015-01-01 or something
  datin$start_date  <- as.Date(datin$start_date)
  str(datin)
  datin
  
  dat_out <- gantt_data_prep(dat_in = datin)
  str(dat_out)
  dat_out
  svg("tests/gantt_tufte_test.svg",height=10,width=8)
  gantt_tufte(dat_out, focal_date = "2015-04-13", time_box = 3*7,
              min_context_xrange = "2015-03-16",
              cex_context_ylab = 0.65, cex_context_xlab = .7,
              cex_detail_ylab = 0.9,  cex_detail_xlab = .4,
              show_today = F)
  dev.off()
  
#+end_src

#+RESULTS: test2
: 1

* Workflow Tools


** R-newnode
*** COMMENT R-newnode
#+name:newnode
#+begin_src R :session *R* :tangle R/newnode.r :exports none :eval yes
  ################################################################
  # name:newnode
  newnode<-function(
    name = "name_of_step"
    ,
    inputs= c("input_to_step", "input2", "in3", "in4")
    ,
    outputs= c("output_from_step", "out2", "out3") # character(0)
    ,
    desc = "some (potentially) long descriptive text saying what this step is about and why and how"
    ,
    graph = 'nodes'
    , newgraph=F, notes=F, code=NA, ttype=NA, plot = T,
    rgraphviz = F,
    nchar_to_snip = 40
    ){
     if(rgraphviz == F){
  
    if(nchar(name) > 140) print("that's a long name. consider shortening this")
    if(nchar(desc) > nchar_to_snip) desc <- paste(substr(desc, 1, nchar_to_snip), "[...]")
    name2paste <- paste('"', name, '"', sep = "")
    inputs <- paste('"', inputs, '"', sep = "")
    inputs_listed <- paste(inputs, name2paste, sep = ' -> ', collapse = "\n")
    #cat(inputs_listed)
    outputs <- paste('"', outputs, '"', sep = "")  
    outputs_listed <- paste(name2paste, outputs, sep = ' -> ', collapse = "\n")
    #cat(outputs_listed)
    strng <- sprintf('%s
  %s  [ shape=record, label="{{ { Name | Description } | { %s | %s } }}"] 
  %s\n\n', inputs_listed, name2paste, name, desc, outputs_listed
    )
    if(newgraph == F) eval(parse(text =
                                   sprintf('strng <- paste(%s, strng, "\n")', graph, graph)
                             ))
    # cat(strng)
    return(strng)
  
    } else {
    # USAGE
    # nodes <- newnode(  # adds to a graph called nodes
    # name = 'aquire the raw data'  # the name of the node being added 
    # inputs = REQUIRED c('external sources','collected by researcher') # single or multiple inputs to it
    # outputs = OPTIONAL c('file server','metadata','cleaning') # single or multiple outputs from it
    # append=F # append to existing graph?  if False remove old graph of that name and start new
    # TODO 
    # nodes <- addEdge(from='analyse using stats package',
    # to='new data in database server',graph=nodes,weights=1)
    # INIT
    # source('http://bioconductor.org/biocLite.R')
    # biocLite("Rgraphviz")
    # or may be needed for eg under ubuntu
    # biocLite("Rgraphviz", configure.args=c("--with-graphviz=/usr"))
    require(Rgraphviz)
    # FURTHER INFO
    # see the Rgraphviz examples
    # example(layoutGraph)
    # require(biocGraph) # for imageMap
    # TODO change names in following
    dsc <- name
    i <- inputs
    o <- outputs
    #   if(!exists('nodes')) {
    if(newgraph==T) {    
      nodes <- new("graphNEL", nodes=c(dsc),
                 edgemode="directed")
      # nodes <- addEdge(from=i, to=dsc, graph=nodes, 1)    
    } else {
      if(length(grep(dsc,nodes@nodes)) == 0) nodes <- addNode(node=dsc,object=nodes)
    }  
    if(sum(i %in% nodes@nodes) != length(i)) {
      inew <- i[!i %in% nodes@nodes]
      nodes <- addNode(node=inew,object=nodes)   
    }
    nodes <- addEdge(i, dsc, nodes, 1)
    #}
    if(length(o) > 0){
    if(sum(o %in% nodes@nodes) != length(o)) {
      onew <- o[!o %in% nodes@nodes]
      nodes <- addNode(node=onew,object=nodes)   
    }
    nodes <- addEdge(from=dsc, to=o, graph=nodes, 1)  
    }
    if(plot == T){
      try(silent=T,dev.off())
      plot(nodes,attrs=list(node=list(label="foo", fillcolor="grey",shape="ellipse", fixedsize=FALSE), edge=list(color="black")))
    }
    return(nodes)
    }
  }
  
#+end_src

#+RESULTS: newnode

*** test-newnode
#+name:newnode
#+begin_src R :session *R* :tangle tests/test-newnode.r :exports reports :eval no
  ################################################################
  # name:newnode
  # REQUIRES GRAPHVIZ, AND TO INSTALL RGRAPHVIZ
  # source('http://bioconductor.org/biocLite.R')
  # biocLite("Rgraphviz")
  # or may be needed for eg under ubuntu
  # biocLite("Rgraphviz", configure.args=c("--with-graphviz=/usr"))
  # FURTHER INFO
  # see the Rgraphviz examples
  # example(layoutGraph)
  # require(biocGraph) # for imageMap
  
  # source("R/newnode.r")
  require(devtools)
  #install_github("disentangle", "ivanhanigan")
  load_all()
  #require(disentangle)
  newnode(
    name = "NAME"
    ,
    inputs="INPUT"
    ,
    outputs = "OUTPUT"
    ,
    graph = 'nodes'
    ,
    newgraph=T
    ,
    notes=F
    ,
    code=NA
    ,
    ttype=NA
    ,
    plot = T, rgraphviz = F
    )
  
  nodes <- newnode("merge", c("d1", "d2", "d3"), c("EDA"),
                   newgraph =T)
  nodes <- newnode("qc", c("data1", "data2", "data3"), c("d1", "d2", "d3"))
  nodes <- newnode("modelling", "EDA")
  nodes <- newnode("model checking", "modelling", c("data checking", "reporting"))
#+end_src
*** COMMENT test-df-input-code
#+name:test-df-input
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:test-df-input####
  library(devtools)
  load_all()
  #require(disentangle)
  # either edit a spreadsheet with filenames, inputs and outputs 
  # filesList <- read.csv("exampleFilesList.csv", stringsAsFactors = F)
  # or 
  filesList <- read.csv(textConnection('
  CLUSTER ,  FILE         , INPUTS                    , OUTPUTS                                , DESCRIPTION                      
  A       ,  siteIDs      , GPS                       ,                                        , latitude and longitude of sites  
  A       ,  weather      , BoM                       ,                                        , weather data from BoM            
  B       ,  trapped      , siteIDs                   ,                                        , counts of species caught in trap 
  B       ,  biomass      , siteIDs                   ,                                        ,                                  
  B       ,  correlations , "weather,trapped,biomass" , report1                                , A study we published             
  C       ,  paper1       , report1                   , "open access repository, data package" ,                                  
  '), stringsAsFactors = F, strip.white = T)
  str(filesList)
  filesList
  ## newnode_df <- function(indat, names_col, in_col, out_col, desc_col, clusters_col){
  ## # start the graph
  i <- 1
  nodes <- newnode(name = indat[i,names_col],
                   inputs = strsplit(indat[,in_col], ",")[[i]],
                   outputs =
                   strsplit(indat[,out_col], ",")[[i]]
                   ,
                   newgraph=T)
   
  for(i in 2:nrow(indat))
  {
    # i <- 2
    if(length(strsplit(indat[,out_col], ",")[[i]]) == 0)
    {
      nodes <- newnode(name = indat[i,names_col],
                       inputs = strsplit(indat[,in_col], ",")[[i]]
      )    
    } else {
      nodes <- newnode(name = indat[i,names_col],
                       inputs = strsplit(indat[,in_col], ",")[[i]],
                       outputs = strsplit(indat[,out_col], ",")[[i]]
      )
    }
  }
  
  help_txt <- c('
  sink("fileTransformations.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  ')
  cat(help_txt)
  ## return(nodes)
  ## }
  
  
  ## newnode_df <- function(indat, names_col, in_col, out_col, desc_col, clusters_col, graph = 'nodes_list'){
  ## # start the graph
  ## i <- 1
  ## nodes_list <- eval(parse(text=sprintf('newnode(name = indat[i,names_col],
  ##                  inputs = strsplit(indat[,in_col], ",")[[i]],
  ##                  outputs =
  ##                  strsplit(indat[,out_col], ",")[[i]]
  ##                  ,
  ##                  newgraph=T, graph = %s)', graph)))
   
  ## for(i in 2:nrow(indat))
  ## {
  ##   # i <- 2
  ##   if(length(strsplit(indat[,out_col], ",")[[i]]) == 0)
  ##   {
  ##     nodes_list <- eval(parse(text=sprintf('newnode(name = indat[i,names_col],
  ##                      inputs = strsplit(indat[,in_col], ",")[[i]]
  ##     , graph = %s)',graph)))    
  ##   } else {
  ##     nodes_list <- eval(parse(text=sprintf('newnode(name = indat[i,names_col],
  ##                      inputs = strsplit(indat[,in_col], ",")[[i]],
  ##                      outputs = strsplit(indat[,out_col], ",")[[i]]
  ##     , graph = %s)', graph)))
  ##   }
  ## }
  ## cat(nodes_list)
  ## help_txt <- c('
  ## sink("fileTransformations.dot")
  ## cat("digraph G {")
  ## cat(nodes)
  ## cat("}")
  ## sink()
  ## system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  ## ')
  ## cat(help_txt)
  ## return(nodes_list)
  ## }
  
  
  
  
  str(filesList)
  ## undebug(newnode_df)
  ## nodes <- newnode_df(
  ## indat = filesList
  ## ,
  ## names_col = "FILE"
  ## ,
  ## in_col = "INPUTS"
  ## ,
  ## out_col = "OUTPUTS"
  ## ,
  ## desc_col = "DESCRIPTION"
  ## ,
  ## clusters_col = "CLUSTER"
  ## )
  cat(nodes)
  
  sink("fileTransformations.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  
#+end_src
*** COMMENT test-df-input-code2
#+name:test-df-input
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test-df-input####
  library(devtools)
  load_all()
  filesList <- read.csv(textConnection('
  CLUSTER ,  FILE    , INPUTS                    , OUTPUTS                                , DESCRIPTION                      
  A  ,  siteIDs      , GPS                       , spatial                                , latitude and longitude of sites  
  A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
  B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
  B  ,  biomass      , spatial                   , biomass_g                              ,                                  
  B  ,  correlations , "exposures,trapped_no,biomass_g" , report1                         , A study we published             
  C  ,  paper1       , report1                   , "open access repository, data package" ,                                  
  D  ,  biomass revision, new estimates          , biomass_g                              , this came late
  '), stringsAsFactors = F, strip.white = T)
  str(filesList)
  
  indat = filesList
  #,
  names_col = "FILE"
  #,
  in_col = "INPUTS"
  #,
  out_col = "OUTPUTS"
  #,
  desc_col = "DESCRIPTION"
  #,
  clusters_col = "CLUSTER"
  
  ## # start the graph
  rm(nodes)
  
  cluster_ids <- names(table(indat[,clusters_col]))
  cluster_ids
  for(cluster_i in cluster_ids){
    #cluster_i <- cluster_ids[1]
    if(cluster_i == cluster_ids[1]){
    nodes <- sprintf('subgraph cluster_%s {
    label = "%s"
    ', cluster_i, cluster_i)
    
    i <- 1
    nodes <- newnode(name = indat[i,names_col],
                     inputs = strsplit(indat[,in_col], ",")[[i]],
                     outputs =
                     strsplit(indat[,out_col], ",")[[i]]
                     ,
                     newgraph=F)
    } else {
    nodes <- paste(nodes, sprintf('subgraph cluster_%s {
    label = "%s"
    ', cluster_i, cluster_i))
  
    #i <- 1
    if(length(strsplit(indat2[,out_col], ",")[[i]]) == 0){
      nodes <- newnode(name = indat2[i,names_col],
                       inputs = strsplit(indat2[,in_col], ",")[[i]]
      )
    } else {
      nodes <- newnode(name = indat2[i,names_col],
                       inputs = strsplit(indat2[,in_col], ",")[[i]],
                       outputs = strsplit(indat2[,out_col], ",")[[i]]
      )
    }
    }
  #cat(nodes)


  indat2 <- indat[indat[,clusters_col] == cluster_i,]
  indat2
  
  for(i in 2:nrow(indat2)){
    # i <- 2
    if(length(strsplit(indat2[,out_col], ",")[[i]]) == 0){
      nodes <- newnode(name = indat2[i,names_col],
                       inputs = strsplit(indat2[,in_col], ",")[[i]]
      )    
    } else {
      nodes <- newnode(name = indat2[i,names_col],
                       inputs = strsplit(indat2[,in_col], ",")[[i]],
                       outputs = strsplit(indat2[,out_col], ",")[[i]]
      )
    }
  }
  
  nodes <- paste(nodes,"}\n\n")
  }
  cat(nodes)
  help_txt <- c('
  sink("fileTransformations.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  ')
  cat(help_txt)
  cat(nodes)
  
  sink("fileTransformations.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  
#+end_src

#+RESULTS: test-df-input
=0
==0
==0
=*** COMMENT test2-code
#+name:test2
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:test2####
  getwd()
  source("R/newnode.r")
  newnode
  rm(nodes)
  nodes <- newnode("Setup_Data.R", inputs = "data",
                   outputs = c("stag.data.all.years.df",
                     "ash.fire.matrix.df",
                     "stag.data.yyyy.xx.df"), # "Subsets of the data by year and fire type"),
                   newgraph = T)
  
  nodes <- newnode("average number of stags standing per site",
                   inputs = "stag.data.yyyy.xx.df",
                   outputs = "ash.no.stags.yyyy.xx"
                   )
  
  nodes <- newnode("variance in the average number of stags per site",
                   inputs = "stag.data.yyyy.xx.df", 
                   outputs = "ash.var.no.stags.yyyy.xx"
                   )
  nodes <- newnode("compute the number of transitions from form to form",
                   inputs = "stag.data.yyyy.xx.df",
                   outputs = "tpm.98.11.xx.coll.ash"
                   )
  nodes <- newnode("convert to a probability transition matrix",
                   inputs = "tpm.98.11.xx.coll.ash",
                   outputs = "tpm.98.11.xx.coll.ash.per"                 
                   )
  nodes <- newnode("frequencies) by form  in 2011 by fire category ",
                   inputs = "stag.data.2011.xx.df",
                   outputs = "freq.11.xx.coll.ash"
                   )
  sink("test.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf test.dot -o test.pdf")
#+end_src

*** COMMENT TODO man-newnode
#+name:newnode
#+begin_src R :session *R* :tangle no :exports none :eval no
################################################################
# name:newnode

#+end_src

** R-newnode_df
*** COMMENT newnode_df-code
#+name:newnode_df
#+begin_src R :session *R* :tangle R/newnode_df.R :exports none :eval no
  #### name:newnode_df####
  
  newnode_df <- function(indat = NA,names_col = "FILE",in_col = "INPUTS",out_col = "OUTPUTS",desc_col = "DESCRIPTION",clusters_col = "CLUSTER", todo_col = "STATUS", nchar_to_snip = 40){
  
  # start the graph
  cluster_ids <- names(table(indat[,clusters_col]))
  #cluster_ids
  
  for(cluster_i in cluster_ids){
    # cluster_i <- cluster_ids[1]
  
    if(cluster_i == cluster_ids[1]){
      nodes_graph <- sprintf('subgraph cluster_%s {
      label = "%s"
      ', cluster_i, cluster_i)
    } else {
      nodes_graph <- paste(nodes_graph, sprintf('subgraph cluster_%s {
      label = "%s"
      ', cluster_i, cluster_i))  
    }
  #  cat(nodes_graph)    
    indat2 <- indat[indat[,clusters_col] == cluster_i,]
  #  indat2
      for(i in 1:nrow(indat2)){
        # i <- 1
        #i
        indat2[i,]
        name <- indat2[i,names_col]
        inputs <- unlist(lapply(strsplit(indat2[i,in_col], ","), str_trim))
        outputs <- unlist(lapply(strsplit(indat2[i,out_col], ","), str_trim))
        desc <- indat2[i,desc_col]
        status <- indat2[i,todo_col]

        if(nchar(name) > 140) print("that's a long name. consider shortening this")
        if(nchar(desc) > nchar_to_snip) desc <- paste(substr(desc, 1, nchar_to_snip), "[...]")
        name2paste <- paste('"', name, '"', sep = "")
        inputs <- paste('"', inputs, '"', sep = "")
        #inputs
        inputs_listed <- paste(inputs, name2paste, sep = ' -> ', collapse = "\n")
        #cat(inputs_listed)
        outputs <- paste('"', outputs, '"', sep = "")  
        outputs_listed <- paste(name2paste, outputs, sep = ' -> ', collapse = "\n")
        #cat(outputs_listed)
  strng <- sprintf('%s\n%s  [ shape=record, label="{{ { Name | Description | Status } | { %s | %s | %s } }}"]\n%s\n\n',
                   inputs_listed, name2paste, name, desc, status, outputs_listed
                   )
        # cat(strng)
        if(!status %in% c("DONE", "WONTDO")){ 
          strng <- gsub("shape=record,", "shape=record, style = \"filled\", color=\"indianred\",", strng)
        }
        nodes_graph <- paste(nodes_graph, strng, "\n")
        if(nrow(indat2) == 1) break
      }
  nodes_graph <- paste(nodes_graph, "}\n\n")
  }
  
  nodes_graph <- paste("digraph transformations {\n\n",
                       nodes_graph,
                       "}\n")
  
  help_text <- c('# to run this graph
  sink("fileTransformations.dot")
  cat(nodes_graph)
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  ')
  cat(help_text)
  return(nodes_graph)
  }
  
#+end_src

*** COMMENT test-df-input-code3
#+name:test-df-input
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test-df-input####
  library(devtools)
  load_all()
  library(stringr)
  ## filesList <- read.csv(textConnection('
  ## CLUSTER ,  FILE    , INPUTS                    , OUTPUTS                                , DESCRIPTION                      
  ## A  ,  siteIDs      , "GPS, helicopter"          , "spatial, site doco"                 , latitude and longitude of sites  
  ## A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
  ## B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
  ## B  ,  biomass      , spatial                   , biomass_g                              ,                                  
  ## B  ,  correlations , "exposures,trapped_no,biomass_g" , report1                         , A study we published             
  ## C  ,  paper1       , report1                   , "open access repository, data package" ,                                  
  ## D  ,  biomass revision, new estimates          , biomass_g                              , this came late
  ## '), stringsAsFactors = F, strip.white = T)
  #write.csv(filesList, "fileTransformations.csv", row.names = F)
  filesList <- read.csv("fileTransformations.csv", stringsAsFactors = F, strip.white = T)
  
  str(filesList)
  # filesList
  
  nodes_graphy <- newnode_df(indat = filesList, names_col = "STEP", in_col = "INPUTS", out_col = "OUTPUTS", desc_col = "DESCRIPTION", clusters_col = "CLUSTER", todo_col = "STATUS", nchar_to_snip = 40)
  
  sink("fileTransformations.dot")
  cat(nodes_graphy)
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  
#+end_src

** 2013-11-25-setting-up-a-workflow-script
#+name:project-template-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-11-25-setting-up-a-workflow-with-code-chunks.md :exports none :eval no :padline no
  ---
  name: 2013-11-25-setting-up-a-workflow-with-code-chunks
  layout: post
  title: Setting Up A Workflow Script With Code Chunks
  date: 2013-11-25
  categories:
  - research methods
  ---
  
  This post describes some ideas and techniques I use to set up a "workflow script".  I use this term to refer to the structured combination of code, data and narrative that make an executable Reproducible Research Report (RRR).
  
  A lot of these ideas are inpsired by  a great paper by Kieran Healy called  "Choosing Your Workflow Applications" available at [https://github.com/kjhealy/workflow-paper](https://github.com/kjhealy/workflow-paper) to accompany [his Emacs Starter Kit](http://kieranhealyo.org/resources/emacs-starter-kit.html). My shortened version of his main points are:
  
  - 1 use a good code editor
  - 2 analyse data with scripts
  - 3 store your work simply and document it properly
  - 4 use a version control system
  - 5 Automate back ups 
  - 6 Avoid distracting gadgets
  
  #### Here's my current approach in each of these categories
  - 1 use Emacs with Orgmode (and kjhealy's drop-in set of useful defaults)
  - 2 Scripts that utilise the literate programming technique of mixing Code Chunks in with descriptive prose
  - 3 John Myles White's ProjectTemplate R Package and Josh Riech's LCFD paradigm 
  - 4 git and GitHub for version control
  
  5 Automated Backups and 6 Avoiding Gadgets are still somethings I find challenging
  
  #### 1 Use a good code editor
  I like using Emacs with Orgmode.
  
  - I have previously tried a variety of code editors from Tinn-r, NppToR, Rstudio and Eclipse.  
  - Emacs with Orgmode suits me the most because it has a great number of features especially the linkage with LaTeX or HTML export
  - A key reference to look at for reasons why Emacs is so good for scientific work is Eric Schulte et al ["A Multi-Language Computing Environment for Literate Programming"](www.jstatsoft.org/v46/i03‎) 
  - Here is a [link to a great orgmode description](http://doc.norang.ca/org-mode.html)
  - (this guy spends a lot of time on tweaking his set up)
  
  #### 2 Analyse data with Scripts (stitch together code chunks)
  I use Scripts but prefer to think of them as stitched together Code Chunks with prose into Compendia.
  
  - Compendia are documents that weave together Code and Prose into an executable report
  - The underlying philosophy is called Reproducible Research Reports
  - A very useful tool is a keyboard shortcut to quickly create a chunk for code
  - so you can be writing parts of the report like this: "Blah Blah Blah as shown in Figure X and Table Y"
  - then just hit the correct keys and WHAMM-O there is a new chunk ready for the code that creates Figure X and Table Y to be written.
  - Here is how I use Emacs to achieve this (the other editors I mentioned above also have the abiltiy to do this too).  The IPython Notebook does this stuff too but calls chunks "cells" for some reason.
  
  #### Emacs Code: Put this into the ~/.emacs.d/init.el file
      (define-skeleton chunk-skeleton
        "Info for a code chunk."
        "Title: "
        "*** " str "-code\n"
        "#+name:" str "\n"
        "#+begin_src R :session *R* :tangle src/" str ".r :exports reports :eval no\n"
        "#### name:" str " ####\n"
        "\n"
        "#+end_src\n"
      )
      (global-set-key [?\C-x ?\C-\\] 'chunk-skeleton)
  
  #### Using the Emacs Shortcut
  - now whenever you type Control-x control-\ a new code chunk will appear
  - you'll be typing "blah blah blah" and think I need a figure or table, just hit it.
  - move into the empty section and add some code
  - you can hit C-c ' to enter a org-babel code execution session that will be able to send these line by line to an R session
  - or within the main org buffer if your eval flag is set to yes then you can run the entire chunk (and return tabular output to the doc) using C-c C-c
  - To export the code chunks and create the modular code scripts without the narrative prose use C-c C-v t
  - this is called "tangling" and the chunks will be written out to the file specified in the chunk header ":tangle" flag
  
  #### Compiling the resulting Compendium
  - Emacs uses LaTeX or HTML to produce the Report
  - I find both of these outputs very pleasing
  - to compile to TEX use C-c C-e d
  - for HTML use C-c C-e h (FOR CODE HIGHLIGHTING INSTALL htmlize.el)
  - these commands will also evaluate all the chunks where ":eval" = yes to load the data and calculate the results fresh. 
  - AWESOME!
      
  #### 3 Store your work simply and document it properly
  - I use the [ProjectTemplate R package](http://www.johnmyleswhite.com/notebook/2010/08/26/projecttemplate/) to organise my code sections into modules
  - These modules are organised into the Reichian LCFD paradigm described first [on StackOverflow here](http://stackoverflow.com/a/1434424), and encoded into [the makeProject R package](http://cran.r-project.org/web/packages/makeProject/makeProject.pdf)
  - documentation is within the main orgmode script
  - data documentation is a whole other universe that I will deal with in a separate post
  
  #### 4 use a version control system using git and github
      # once you have the project via R
      R
      require(ProjectTemplate)
      create.project("AwesomeProject", minimal = T)
      q()
      # use the shell to start a git repo
      cd AwesomeProject
      git init
      # and commit the TODO
      git add TODO
      git commit -m "first commit"
      # tada!
  
  - Emacs can now be used to manage the git repo using the C-x g command
  - Rstudio has a really nice GUI for doing this inside it;s Project management interface too.
  
  #### Using Github or another Git Server
  - You can easily set up a Github repo for this now but it will be public
  - Alternatative is to set up your own private Git server.  I followed [these instructions to Running a Simple Git Server Using SSH](http://blog.goosoftware.co.uk/2012/02/07/quick-git-server/)
  - Either way once you have set up your remote git repo you need to set the remote tracking
  
  #### Git Code:
      cd /path/to/local/git/repo
      git remote add origin git@github-or-other-server:myname/myproject.git
      git push origin master
  
  #### 5 Automate back ups AND 6 Avoid distracting gadgets
  - OMG backups stress me out
  - ideally I would follow [this advice because "when it comes to losing your data the universe tends toward maximum irony. Don't push it."](http://www.jwz.org/blog/2007/09/psa-backups/)
  - But I don;t fully comply
  - Instead I generally use Dropbox for  basic project management admin stuff
  - I use github for code projects I am happy to share, I also pay for 10 private repos 
  - I Set up a git server at my workplace for extra projects but this is on a test server that is not backed up, and I am not really happy about this
  - In terms of Distracting Gadgets, I think that with the current tempo of new innovations related to new software tools for this type of work I should keep trying new things but I have pretty much settled into a comfortable zone with the gadgets I described here. 
  
  #### Conclusions
  - This is how I've worked for a couple of years
  - I find it very enjoyable, mostly productive but prone to the distractions of "distractions by gadgets"
  - The main thing I want to point out is the usage of Code Chunks in RRR scripts.
  - These things are awesome.
      
#+end_src

** 2013-12-01-graphviz-automagic-flowcharts
*** workflow_steps-code
#+begin_src R :session *R* :tangle no :exports reports :eval no :padline no
# ~/tools/transformations/workflow_steps.txt
Transformation
	description
		the thing 
	inputs
		the thing before
		another thing
	output
		the next thing
	notes
		the thing is this other thing	this is a really long description 
		blah blah asdfasdfasdfasdfasdfa 

Transformation
	description
		yet another thing
	inputs
		the next thing
	output
		a final thing
	notes
		this is a note

#+end_src

*** post
#+name:graphviz-automagic-flowcharts-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-12-01-graphviz-automagic-flowcharts.md :exports none :eval no :padline no
  ---
  name: 2013-12-01-graphviz-automagic-flowcharts
  layout: post
  title: graphviz-automagic-flowcharts
  date: 2013-12-01
  categories:
  - research methods 
  ---
  
  - Back in 2009 Joseph Guillaume worked with me on a complicated workflow
  - He came up with this python script to help keep track of the steps
  - The simple text file is a list of transformations, inputs and output
  - It is converted to the right format and graphviz creates a html page with pop-outs
  
  #### Simple text list
      Transformation
              description
                      the thing 
              inputs
                      the thing before
                      another thing
              output
                      the next thing
              notes
                      the thing is this other thing   this is a really long description 
                      blah blah asdfasdfasdfasdfasdfa 
       
      Transformation
              description
                      yet another thing
              inputs
                      the next thing
              output
                      a final thing
              notes
                      this is a note
  
  <p></p>
  
  - keep this in your work directory and update it whenever you add a step to the workflow
  - the list can be as big as you like (hundreds of steps), and entered in any order, the inputs/output relationships determine how the graph looks at the end
  - to run the script just do the one line

  #### Python code: run
      python transformations.py workflow_steps.txt index
  
  <p></p>
  - open the html page and click on a square box to bring up the pop-out
  - short text is shown, long text is replaced by an ellipse and only shown in pop-out
  
  #### Conclusions  
  - I've popped the script up as a [Github repo](https://github.com/ivanhanigan/transformations)
  - The example is in the [gh-pages branch](http://ivanhanigan.github.io/transformations/)
  
#+end_src

** 2013-12-24-a-few-best-practices-for-statistical-programming
#+name: a-few-best-practices-for-statistical-programming-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-12-24-a-few-best-practices-for-statistical-programming.md :exports none :eval no :padline no
---
name: 2013-12-24-a-few-best-practices-for-statistical-programming
layout: post
title: a-few-best-practices-for-statistical-programming
date: 2013-12-24
categories:
- research methods
---

- John Myles White invented the ProjectTemplate R Package
- This is a great application that helps streamline the process of creating a data analysis project
- Recently John posted about some tips for [best practices for statistical programming](http://www.johnmyleswhite.com/notebook/2013/01/24/writing-better-statistical-programs-in-r/)


#### Best Practices for Statistical Programming

- Write Out a Directed Acyclic Graph (DAG)
- Vectorize Your Operations
- Profile your code and understand where it spends its time
- Generate Data and Fit Models
- Correctness: always ensure that code infers  parameters of models given simulated data with known parameters.


#### Additional suggestions

- Unit Testing (use testthat)
- Create modular code with discrete chunks
- Write functions as much as possible, put these into a personal 'misc' package
   
#+end_src

* XML
** COMMENT review-xml-code
#+name:review-xml
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:review-xml####
  require(XML)
  # first read one without a public access
  lnk <- "http://www.ltern.org.au/knb/metacat/ltern2.42.44/xml"
  xmlfile=xmlParse(lnk)
  xmltop = xmlRoot(xmlfile) 
  length(names(xmltop[[1]]))
  
  names(xmltop[[2]])
  names(xmltop[[2]]['project']['project'])
  names(xmltop[[2]]['project'][['project']]) == 'personnel'
  names(xmltop[[2]]['project'][[1]][[4]])
  xmltop[[2]]['project'][['project']][[4]]
  xmltop[[2]]['project'][['project']][[4]][['individualName']]
  xmltop[[2]]['project'][['project']][[4]][['role']]
  
  # the first
  xmltop[[2]]['project'][['project']][['personnel']][['role']]
  
  xmltop[[2]]['contact'][['contact']][['organizationName']]
  
  
  # lnk <- "http://dev.ltern.org.au/knb/metacat/datalibrarian.67/xml"
   lnk <- "http://dev.ltern.org.au/knb/metacat/ltern2.42/xml"
  xmlfile=xmlParse(lnk)
  xpath = xmlRoot(xmlfile)
  
  names(xpath)
  xpath[[1]]
  names(xpath[[2]])
  
  #xpath_dataset <- xpath[['dataset']]
  xmlValue(xpath[['dataset']][['title']])
  xpath[['dataset']]['creator']
  lapply(
    xpath[['dataset']]['creator']
    , xmlValue)
  
  # http://stackoverflow.com/a/22626191
  flatten_xml <- function(x) {
    if (length(xmlChildren(x)) == 0) structure(list(xmlValue(x)), .Names = xmlName(xmlParent(x)))
    else Reduce(append, lapply(xmlChildren(x), flatten_xml))
  }
  
  dfs <- lapply(getNodeSet(xpath,"//creator"), function(x) data.frame(flatten_xml(x)))
  allnames <- unique(c(lapply(dfs, colnames), recursive = TRUE))
  df <- do.call(rbind, lapply(dfs, function(df) { df[, setdiff(allnames,colnames(df))] <- NA; df }))
  head(df)
  
  
  
  xpath[['dataset']][['project']]
  names(xpath[['dataset']][['project']])
  xpath[['dataset']][['project']][['title']]
  lapply(xpath[['dataset']][['project']]['personnel'], xmlValue)
  dfs <- lapply(getNodeSet(xpath,"//personnel"), function(x) data.frame(flatten_xml(x)))
  allnames <- unique(c(lapply(dfs, colnames), recursive = TRUE))
  df <- do.call(rbind, lapply(dfs, function(df) { df[, setdiff(allnames,colnames(df))] <- NA; df }))
  head(df)
  
  
  xmlValue(xpath[['dataset']][['project']][['funding']])
  
#+end_src

