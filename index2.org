#+TITLE:Disentangle Things (overflow)
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----
* COMMENT DESCRIPTION-code
#+name:DESCRIPTION
#+begin_src R :session *R* :tangle DESCRIPTION :exports none :eval no :padline no
Package: disentangle
Type: Package
Title: disentangle
Version: 1.4.6
Date: 2015-09-15
Author: ivanhanigan
Maintainer: <ivan.hanigan@gmail.com>
Suggests: stringr, ggmap, maps, maptools, rgdal, sqldf
Description:  Functions for helping to disentangle data and analysis pipelines.
License: GPL (>= 2)
#+end_src

* COMMENT DEPRECATED Vignette SEE THESIS
*** COMMENT run-code
#+name:run
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:run ####
  #devtools::use_vignette("disentangle-pipelines")
  #system("cp -R ~/Dropbox/tools/LaTeX\\ templates/manuscriptPackage/vignettes/components/* vignettes/components/")
  #rm(bib)
  rmarkdown::render("vignettes/disentangle-pipelines.Rmd")
  #browseURL("vignettes/disentangle-pipelines.pdf")
#+end_src

#+RESULTS: run
: 0

*** COMMENT rmd head-code
#+name:rmd head
#+begin_src R :session *R* :tangle vignettes/disentangle-pipelines.Rmd :exports none :eval no :padline
---
title: 'Disentangling Data Analysis Pipelines'
author:  
- name: Ivan Hanigan
  affilnum: '1'
  email: ivan.hanigan@gmail.com
affiliation:
- affilnum: 1
  affil: National Centre for Epidemiology and Population Health, Australian National University
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
    template: components/manuscript.latex
    toc: true    
  html_document: null
  word_document: null
fontsize: 11pt
capsize: normalsize
csl: components/ecology.csl
documentclass: article
papersize: a4paper
spacing: singlespacing
linenumbers: no
bibliography: components/manuscript.bib
abstract: no
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Example Manuscript}
-->

```{r setup, include=FALSE, echo=FALSE}
#Put whatever you normally put in a setup chunk.
#I usually at least include:
#devtools::install_github("manuscriptPackage","jhollist")
#library("manuscriptPackage")
#Didn't do that here to expedite building of the example vignette
library("knitr")
library("knitcitations")
library(bibtex)
cleanbib()
# rm("bib")
#options("cite_format"="pandoc")
cite_options(citation_format = "pandoc", check.entries=FALSE)

opts_chunk$set(dev = 'pdf', fig.width=6, fig.height=5)

# Table Captions from @DeanK on http://stackoverflow.com/questions/15258233/using-table-caption-on-r-markdown-file-using-knitr-to-use-in-pandoc-to-convert-t
#Figure captions are handled by LaTeX

knit_hooks$set(tab.cap = function(before, options, envir) {
                  if(!before) { 
                    paste('\n\n:', options$tab.cap, sep='') 
                  }
                })
default_output_hook = knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  if (is.null(options$tab.cap) == FALSE) {
    x
  } else
    default_output_hook(x,options)
})

if(!exists("bib")){
  bib <- read.bibtex("~/references/library.bib")
}

```

```{r analysis , include=FALSE, echo=FALSE, cache=FALSE}
#All analysis in here, that way all bits of the paper have access to the final objects
#Place tables and figures and numerical results where they need to go.
```

<!-- Abstract is being wrapped in latex here so that all analysis can be run in the chunk above and the results reproducibly referenced in the abstract. -->

<!-- \singlespace -->

<!-- \vspace{2mm}\hrule -->

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer in -->
<!-- sem sed sem pharetra eleifend vitae id massa. Curabitur et erat sit -->
<!-- amet enim gravida dapibus quis vel ex. Maecenas luctus suscipit magna -->
<!-- id vehicula. Quisque tincidunt auctor dignissim. Nunc vitae nulla vel -->
<!-- lorem facilisis interdum -->

<!-- \vspace{3mm}\hrule -->
<!-- \doublespace -->
#+end_src
*** 2015-09-02-tracking-a-data-analysis-pipeline
#+name:tracking-a-data-analysis-pipeline-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-08-28-tracking-a-data-analysis-pipeline.md :exports none :eval no :padline no
  ---
  name: tracking-a-data-analysis-pipeline
  layout: post
  title: tracking-a-data-analysis-pipeline
  date: 2015-08-28
  categories:
  - disentangle
  ---

  I have just uploaded a new version of the windows build for my 'disentangle' package.  The blurb of the  draft vignette is below.
  
  # Introduction  
  
  It can be much easier to conceptually understand a complicated data
  analysis pipeline than it is to implement that pipeline effectively.
  This report outlines the use of the 'disentangle' R package, available from [http://ivanhanigan.github.io/projects.html](http://ivanhanigan.github.io/projects.html).  This package contains functions that were developed to aid data
  analysts to map out all the aspects of their work when planning and
  conducting complicated data analyses using the pipeline concept.    There are often many steps in the design and analysis of a study and
  when these are put together as a data analysis pipeline this addresses
  the challenge of reproducibility (Peng 2006).  The
  credibility of data analyses requires that every step is able to be
  scrutinised (Leek 2015).
  
  ## Motivating scientific questions 

  The type of data analysis that is
  the focus of this work is more complicated than simply loading some
  data that are already cleaned, fitting some models and reporting some
  output.  Typically the type of data analysis projects that these tools
  are aimed at involve attempts to control for a large number
  of inter-relationships and associations between variables. It is
  especially problematic that these variables need to have been selected
  by the scientists from a multitude of possible variables and a
  plethora of possible data sources, during a long process of data
  collection, cleaning, exploration and decision making in preparation
  for data analysis. There are also a multitude of steps and decision
  points in the process of model building and model checking. The use of
  statistical models involving many entangled environmental and social
  variables can easily result in spurious association that may be
  mistakenly interpreted as causation.  Projects that the author has
  been involved in include explorations of hypotheses about health effects from
  droughts, bushfire smoke, heat-waves and dust-storms which produced
  novel findings, and informed controversial debates about the
  implications of climate change. The requirement to adequately convey
  the methods and results of this research was problematic and motivated
  the work on effective use of reproducible research techniques and data
  analysis pipelines.
  
  
  
#+end_src

*** COMMENT intro
#+name:rmd head
#+begin_src R :session *R* :tangle vignettes/disentangle-pipelines.Rmd :exports none :eval no :padline
  
  #Introduction  
  
  It can be much easier to conceptually understand a complicated data
  analysis pipeline than it is to implement that pipeline effectively.
  This report outlines the use of the `disentangle` R package, available from http://ivanhanigan.github.io/projects.html.  This package contains functions that were developed to aid data
  analysts to map out all the aspects of their work when planning and
  conducting complicated data analyses using the pipeline concept.    There are often many steps in the design and analysis of a study and
  when these are put together as a data analysis pipeline this addresses
  the challenge of reproducibility `r citep(bib[["Peng2006"]])`.  The
  credibility of data analyses requires that every step is able to be
  scrutinised `r citep(bib[[c("Leek2015a")]])`.
  
  ##Motivating scientific questions 
  The type of data analysis that is
  the focus of this work is more complicated than simply loading some
  data that are already cleaned, fitting some models and reporting some
  output.  Typically the type of data analysis projects that these tools
  are aimed at involve attempts to control for a large number
  of inter-relationships and associations between variables. It is
  especially problematic that these variables need to have been selected
  by the scientists from a multitude of possible variables and a
  plethora of possible data sources, during a long process of data
  collection, cleaning, exploration and decision making in preparation
  for data analysis. There are also a multitude of steps and decision
  points in the process of model building and model checking. The use of
  statistical models involving many entangled environmental and social
  variables can easily result in spurious association that may be
  mistakenly interpreted as causation.  Projects that the author has
  been involved in include explorations of hypotheses about health effects from
  droughts, bushfire smoke, heat-waves and dust-storms which produced
  novel findings, and informed controversial debates about the
  implications of climate change. The requirement to adequately convey
  the methods and results of this research was problematic and motivated
  the work on effective use of reproducible research techniques and data
  analysis pipelines.
  
  
  
#+end_src
*** COMMENT building-blocks-code
#+name:building-blocks
#+begin_src R :session *R* :tangle vignettes/disentangle-pipelines.Rmd :exports none :eval no :padline
  
  #Definitions
  ##Data analysis 
  In this paper data analysis is a very broad topic and includes, in
  the words of `r citet(bib[["Tukey1962"]])`:
  
  \begin{quote}
      \emph{'procedures for analyzing data, techniques for
      interpreting the results of such procedures, ways of planning the
      gathering of data to make its analysis easier, more precise or
      more accurate, and all the machinery and results of (mathematical)
      statistics which apply to analyzing data.'}
  \end{quote}
  
  
  ##Basic building blocks of a pipeline
  The basic components of a pipeline are:
  
  - Steps
  - Inputs 
  - Outputs
  
  A simple way to keep track of the steps, inputs and outputs is shown in Table \ref{tab:TableBasic}.
  
  ```{r results='asis', echo=FALSE}
  library(stringr)
  steps <- read.csv(textConnection('
  CLUSTER ,  STEP    , INPUTS                  , OUTPUTS                   
  A  ,  Step1      , "Input 1, Input 2"       , "Output 1"                 
  A  ,  Step2      , Input 3                  , Output 2                   
  B  ,  Step3      , "Output 1, Output 2"      , Output 3                  
  '), stringsAsFactors = F, strip.white = T)
  
  #kable(
  steps <- steps[,c("STEP", "INPUTS", "OUTPUTS")]
  library(xtable)
  tabcode <- xtable(steps, caption = 'Simple', label = 'tab:TableBasic')
  align(tabcode) <-  c( 'l', 'p{.6in}','p{2in}','p{2in}' )
  #sink(paste(fname, '.tex',sep = ""))
  #cat(txt)
  # print(tabcode,  include.rownames = F, table.placement = '!ht',
  # caption.placement = 'top') #, type = "html")
  rws <- seq(1, (nrow(steps)), by = 2)
  col <- rep("\\rowcolor[gray]{0.95}", length(rws))
  print(tabcode, booktabs = TRUE, include.rownames = F, table.placement = '!ht',
   caption.placement = 'top',
   add.to.row = list(pos = as.list(rws), command = col),
   comment=FALSE)
  
  ```  
  
  The steps and data listed in Table \ref{tab:TableBasic} can be visualised
  using the `newnode` function.  The function returns a string of text
  written in the `dot` language which can be rendered in R using the
  `DiagrammeR` package, or the standalone `graphviz` package.   This creates the graph of this pipeline shown in Figure \ref{fig:FigBasic}.  Note that a new field was added for Descriptions as these are highly recommended.
  
  ```{r echo=F, eval=F}
  library(disentangle); library(stringr)
  nodes <- newnode(indat = steps,   names_col = "STEP", in_col = "INPUTS",
    out_col = "OUTPUTS", 
    nchar_to_snip = 40)
  sink("vignettes/fig-basic.dot");
  cat(nodes);
  sink()
  #DiagrammeR::grViz("fig-basic.dot")
  system("dot -Tpdf vignettes/fig-basic.dot -o vignettes/fig-basic.pdf")
  
  ```
  
  
  
  \begin{figure}[!h]
  \centering
  \includegraphics[width=.5\textwidth]{fig-basic.pdf}
  \caption{steps basic}
  \label{fig:FigBasic}
  \end{figure}
  
  
#+end_src


*** COMMENT style
#+name:rmd head
#+begin_src R :session *R* :tangle vignettes/disentangle-pipelines.Rmd :exports none :eval no :padline
  ##Naming conventions
  Steps should have a name based on a verb. Some examples are
  'test for overdispersion', 'run model1',
  'test for high leverage points' or 'drop outliers'.  Names of steps,
  inputs and outputs should all be kept short to allow visualisation in
  flow charts. It is recommended that each step also have a description for
  longer comments that can be excluded from summaries of
  the pipeline. Inputs and outputs can be either data or information.
  For example inputs to 'model1' might include both data as
  'cleaned dataset' and information such as
  'result of test for overdispersion'.  Output of 'model1' might be a
  different kind of data such as a table of
  'postestimation statistics for model1' or information such as
  'interaction term nonsignificant so exclude from model2'.
  
  
#+end_src
*** COMMENT Additional attributes
#+name:rmd head
#+begin_src R :session *R* :tangle vignettes/disentangle-pipelines.Rmd :exports none :eval no :padline
  
  ##Adding additional attributes
  
  The fundamental building blocks of a pipeline can be defined in a number of ways.  Some additional attributes to consider are the following:
  
  - Source data are different from derived data
  - Methods steps include information and decisions, and
  - Steps can have a task status (ie TODO or DONE)
  
  The `source data` used by a data analyst should be treated much like the
  evidence from a crime scene, protected by the chain of custody.  These
  data files usually need to be transformed into `derived data` during the course of an
  analysis.  The `methods steps` during this transformation (as well as
  those during data collection and curation) need to be planned, tracked
  and sometimes audited.  At each step a `task status` can be defined, in
  simple terms things are either DONE or remain TODO, but might additionally be considered WONTDO.
#+end_src

*** COMMENT simple eg
#+name:rmd head
#+begin_src R :session *R* :tangle vignettes/disentangle-pipelines.Rmd :exports none :eval no :padline
  
  #A simple example
  ##Make a list of steps, inputs and outputs
  
  A very simple example of a pipeline is shown in Table \ref{tab:TablePipe1}.
  
  ```{r TablePipe1, results='asis', echo=FALSE}
  #, tab.cap="This is the first example table\\label{tab:Table1}",cache=FALSE}
  library(stringr)
  steps <- read.csv(textConnection('
  CLUSTER ,  STEP    , INPUTS                  , OUTPUTS                                , DESCRIPTION                        , STATUS 
  A  ,  Step1      , "Source 1, Source 2"       , "Derived 1, QC check"                 , "This might be latitude and longitude of sites, and observations"     ,  DONE
  A  ,  Step2      , Source 3                  , Derived 2                           , This might be weather data               , DONE
  B  ,  Step3      , "Derived 1, Derived 2"      , Derived 3                             , Merging these data means they can be analysed   , TODO
  C  ,  Step4      , Derived 3                 , Model selection                              ,                                    , TODO
  C  ,  Step5      , Model selection           , Sensitivity analysis                         ,                                    , TODO
  '), stringsAsFactors = F, strip.white = T)
  
  #kable(
  dat <- steps[,c("STEP", "INPUTS", "OUTPUTS", "DESCRIPTION", "STATUS")]
  library(xtable)
  tabcode <- xtable(dat, caption = 'Simple', label = 'tab:TablePipe1')
  align(tabcode) <-  c( 'l', 'p{.6in}','p{1.2in}','p{1.2in}', 'p{2in}','p{1.2in}' )
  #sink(paste(fname, '.tex',sep = ""))
  #cat(txt)
  # print(tabcode,  include.rownames = F, table.placement = '!ht',
  # caption.placement = 'top') #, type = "html")
  rws <- seq(1, (nrow(dat)), by = 2)
  col <- rep("\\rowcolor[gray]{0.95}", length(rws))
  print(tabcode, booktabs = TRUE, include.rownames = F, table.placement = '!ht',
   caption.placement = 'top',
   add.to.row = list(pos = as.list(rws), command = col),
   comment=FALSE)
  
  ```
  \clearpage
#+end_src
*** COMMENT simple eg2
#+name:rmd head
#+begin_src R :session *R* :tangle vignettes/disentangle-pipelines.Rmd :exports none :eval no :padline
  
  ##Visualise the steps as a network
  The steps and data listed in Table \ref{tab:TablePipe1} can be visualised
  using the `newnode` function as shown in the code chunk below.  This creates the graph of this pipeline shown in Figure \ref{fig:FigSteps}.
  
  ```{r echo=T, eval=F}
  library(disentangle); library(stringr)
  nodes <- newnode(indat = steps,   names_col = "STEP", in_col = "INPUTS",
    out_col = "OUTPUTS", desc_col = 'DESCRIPTION', todo_col = "STATUS",
    nchar_to_snip = 70)
  sink("vignettes/steps-fig1.dot"); cat(nodes); sink()
  #DiagrammeR::grViz("steps-fig1.dot")
  system("dot -Tpdf vignettes/steps-fig1.dot -o vignettes/steps-fig1.pdf")
  
  ```
  
  
  
  \begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{steps-fig1.pdf}
  \caption{steps fig1}
  \label{fig:FigSteps}
  \end{figure}
  \clearpage
  
#+end_src
*** COMMENT clusters-code
#+name:clusters

#+begin_src R :session *R* :tangle vignettes/disentangle-pipelines.Rmd :exports none :eval no :padline
  
  ##Clusters and modularised code
  Clusters of steps, inputs and outputs can be defined to help keep things modular such as 'processing' and 'modelling'.
  
  ```{r clust,  results='asis', echo=FALSE}
  dat <- steps[,c("CLUSTER", "STEP", "INPUTS", "OUTPUTS")]
  library(xtable)
  tabcode <- xtable(dat, caption = 'Simple', label = 'tab:TablePipe1')
  align(tabcode) <-  c( 'l', 'p{.6in}','p{1.2in}','p{1.2in}', 'p{2in}' )
  #sink(paste(fname, '.tex',sep = ""))
  #cat(txt)
  # print(tabcode,  include.rownames = F, table.placement = '!ht',
  # caption.placement = 'top') #, type = "html")
  rws <- seq(1, (nrow(dat)), by = 2)
  col <- rep("\\rowcolor[gray]{0.95}", length(rws))
  print(tabcode, booktabs = TRUE, include.rownames = F, table.placement = '!ht',
   caption.placement = 'top',
   add.to.row = list(pos = as.list(rws), command = col),
   comment=FALSE)
  ```
  
  ```{r echo=T, eval=F}
  library(disentangle); library(stringr)
  nodes <- newnode(indat = steps,   names_col = "STEP", in_col = "INPUTS",
    out_col = "OUTPUTS", clusters_col = "CLUSTER",
    nchar_to_snip = 70)
  sink("vignettes/steps-cluster.dot"); cat(nodes); sink()
  #DiagrammeR::grViz(nodes)
  system("dot -Tpdf vignettes/steps-cluster.dot -o vignettes/steps-cluster.pdf")
  
  ```
  
  \begin{figure}[!h]
  \centering
  \includegraphics[width=.3\textwidth]{steps-cluster.pdf}
  \caption{steps-cluster.pdf}
  \label{fig:steps-cluster.pdf}
  \end{figure}
  \clearpage
  
#+end_src

*** COMMENT advanced usage-code
#+name:advanced usage
#+begin_src R :session *R* :tangle vignettes/disentangle-pipelines.Rmd :exports none :eval no :padline
#Advanced usage

#+end_src


**** COMMENT planning a pipeline
#+name:rmd head
#+begin_src R :session *R* :tangle vignettes/disentangle-pipelines.Rmd :exports none :eval no :padline
##Planning a pipeline

TODO.  In this section I'll describe the ordering of the process.  Roughly:

1. decide on a research question 
1. select a modelling framework 
1. conceptualise the ideal analysis data
1. acquire and pre-process the measured data 
1. model selection  
1. sensitivity analysis
1. data checking
1. reporting
1. distribution of code and data

#+end_src
**** COMMENT Implementing a pipeline
#+name:rmd head
#+begin_src R :session *R* :tangle vignettes/disentangle-pipelines.Rmd :exports none :eval no :padline
##Implementing a pipeline

I characterise this as tracking steps, inputs and outputs in such a way that if an auditor were to access the pipeline they could scrutinise these.  An example is the mass replication study, `r citep(bib[["OpenScienceCollaboration2015"]])`.

TODO. In this section the basic scripted workflow is described using R and STATA as example.
I might mention workflow software such as Kepler, VisTrails, Taverna, Ruffus.  I might also mention electronic notebooks and IDE such as Rstudio, Notepad++, Emacs, Eclipse and IPython (Jupyter). But these things are outside the scope of this paper.

I'll use my Suicide and Drought paper as an example `r citep(bib[["Hanigan2012e"]])`.  The pipeline is shown in Figure \ref{fig:SuiDrtNSWoverviewtransformations}
\clearpage
\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{SuiDrtNSWoverview_transformations.jpg}
\caption{SuiDrtNSWoverview transformations}
\label{fig:SuiDrtNSWoverviewtransformations}
\end{figure}
\clearpage

#+end_src
**** COMMENT complicated journal paper
#+name:rmd head
#+begin_src R :session *R* :tangle vignettes/disentangle-pipelines.Rmd :exports none :eval no :padline
  
  ##Disentangling a complicated method in a journal paper
  
  In a more complicated example `r citet(bib[["Akita2014"]])` introduce
  the Bayesian Maximum Entropy (BME) method for combining air pollution
  data from land use regression (LUR) and chemical transport modeling
  (CTM) in a geostatistical framework.  The BME method combines various
  sources of data with different levels of uncertainty. The data are
  categorized into two groups: (i) hard data, corresponding to
  measurements; and (ii) soft data, having an uncertainty characterized
  by a probability density function.  At the prior stage describe the
  global characteristics of the spatial field. At the posterior stage
  site specific hard and soft data update the prior to estimate the
  value at any point. Air pollution data (NO2) were sourced for this
  study from monitoring stations, and a LUR and CTM. Many comparisons
  and error statistics are discussed in the paper but perhaps the best
  ‘take-home message’ is that the BME method proposed reduced the RMSE
  for validation data by approximately 45\% relative to CTM and LUR.
  This method is shown as the graph in Figure \ref{fig:Fig2}.
  
  ```{r Fig2, echo=F, eval=F}
  dir()
  ```
  
  
  
  \begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{BME_steps.pdf}
  \caption{steps fig2}
  \label{fig:Fig2}
  \end{figure}
  \clearpage
  
#+end_src





**** COMMENT Sharing data
#+name:rmd head
#+begin_src R :session *R* :tangle vignettes/disentangle-pipelines.Rmd :exports none :eval no :padline
#Sharing data
TODO. Data sharing and documentation is another important topic to cover.
I have developed R tools to assist with this.

1. data_dictionary: this produces a similar output to the FREQUENCIES command in SPSS
1. variable_names_and_labels: this summarises a (potentially cleaned) R `data.frame` whose attributes may be different from the source file
1. reml_boilerplate: an automated method for extracting bulk information about columns for an EML metadata document
1. data_inventory: a web2py database application that can be used to keep an inventory of data in a simple way that is also compliant with EML

#+end_src
*** COMMENT snip
#+name:rmd head
#+begin_src R :session *R* :tangle no :exports none :eval no :padline

#Methods

##Data and Study Area

Sed in augue non augue finibus lobortis. Maecenas imperdiet metus non
nisi imperdiet feugiat. Duis ac mauris metus. Nunc tempus est quis
metus consectetur, nec suscipit dui condimentum. Nam quis neque eu
magna

```{r Table1, results='asis', echo=FALSE, tab.cap="This is the first example table\\label{tab:Table1}",cache=FALSE}
kable(head(iris))
```

#Results

Nullam et accumsan urna, mollis vulputate dolor. Donec nec nisl
sagittis, laoreet nibh a, imperdiet eros. Ut sagittis ipsum
diam. Nulla auctor justo eu ante sodales sollicitudin. Aenean leo
lacus,

```{r Fig1, echo=FALSE, fig.cap="Just my first figure with a very fantastic caption.\\label{fig:Fig1}", cache=FALSE}
x<-rnorm(100)
y<-jitter(x,1000)
plot(x,y)

```

```{r Table2, results='asis', echo=FALSE,tab.cap="A second table showing some of the mtcars dataset.\\label{tab:Table2}",cache=FALSE}
kable(mtcars[10:16,])
```

Nullam et accumsan urna, mollis vulputate dolor. Donec nec nisl
sagittis, laoreet nibh a, imperdiet

```{r Fig2, echo=FALSE, fig.cap="Second figure showing a boxplot with ground breaking results. \\label{fig:Fig2}",cache=FALSE}
a<-sort(rnorm(100))
b<-c(rep("Group Small",35),rep("Group Big",65))
boxplot(a~b)
```

#+end_src
*** COMMENT conclusions
#+name:rmd head
#+begin_src R :session *R* :tangle vignettes/disentangle-pipelines.Rmd :exports none :eval no :padline
  
  
  #Conclusions 
  
  In this way the pipeline of steps for preparing and
  analysing data can be effectively managed and visualised
  
  ```{r, echo=FALSE, message=FALSE, eval = T}
  write.bibtex(file="components/manuscript.bib")
  ```
  #Acknowledgements
  - Joe Guillaume for introducing me to graphviz and first coding up a tool in python
  - Graphviz for inventing DOT
  - This document benefits from Hollister's template at https://github.com/jhollist/manuscriptPackage
  
  \clearpage
  
  #References
  
#+end_src

* Project Management
** 2013-12-02-research-protocol-we-used-for-our-bushfire-project
#+name:research-protocol-we-used-for-our-bushfire-project-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-12-02-research-protocol-we-used-for-our-bushfire-project.md :exports none :eval no :padline no
  ---
  name: 2013-12-02-research-protocol-we-used-for-our-bushfire-project
  layout: post
  title: research-protocol-we-used-for-our-bushfire-project
  date: 2013-12-02
  categories:
  - research methods
  ---

  - For a three year project on Bushfire smoke and Health we used the following structure in a wiki
  
  #### Sections:
      A.Background        
      B.Proposals         
      C.Approvals         
      D.Budget    
      E.Datasets  
      F.Analysis  
      G.Literature        
      H.Communication     
      I.Correspondance    
      J.Meetings  
      K.Completion        
      ContactDetails      
      README
      TODO        
  
  #### Conclusion
  - it worked quite well in the first year.
  - we didn't use it much after that.
  - it is still on the ANU webserver.  I sometimes refer back to it now, a couple of years later.

    
#+end_src

** 2013-12-02-research-protocol-for-manitoba-centre-for-health-policy-raw-U-Manitoba Centre for Health Policy Guidelines
These guidelines come from:

\noindent http://umanitoba.ca/faculties/health_sciences/medicine/units/community_health_sciences/departmental_units/mchp/protocol/media/manage_guidelines.pdf\\


Most of the material below is taken verbatim from the original. Unfortunately many of the items described below have links to internal MCHP documents that we cannot access.  Nonetheless the structure of the guidelines provides a useful skeleton to frame our thinking. 

The following areas should be reviewed with project team members near the beginning of the study and throughout the
project as needed:
- Confidentiality
- Project team
- File organization and documentation development
- Communication
- Administrative
- Report Preparation
- Project Completion
*** Confidentiality
Maintaining data access
*** Project Team Makeup
Roles and contact information should be documented on the project website for the following, where applicable (information may also be included on level of access approved for each team member).

**** Principal Investigator 
This is the lead person on the project, who assumes responsibility for delivering the project. The PI makes decisions on project direction and analysis requirements, with input from programmers and the research coordinator (an iterative process). If there is more than one PI (e.g., multi-site studies), overall responsibility for the study needs to be determined, and how the required work will be allocated and coordinated among the co-investigators. Researcher Workgroup website (internal link) 

**** Research Coordinator
Th RC is always assigned to deliverables and is usually brought in on other types of projects involving multiple sites, investigators and/or programmers. Responsibilities include project documentation, project management (e.g., ensuring that timelines are met, ensuring that project specifications are being followed), and working with both investigator(s) and the Programmer Coordinator throughout the project to coordinate project requirements. 

**** The Programmer Coordinator 
The PC is a central management role who facilitates assignment of programming resources to projects, ensuring the best possible match among programmers and investigators.
Research Coordinator Workgroup website(internal link) 

**** Programmer Analyst
This is primarily responsible for programming and related programming documentation (such that the purpose of the program and how results were derived can be understood by others). However, a major role may be taken in the analyses of the project as well, and this will characteristically vary with the project.
Programmer Analyst Workgroup website(internal link) 

**** Research Support
This is primarily responsible for preparing the final product (i.e., the report), including editing and formatting of final graphs and manuscript and using Reference Manager to set up the references. Research support also normally sets up and attends working group meetings. All requests for research support go through the Office Manager.

*** Project Team considerations
**** Roles
It is important to clarify everyone's roles at the beginning of the project; for example, whether the investigator routinely expects basic graphs and/or programming logs from the programmer.

**** Continuity
It is highly desirable to keep the same personnel, from the start of the project, where possible. It can take some time to develop a cohesive working relationship, particularly if work styles are not initially compatible. Furthermore, requesting others to temporarily fill in for team absences is generally best avoided, particularly for programming tasks (unless there is an extended period of absence). The original programmer will know best the potential impact of any changes that may need to be made to programming code.

**** Access levels
Access to MCHP internal resources (e.g., Windows, Unix) need to be assessed for all team members and set up as appropriate to their roles on the project.

**** Working group
A WG is always set up for deliverables (and frequently for other projects):
Terms of Reference for working group (internal)

**** Atmospherics
*** File organization and Documentation Development.
All project-related documentation, including key e-mails used to update project methodology, should be saved within the project directory. Resources for directory setup and file development include:
**** Managing MCHP resources
This includes various process documents as well as an overview of the documentation process for incorporating research carried out by MCHP into online resources: Documentation Management Guide (internal)
**** MCHP directory structure
A detailed outline of how the Windows environment is structured at MCHP
**** Managing project files
How files and sub-directories should be organized and named as per the MCHP Guide to Managing Project Files (internal pdf). Information that may be suitable for incorporating into MCHP online resources should be identified; for example, a Concept Development section for subsequent integration of a new concept(s) into the MCHP
Concept Dictionary. The deliverable glossary is another resource typically integrated into the MCHP Glossary.
**** Recommended Directories
NOTE this is a diversion from the MCHP guidelines.  These recommended directories are from a combination of sources that we have synthesised.
***** Background: concise summaries: possibly many documents for main project and any main analyses based on the 1:3:25 paradigm: one page of main messages; a three-page executive summary; 25 pages of detailed findings.
*****  Proposals: for documents related to grant applications.
*****  Approvals: for ethics applications.
*****  Budget: spreadsheets and so-forth.
*****  Data
******  dataset1
******  dataset2
*****  Paper1
******  Data
*******  merged dataset1 and 2
******  Analysis (also see http://projecttemplate.net for a programmer oriented template)
*******  exploratory analyses
*******  data cleaning
*******  main analysis
*******  sensitivity analysis
*******  data checking
*******  model checking
*******  internal review
******  Document
*******  Draft
*******  Journal1
********  rejected? :-(
*******  Journal2
********  Response to reviews
******  Versions: folders named by date - dump entire copies of the project at certain milestones/change points
******  Archiving final data with final published paper
*****  Papers 2, 3, etc: same structure as paper 1 hopefully the project spawns several papers 
*****  Communication: details of communication with stakeholders and decision makers
*****  Meetings: for organisation and records of meetings
*****  Contact details. table contacts lists
*****  Completion: checklists to make sure project completion is systematic.  Factor in a critical reflection of lessons learnt.
*****  References
*** Communication
Project communication should be in written form, wherever possible, to serve as reference for project documentation. Access and confidentiality clearance levels for all involved in the project will determine whether separate communication plans need to be considered for confidential information.
**** E-mail
provides opportunities for feedback/ discussion from everyone and for documenting key project decisions. Responses on any given issue would normally be copied to every project member, with the expectation of receiving feedback within a reasonable period of time - e.g.,a few days). The Research Coordinator should be copied on ALL project correspondence in order to keep the information up to date on the project website.
***** E-mail etiquette (internal) 
**** Meetings
Regularly-scheduled meetings or conference calls should include all project members where possible. Research Coordinators typically arrange project team meetings and take meeting minutes, while Research Support typically arranges the Working Group meetings.
***** Tips for taking notes (internal)
***** Outlook calendar
Used for booking rooms, it displays information on room availability and may include schedules of team members.
*** Administrative
**** Time entry 
Time spent on projects should be entered by all MCHP employees who are members of the project team.
***** website for time entry (internal) 
***** procedures for time entry (internal)

*** Report preparation
This includes:
- Policies - e.g., Dissemination of Research Findings
- Standards - e.g., deliverable production, use of logos, web publishing
- Guidelines - e.g., producing PDFs, powerpoint, and Reference Manager files
- Other resources - e.g., e-mail etiquette, technical resources, photos.

**** Reliability and Validity Checks
Making sure the numbers "make sense". Carrying out these checks requires spelling out who will do which checks.
***** Data Validity Checks 
A variety of things to check for at various stages of the study. Programming can be reviewed, for example, by checking to ensure all programs have used the right exclusions, the correct definitions, etc. , and output has been accurately transferred to graphs, tables, and maps for the report.
***** Discrepancies between data sources
In this case it is MCHP and Manitoba Health Reports - an example of cross-checking against another source of data.
*** Project Completion
Several steps need to take place to "finish" the project:
**** Final Project Meeting. 
Wind-up or debriefing meetings are held shortly after public release of a deliverable. Such meetings provide all team members with an opportunity to communicate what worked/did not work in bringing the project to completion, providing lessons learned for future deliverables.
**** Final Documentation Review. 
Findings from the wind-up meeting should be used to update and finalize the project website (including entering the date of release of report/paper). Both Windows and Unix project directories should be reviewed to ensure that only those SAS programs relevant to project analyses are kept (and well-documented) for future reference. Any related files which may be stored in a user directory should be moved to the project directory.
**** System Cleanup. 
When the project is complete, the Systems Administrator should be informed. Project directories, including program files and output data sets, will be archived to tape or CD. Tape backups are retained for a 5-year period before being destroyed so any project may be restored up to five years after completion.
**** Integration of new material to institution repository
This is with MCHP resource repository - a general overview of this process is described in General Documentation Process {internal}.
** 2013-12-02-research-protocol-for-manitoba-centre-for-health-policy
#+name:research-protocol-for-manitoba-centre-for-health-policy-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-12-02-research-protocol-for-manitoba-centre-for-health-policy.md :exports none :eval no :padline no
  ---
  name: 2013-12-02-research-protocol-for-manitoba-centre-for-health-policy
  layout: post
  title: Research Protocol For Manitoba Centre For Health Policy
  date: 2013-12-02
  categories:
  - disentangle
  - Project Management
  tags:
  - project management
  - Data Management System
  ---

  This post has moved here: [http://ivanhanigan.github.io/2015/10/research-protocol-for-manitoba-centre-for-health-policy/](http://ivanhanigan.github.io/2015/10/research-protocol-for-manitoba-centre-for-health-policy/).

  #### Version control
      2013-12-02: This post was originally released
      2015-10-02: The URL to the University Manitoba guidelines changed and has been updated.

#+end_src
** COMMENT 2015-10-02-research-protocol-for-manitoba-centre-for-health-policy
#+name:research-protocol-for-manitoba-centre-for-health-policy-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-02-research-protocol-for-manitoba-centre-for-health-policy.md :exports none :eval no :padline no
  ---
  name: 2015-10-02-research-protocol-for-manitoba-centre-for-health-policy
  layout: post
  title: Research Protocol For Manitoba Centre For Health Policy
  date: 2015-10-02
  categories:
  - disentangle
  - Project Management
  tags:
  - project management
  - Data Management System
  ---

  #### Version control
      2013-12-02: This post was originally released
      2015-10-02: The URL to the University Manitoba guidelines changed and has been updated.
      
  <p></p>

  - When we started a three year project on bushfire smoke we did some planning
  - We had a deep discussion about this research management protocol from University Manitoba Centre for Health Policy [http://umanitoba.ca/faculties/health_sciences/medicine/units/community_health_sciences/departmental_units/mchp/protocol/media/manage_guidelines.pdf](http://umanitoba.ca/faculties/health_sciences/medicine/units/community_health_sciences/departmental_units/mchp/protocol/media/manage_guidelines.pdf)
  - That URL has changed several times since I first downloaded the guidelines, so I will also host a copy here [UManitoba_MCHP_manage_guidelines.pdf](/pdfs/UManitoba_MCHP_manage_guidelines.pdf)
  
  <div id="content">
  <h1 class="title">Data Management Plan Checklist</h1>
  
  
  <div id="table-of-contents">
  <h2>Table of Contents</h2>
  <div id="text-table-of-contents">
  <ul>
  <li><a href="#sec-1">1 U-Manitoba Centre for Health Policy Guidelines</a>
  <ul>
  <li><a href="#sec-1-1">1.1 Confidentiality</a></li>
  <li><a href="#sec-1-2">1.2 Project Team Makeup</a>
  <ul>
  <li><a href="#sec-1-2-1">1.2.1 Principal Investigator</a></li>
  <li><a href="#sec-1-2-2">1.2.2 Research Coordinator</a></li>
  <li><a href="#sec-1-2-3">1.2.3 The Programmer Coordinator</a></li>
  <li><a href="#sec-1-2-4">1.2.4 Programmer Analyst</a></li>
  <li><a href="#sec-1-2-5">1.2.5 Research Support</a></li>
  </ul>
  </li>
  <li><a href="#sec-1-3">1.3 Project Team considerations</a>
  <ul>
  <li><a href="#sec-1-3-1">1.3.1 Roles</a></li>
  <li><a href="#sec-1-3-2">1.3.2 Continuity</a></li>
  <li><a href="#sec-1-3-3">1.3.3 Access levels</a></li>
  <li><a href="#sec-1-3-4">1.3.4 Working group</a></li>
  <li><a href="#sec-1-3-5">1.3.5 Atmospherics</a></li>
  </ul>
  </li>
  <li><a href="#sec-1-4">1.4 File organization and Documentation Development.</a>
  <ul>
  <li><a href="#sec-1-4-1">1.4.1 Managing MCHP resources</a></li>
  <li><a href="#sec-1-4-2">1.4.2 MCHP directory structure</a></li>
  <li><a href="#sec-1-4-3">1.4.3 Managing project files</a></li>
  <li><a href="#sec-1-4-4">1.4.4 Recommended Directories</a></li>
  </ul>
  </li>
  <li><a href="#sec-1-5">1.5 Communication</a>
  <ul>
  <li><a href="#sec-1-5-1">1.5.1 E-mail</a></li>
  <li><a href="#sec-1-5-2">1.5.2 Meetings</a></li>
  </ul>
  </li>
  <li><a href="#sec-1-6">1.6 Administrative</a>
  <ul>
  <li><a href="#sec-1-6-1">1.6.1 Time entry</a></li>
  </ul>
  </li>
  <li><a href="#sec-1-7">1.7 Report preparation</a>
  <ul>
  <li><a href="#sec-1-7-1">1.7.1 Reliability and Validity Checks</a></li>
  </ul>
  </li>
  <li><a href="#sec-1-8">1.8 Project Completion</a>
  <ul>
  <li><a href="#sec-1-8-1">1.8.1 Final Project Meeting.</a></li>
  <li><a href="#sec-1-8-2">1.8.2 Final Documentation Review.</a></li>
  <li><a href="#sec-1-8-3">1.8.3 System Cleanup.</a></li>
  <li><a href="#sec-1-8-4">1.8.4 Integration of new material to institution repository</a></li>
  </ul>
  </li>
  </ul>
  </li>
  </ul>
  </div>
  </div>
  
  <div id="outline-container-1" class="outline-2">
  <h2 id="sec-1"><span class="section-number-2">1</span> U-Manitoba Centre for Health Policy Guidelines</h2>
  <div class="outline-text-2" id="text-1">
  
  <p>These guidelines come from:
  </p>
  <p>
  \noindent <a href="http://umanitoba.ca/faculties/medicine/units/mchp/protocol/media/manage_guidelines.pdf">http://umanitoba.ca/faculties/medicine/units/mchp/protocol/media/manage_guidelines.pdf</a><br/>
  </p>
  <p>
  Most of the material below is taken verbatim from the original. Unfortunately many of the items described below have links to internal MCHP documents that we cannot access.  Nonetheless the structure of the guidelines provides a useful skeleton to frame our thinking. 
  </p>
  <p>
  The following areas should be reviewed with project team members near the beginning of the study and throughout the
  project as needed:
  </p><ul>
  <li>Confidentiality
  </li>
  <li>Project team
  </li>
  <li>File organization and documentation development
  </li>
  <li>Communication
  </li>
  <li>Administrative
  </li>
  <li>Report Preparation
  </li>
  <li>Project Completion
  </li>
  </ul>
  
  
  </div>
  
  <div id="outline-container-1-1" class="outline-3">
  <h3 id="sec-1-1"><span class="section-number-3">1.1</span> Confidentiality</h3>
  <div class="outline-text-3" id="text-1-1">
  
  <p>Maintaining data access
  </p></div>
  
  </div>
  
  <div id="outline-container-1-2" class="outline-3">
  <h3 id="sec-1-2"><span class="section-number-3">1.2</span> Project Team Makeup</h3>
  <div class="outline-text-3" id="text-1-2">
  
  <p>Roles and contact information should be documented on the project website for the following, where applicable (information may also be included on level of access approved for each team member).
  </p>
  
  </div>
  
  <div id="outline-container-1-2-1" class="outline-4">
  <h4 id="sec-1-2-1"><span class="section-number-4">1.2.1</span> Principal Investigator</h4>
  <div class="outline-text-4" id="text-1-2-1">
  
  <p>This is the lead person on the project, who assumes responsibility for delivering the project. The PI makes decisions on project direction and analysis requirements, with input from programmers and the research coordinator (an iterative process). If there is more than one PI (e.g., multi-site studies), overall responsibility for the study needs to be determined, and how the required work will be allocated and coordinated among the co-investigators. Researcher Workgroup website (internal link) 
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-2-2" class="outline-4">
  <h4 id="sec-1-2-2"><span class="section-number-4">1.2.2</span> Research Coordinator</h4>
  <div class="outline-text-4" id="text-1-2-2">
  
  <p>Th RC is always assigned to deliverables and is usually brought in on other types of projects involving multiple sites, investigators and/or programmers. Responsibilities include project documentation, project management (e.g., ensuring that timelines are met, ensuring that project specifications are being followed), and working with both investigator(s) and the Programmer Coordinator throughout the project to coordinate project requirements. 
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-2-3" class="outline-4">
  <h4 id="sec-1-2-3"><span class="section-number-4">1.2.3</span> The Programmer Coordinator</h4>
  <div class="outline-text-4" id="text-1-2-3">
  
  <p>The PC is a central management role who facilitates assignment of programming resources to projects, ensuring the best possible match among programmers and investigators.
  Research Coordinator Workgroup website(internal link) 
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-2-4" class="outline-4">
  <h4 id="sec-1-2-4"><span class="section-number-4">1.2.4</span> Programmer Analyst</h4>
  <div class="outline-text-4" id="text-1-2-4">
  
  <p>This is primarily responsible for programming and related programming documentation (such that the purpose of the program and how results were derived can be understood by others). However, a major role may be taken in the analyses of the project as well, and this will characteristically vary with the project.
  Programmer Analyst Workgroup website(internal link) 
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-2-5" class="outline-4">
  <h4 id="sec-1-2-5"><span class="section-number-4">1.2.5</span> Research Support</h4>
  <div class="outline-text-4" id="text-1-2-5">
  
  <p>This is primarily responsible for preparing the final product (i.e., the report), including editing and formatting of final graphs and manuscript and using Reference Manager to set up the references. Research support also normally sets up and attends working group meetings. All requests for research support go through the Office Manager.
  </p>
  </div>
  </div>
  
  </div>
  
  <div id="outline-container-1-3" class="outline-3">
  <h3 id="sec-1-3"><span class="section-number-3">1.3</span> Project Team considerations</h3>
  <div class="outline-text-3" id="text-1-3">
  
  
  </div>
  
  <div id="outline-container-1-3-1" class="outline-4">
  <h4 id="sec-1-3-1"><span class="section-number-4">1.3.1</span> Roles</h4>
  <div class="outline-text-4" id="text-1-3-1">
  
  <p>It is important to clarify everyone's roles at the beginning of the project; for example, whether the investigator routinely expects basic graphs and/or programming logs from the programmer.
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-3-2" class="outline-4">
  <h4 id="sec-1-3-2"><span class="section-number-4">1.3.2</span> Continuity</h4>
  <div class="outline-text-4" id="text-1-3-2">
  
  <p>It is highly desirable to keep the same personnel, from the start of the project, where possible. It can take some time to develop a cohesive working relationship, particularly if work styles are not initially compatible. Furthermore, requesting others to temporarily fill in for team absences is generally best avoided, particularly for programming tasks (unless there is an extended period of absence). The original programmer will know best the potential impact of any changes that may need to be made to programming code.
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-3-3" class="outline-4">
  <h4 id="sec-1-3-3"><span class="section-number-4">1.3.3</span> Access levels</h4>
  <div class="outline-text-4" id="text-1-3-3">
  
  <p>Access to MCHP internal resources (e.g., Windows, Unix) need to be assessed for all team members and set up as appropriate to their roles on the project.
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-3-4" class="outline-4">
  <h4 id="sec-1-3-4"><span class="section-number-4">1.3.4</span> Working group</h4>
  <div class="outline-text-4" id="text-1-3-4">
  
  <p>A WG is always set up for deliverables (and frequently for other projects):
  Terms of Reference for working group (internal)
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-3-5" class="outline-4">
  <h4 id="sec-1-3-5"><span class="section-number-4">1.3.5</span> Atmospherics</h4>
  <div class="outline-text-4" id="text-1-3-5">
  
  </div>
  </div>
  
  </div>
  
  <div id="outline-container-1-4" class="outline-3">
  <h3 id="sec-1-4"><span class="section-number-3">1.4</span> File organization and Documentation Development.</h3>
  <div class="outline-text-3" id="text-1-4">
  
  <p>All project-related documentation, including key e-mails used to update project methodology, should be saved within the project directory. Resources for directory setup and file development include:
  </p>
  </div>
  
  <div id="outline-container-1-4-1" class="outline-4">
  <h4 id="sec-1-4-1"><span class="section-number-4">1.4.1</span> Managing MCHP resources</h4>
  <div class="outline-text-4" id="text-1-4-1">
  
  <p>This includes various process documents as well as an overview of the documentation process for incorporating research carried out by MCHP into online resources: Documentation Management Guide (internal)
  </p></div>
  
  </div>
  
  <div id="outline-container-1-4-2" class="outline-4">
  <h4 id="sec-1-4-2"><span class="section-number-4">1.4.2</span> MCHP directory structure</h4>
  <div class="outline-text-4" id="text-1-4-2">
  
  <p>A detailed outline of how the Windows environment is structured at MCHP
  </p></div>
  
  </div>
  
  <div id="outline-container-1-4-3" class="outline-4">
  <h4 id="sec-1-4-3"><span class="section-number-4">1.4.3</span> Managing project files</h4>
  <div class="outline-text-4" id="text-1-4-3">
  
  <p>How files and sub-directories should be organized and named as per the MCHP Guide to Managing Project Files (internal pdf). Information that may be suitable for incorporating into MCHP online resources should be identified; for example, a Concept Development section for subsequent integration of a new concept(s) into the MCHP
  Concept Dictionary. The deliverable glossary is another resource typically integrated into the MCHP Glossary.
  </p></div>
  
  </div>
  
  <div id="outline-container-1-4-4" class="outline-4">
  <h4 id="sec-1-4-4"><span class="section-number-4">1.4.4</span> Recommended Directories</h4>
  <div class="outline-text-4" id="text-1-4-4">
  
  <p>NOTE this is a diversion from the MCHP guidelines.  These recommended directories are from a combination of sources that we have synthesised.
  </p><ul>
  <li id="sec-1-4-4-1">Background: concise summaries: possibly many documents for main project and any main analyses based on the 1:3:25 paradigm: one page of main messages; a three-page executive summary; 25 pages of detailed findings.<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-2">Proposals: for documents related to grant applications.<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-3">Approvals: for ethics applications.<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-4">Budget: spreadsheets and so-forth.<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-5">Data<br/>
  <ul>
  <li id="sec-1-4-4-5-1">dataset1<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-5-2">dataset2<br/>
  </li>
  </ul>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-6">Paper1<br/>
  <ul>
  <li id="sec-1-4-4-6-1">Data<br/>
  <ul>
  <li id="sec-1-4-4-6-1-1">merged dataset1 and 2<br/>
  </li>
  </ul>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-6-2">Analysis (also see <a href="http://projecttemplate.net">http://projecttemplate.net</a> for a programmer oriented template)<br/>
  <ul>
  <li id="sec-1-4-4-6-2-1">exploratory analyses<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-6-2-2">data cleaning<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-6-2-3">main analysis<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-6-2-4">sensitivity analysis<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-6-2-5">data checking<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-6-2-6">model checking<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-6-2-7">internal review<br/>
  </li>
  </ul>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-6-3">Document<br/>
  <ul>
  <li id="sec-1-4-4-6-3-1">Draft<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-6-3-2">Journal1<br/>
  <ul>
  <li id="sec-1-4-4-6-3-2-1">rejected? :-(<br/>
  </li>
  </ul>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-6-3-3">Journal2<br/>
  <ul>
  <li id="sec-1-4-4-6-3-3-1">Response to reviews<br/>
  </li>
  </ul>
  </li>
  </ul>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-6-4">Versions: folders named by date - dump entire copies of the project at certain milestones/change points<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-6-5">Archiving final data with final published paper<br/>
  </li>
  </ul>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-7">Papers 2, 3, etc: same structure as paper 1 hopefully the project spawns several papers<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-8">Communication: details of communication with stakeholders and decision makers<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-9">Meetings: for organisation and records of meetings<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-10">Contact details. table contacts lists<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-11">Completion: checklists to make sure project completion is systematic.  Factor in a critical reflection of lessons learnt.<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-4-4-12">References<br/>
  </li>
  </ul>
  </div>
  </div>
  
  </div>
  
  <div id="outline-container-1-5" class="outline-3">
  <h3 id="sec-1-5"><span class="section-number-3">1.5</span> Communication</h3>
  <div class="outline-text-3" id="text-1-5">
  
  <p>Project communication should be in written form, wherever possible, to serve as reference for project documentation. Access and confidentiality clearance levels for all involved in the project will determine whether separate communication plans need to be considered for confidential information.
  </p>
  </div>
  
  <div id="outline-container-1-5-1" class="outline-4">
  <h4 id="sec-1-5-1"><span class="section-number-4">1.5.1</span> E-mail</h4>
  <div class="outline-text-4" id="text-1-5-1">
  
  <p>provides opportunities for feedback/ discussion from everyone and for documenting key project decisions. Responses on any given issue would normally be copied to every project member, with the expectation of receiving feedback within a reasonable period of time - e.g.,a few days). The Research Coordinator should be copied on ALL project correspondence in order to keep the information up to date on the project website.
  </p><ul>
  <li id="sec-1-5-1-1">E-mail etiquette (internal)<br/>
  </li>
  </ul>
  </div>
  
  </div>
  
  <div id="outline-container-1-5-2" class="outline-4">
  <h4 id="sec-1-5-2"><span class="section-number-4">1.5.2</span> Meetings</h4>
  <div class="outline-text-4" id="text-1-5-2">
  
  <p>Regularly-scheduled meetings or conference calls should include all project members where possible. Research Coordinators typically arrange project team meetings and take meeting minutes, while Research Support typically arranges the Working Group meetings.
  </p><ul>
  <li id="sec-1-5-2-1">Tips for taking notes (internal)<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-5-2-2">Outlook calendar<br/>
  Used for booking rooms, it displays information on room availability and may include schedules of team members.
  </li>
  </ul>
  </div>
  </div>
  
  </div>
  
  <div id="outline-container-1-6" class="outline-3">
  <h3 id="sec-1-6"><span class="section-number-3">1.6</span> Administrative</h3>
  <div class="outline-text-3" id="text-1-6">
  
  
  </div>
  
  <div id="outline-container-1-6-1" class="outline-4">
  <h4 id="sec-1-6-1"><span class="section-number-4">1.6.1</span> Time entry</h4>
  <div class="outline-text-4" id="text-1-6-1">
  
  <p>Time spent on projects should be entered by all MCHP employees who are members of the project team.
  </p><ul>
  <li id="sec-1-6-1-1">website for time entry (internal)<br/>
  </li>
  </ul>
  <ul>
  <li id="sec-1-6-1-2">procedures for time entry (internal)<br/>
  
  </li>
  </ul>
  </div>
  </div>
  
  </div>
  
  <div id="outline-container-1-7" class="outline-3">
  <h3 id="sec-1-7"><span class="section-number-3">1.7</span> Report preparation</h3>
  <div class="outline-text-3" id="text-1-7">
  
  <p>This includes:
  </p><ul>
  <li>Policies - e.g., Dissemination of Research Findings
  </li>
  <li>Standards - e.g., deliverable production, use of logos, web publishing
  </li>
  <li>Guidelines - e.g., producing PDFs, powerpoint, and Reference Manager files
  </li>
  <li>Other resources - e.g., e-mail etiquette, technical resources, photos.
  </li>
  </ul>
  
  
  
  </div>
  
  <div id="outline-container-1-7-1" class="outline-4">
  <h4 id="sec-1-7-1"><span class="section-number-4">1.7.1</span> Reliability and Validity Checks</h4>
  <div class="outline-text-4" id="text-1-7-1">
  
  <p>Making sure the numbers "make sense". Carrying out these checks requires spelling out who will do which checks.
  </p><ul>
  <li id="sec-1-7-1-1">Data Validity Checks<br/>
  A variety of things to check for at various stages of the study. Programming can be reviewed, for example, by checking to ensure all programs have used the right exclusions, the correct definitions, etc. , and output has been accurately transferred to graphs, tables, and maps for the report.
  </li>
  </ul>
  <ul>
  <li id="sec-1-7-1-2">Discrepancies between data sources<br/>
  In this case it is MCHP and Manitoba Health Reports - an example of cross-checking against another source of data.
  </li>
  </ul>
  </div>
  </div>
  
  </div>
  
  <div id="outline-container-1-8" class="outline-3">
  <h3 id="sec-1-8"><span class="section-number-3">1.8</span> Project Completion</h3>
  <div class="outline-text-3" id="text-1-8">
  
  <p>Several steps need to take place to "finish" the project:
  </p>
  </div>
  
  <div id="outline-container-1-8-1" class="outline-4">
  <h4 id="sec-1-8-1"><span class="section-number-4">1.8.1</span> Final Project Meeting.</h4>
  <div class="outline-text-4" id="text-1-8-1">
  
  <p>Wind-up or debriefing meetings are held shortly after public release of a deliverable. Such meetings provide all team members with an opportunity to communicate what worked/did not work in bringing the project to completion, providing lessons learned for future deliverables.
  </p></div>
  
  </div>
  
  <div id="outline-container-1-8-2" class="outline-4">
  <h4 id="sec-1-8-2"><span class="section-number-4">1.8.2</span> Final Documentation Review.</h4>
  <div class="outline-text-4" id="text-1-8-2">
  
  <p>Findings from the wind-up meeting should be used to update and finalize the project website (including entering the date of release of report/paper). Both Windows and Unix project directories should be reviewed to ensure that only those SAS programs relevant to project analyses are kept (and well-documented) for future reference. Any related files which may be stored in a user directory should be moved to the project directory.
  </p></div>
  
  </div>
  
  <div id="outline-container-1-8-3" class="outline-4">
  <h4 id="sec-1-8-3"><span class="section-number-4">1.8.3</span> System Cleanup.</h4>
  <div class="outline-text-4" id="text-1-8-3">
  
  <p>When the project is complete, the Systems Administrator should be informed. Project directories, including program files and output data sets, will be archived to tape or CD. Tape backups are retained for a 5-year period before being destroyed so any project may be restored up to five years after completion.
  </p></div>
  
  </div>
  
  <div id="outline-container-1-8-4" class="outline-4">
  <h4 id="sec-1-8-4"><span class="section-number-4">1.8.4</span> Integration of new material to institution repository</h4>
  <div class="outline-text-4" id="text-1-8-4">
  
  <p>This is with MCHP resource repository - a general overview of this process is described in General Documentation Process {internal}.
  </p></div>
  </div>
  </div>
  </div>
  </div>
  
  </body>
      
#+end_src


** 2015-03-15-tuftes-gantt-alternative-for-detail-within-context

*** blog
  
  #### Blocker:
      property which allows you to state that a task depends on either
      a previous sibling ("previous-sibling") or
      any other task by stating the task_id property of the predecessor

#+begin_src R :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-03-15-tuftes-gantt-alternative-for-detail-within-context.md :exports none :eval no :padline no
  ---
  name: tuftes-gantt-alternative-for-detail-within-context 
  layout: post
  title: tuftes-gantt-alternative-for-detail-within-context 
  date: 2015-03-15
  categories:
  - project management
  ---
  
  - During the end of 2014 I found that the Gantt Chart by TaskJuggler was a struggle to really achieve any decent task management with (fine for higher level overviews though).
  -   I had been following the approach described at [this link](http://orgmode.org/worg/org-tutorials/org-taskjuggler.html)
  - I decided to code up an alternative based on the theory explained on [this link](http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=000076) 
  
  
  #### Project Management Graphics (or Gantt Charts), by Edward Tufte
      Computer screens are generally too small for an overview of big
      serious projects. Horizontal and vertical scrolling are necessary to
      see more than about 40 horizontal time lines for a reasonable period
      of time. Thus, for large projects, print out the sequence on a big
      roll of paper and put it up on a wall.
       
      The chart might be retrospective as well as prospective. That is, the
      chart should show actualdates of achieved goals, evidence which will
      continuously reinforce a reality principle on the mythical future
      dates of goal achievement.
       
      Most of the Gantt charts are analytically thin, too simple, and lack
      substantive detail. The charts should be more intense. At a minimum,
      the charts should be annotated--for example, with to-do lists at
      particular points on the grid. Costs might also be included in
      appropriate cells of the table.
       
      About half the charts show their thin data in heavy grid prisons. For
      these charts the main visual statement is the administrative grid
      prison, not the actual tasks contained by the grid. No explicitly
      expressed grid is necessary--or use the ghost-grid graph
      paper. Degrid!
  
  #### The Results:
  
  I used the example for a fictional Journal Paper submission from my favourite reference for anything to do with Project Management:

      Aragon, T., Mier, H. M., Payauys, T., & Siador, C. (2012). 
      Project Management for Health Professionals.
    [http://www.academia.edu/1746564/Project_Management_for_Health_Professionals](http://www.academia.edu/1746564/Project_Management_for_Health_Professionals)    


  <p></p>

  With the following results (PS SVG format allows you to zoom in).

  ![alttext2](/images/gantt_tufte_test.svg)
  
  #### The codes: 
      library(disentangle)
      library(sqldf)
      library(lubridate)
      
      datin  <- read.csv(
      textConnection("
      container_task_title  , task_id                      , allocated , fte , blocker               ,       start_date , effort , status , notes 
      01 Start              , Start                        , ivan      ,   1 , NA                    ,       2015-03-15 ,     1d , DONE   , NA    
      02 Update Lit Review  , Repeat MEDLINE search        , ivan      ,   1 , Start                 ,       2015-03-16 ,     5d , DONE   , NA    
      02 Update Lit Review  , Retrieve articles            , ivan      ,   1 , Repeat MEDLINE search ,               NA ,     5d , DONE   , NA    
      02 Update Lit Review  , Read articles                , ivan      ,   1 ,                       ,       2015-03-26 ,    11d , DONE   ,       
      02 Update Lit Review  , Summarize articles           , ivan      ,   1 ,                       ,       2015-04-06 ,     9d , TODO   ,       
      03 Write Draft        , Write introduction           , ivan      ,   1 ,                       ,       2015-04-09 ,     6d , TODO   ,       
      03 Write Draft        , Write methods                , ivan      ,   1 , Start                 ,                  ,    15d , TODO   ,       
      03 Write Draft        , Write results                , ivan      ,   1 ,                       ,       2015-03-30 ,    10d , TODO   ,       
      03 Write Draft        , Write discussion             , ivan      ,   1 ,                       ,       2015-04-15 ,    10d , TODO   ,       
      04 Internal Review    , Send to co-author for review , ivan      ,   1 , Write discussion      ,                  ,     2d , TODO   ,        
      04 Internal Review    , Revise draft 1               , ivan      ,   1 ,                       ,       2015-04-19 ,    10d , TODO   ,       
      05 Peer Review        , Submit article 1             , ivan      ,   1 , Revise draft 1        ,                  ,     5d , TODO   ,       
      06 Revise and Resubmit, Revise draft 2               , ivan      ,   1 ,                       ,       2015-04-30 ,    10d , TODO   ,       
      06 Revise and Resubmit, Submit article 2             , ivan      ,   1 , Revise draft 2        ,                  ,     5d , TODO   ,       
      07 End                , Accepted                     , ivan      ,   1 ,                       ,       2015-05-15 ,     1d , TODO   ,       
      "),
      stringsAsFactor = F, strip.white = T)
      # or 
      # datin <- get_gantt_data("gantt_todo", test_data = T) # need to
      # adjust min_context_xrange to 2015-01-01 or something
      datin$start_date  <- as.Date(datin$start_date)
      str(datin)
      datin
      
      dat_out <- gantt_data_prep(dat_in = datin)
      str(dat_out)
      dat_out
      svg("tests/gantt_tufte_test.svg",height=10,width=8)
      gantt_tufte(dat_out, focal_date = "2015-04-13", time_box = 3*7,
                  min_context_xrange = "2015-03-16",
                  cex_context_ylab = 0.65, cex_context_xlab = .7,
                  cex_detail_ylab = 0.9,  cex_detail_xlab = .4,
                  show_today = F)
      dev.off()
      
  
  
  
      
#+end_src

** gantt_tufte
*** EXAMPLES
/home/ivan_hanigan/projects/asn-ltern.bitbucket.org/ServerDetails/gantt-chart

*** COMMENT R-gantt_tufte_test_data
#+name:gantt_tufte
#+begin_src R :session *R* :tangle no :exports none :eval yes
  # func
  library(sqldf)
  library(lubridate)
  library(swishdbtools)
  ch <- connect2postgres('localhost','gantt_tufte2', 'w2p_user', p='xpassword')
  pgListTables(ch, "public")
  
  # load
  datin  <- read.csv(textConnection("container_task_title, task_id, allocated, fte, blocker, start_date, effort
  Container 1, task 0, jim,   1,   ,     2014-12-01, 1m
  Container 1, task 1, jim,   1,   ,     2014-12-20, 1m
  Container 1, task 2, bob,   1, task 1,           , 10d 
  Container 2, task 3, sue,   1,   ,     2014-12-01, 2w
  Container 2, task 4, jim,   1, task 3,           , 2d
  Container 3, task 5, jimmy, 1, task 3,           , 10d
  Container 3, task 6, jimmy, 1,       , 2015-01-01, 10d
  Container 4, task 7, jimmy, 1, task 3,           , 10d
  "),
  stringsAsFactor = F, strip.white = T)
  datin$start_date  <- as.Date(datin$start_date)
  str(datin)
  datin
  
  cnt  <- sqldf("select container_task from datin group by container_task", drv = "SQLite")
  cnt$key_contact  <- NA
  cnt$abstract  <- NA
  cnt
  dbWriteTable(ch, "container_task", cnt, append = T)
  cnt  <- dbReadTable(ch, "container_task")
  cnt
  
  paste(  names(datin), sep = "", collapse = ", ")
  datin2  <- sqldf("select id as container_id, task_id, allocated, fte, blocker, start_date, effort
  from cnt
  join datin
  on cnt.container_task_title = datin.container_task", drv = "SQLite")
  datin2
  datin2$notes_issues  <- NA
  dbWriteTable(ch, "work_package", datin2, append = T)
  
  # psql got munteded, so revert to sqlite, tried swapping to sqlite, noto
  
  ## drv <- dbDriver("SQLite")
  ## tfile <- tempfile()
  ## con <- dbConnect(drv, dbname = "~/tools/web2py/applications/gantt_tufte/databases/storage.sqlite")
  ## dbListTables(con)
  ## datin2 <- dbGetQuery(con , "select * from work_package")
  ## dbWriteTable(ch, "work_package", datin2, append = T)
  
  
  # ended up deleteing from the applications folder
  
   
#+end_src

#+RESULTS: gantt_tufte
=1
==1
==1
==2
==2
==2
==2
==2
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==2
==1
==1
==1
==1
==1
==2
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==2
==2
==1
==1
==2
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==2
==1
==1
==2
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==2
==1
==1
==1
==TRUE
==1
==1
==1
==1
==1
==1
==1
==1
==1

*** COMMENT R-gantt_tufte_preprocessing
#+name:gantt_tufte
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################ 
  gantt_tufte_preprocessing  <- function(
    indat = datin
    ){
    # self join to collect the dependencies
    # paste(names(datint), sep = "", collapse = ", ")
    library(sqldf)
    library(lubridate)
    indat
    #indat$indat_id <- paste(indat$container_task, indat$task_id, sep = "_")
    # self join to return dependents
    indat2 <- sqldf("
    select t1.container_task,
    t1.task_id as predecessor,
    t2.task_id, t2.efforti,
    t1.end
    from indat t1
    left join
    indat t2
    on t1.task_id = t2.blocker
    
    ", drv = 'SQLite')
    #where t2.task_id is not null 
    indat2
    # get any other containers... not sure this helps
    indat2_1 <- sqldf("select t1.container_task, t1.predecessor, t2.predecessor as task_id,
    t2.efforti,
    t2.end
    from indat2 t1
    join
    indat2 t2
    where t1.predecessor = t2.task_id")
    indat2_1
    indat2$start  <- indat2$end 
    indat2$end  <- indat2$start + indat2$efforti
    indat2_1$start  <- indat2_1$end 
    indat2_1$end  <- indat2_1$start + indat2_1$efforti
    indat2  <- indat2[!is.na(indat2$start) & !is.na(indat2$end) ,]
    indat2
    indat2_1
    indat2 <- rbind(indat2, indat2_1)
    
    indat2 <- unique(indat2)
    # now you know the start of the dependents
    
    # now get other independent tasks
    indat3 <- sqldf("select container_task,
    task_id as predecessor,
    task_id,
    efforti,
    end, start
    from indat
    where start is not null
    ")
    # TODO at this point need to figure out how to get proper locs
    #indat3$loc <- nrow(indat3):1
    indat3
    indat2 
    # add loc of siblings
    ## indatx <- sqldf("select t1.*, t2.loc
    ## from indat2 t1
    ## left join
    ## indat3 t2
    ## where (t1.predecessor = t2.task_id)
    ## and t1.task_id is not null
    ## ")
    #indatx
    
    indat4 <- rbind(indat2, indat3)
    indat4 <- indat4[order(indat4$start),]
    indat4[order(indat4$container_task),]
    indat4 
    return(indat4)
  }
  datin2 <- indat4
  #datin2 <- gantt_tufte_preprocessing(datin)
  #str(datin2)
    
#+end_src

*** R timebox

#+name:timebox
#+begin_src R :session *R* :tangle R/timebox.R :exports none :eval yes
  #### name:timebox####
  # func to calculate time boxes
  timebox <- function(dat_in){
    # dat_in  <- datin
    if(
      !exists("dat_in$end_date")
      ) dat_in$end_date <- NA
    # str(dat_in)
    nameslist <- names(dat_in)
    dat_in$effortt <- as.numeric(gsub("[^\\d]+", "", dat_in$effort, perl=TRUE))
    dat_in$effortd <- gsub("d", 1, gsub("[[:digit:]]+", "", dat_in$effort, perl=TRUE))
    dat_in$effortd <- gsub("w", 7, dat_in$effortd)
    dat_in$effortd <- gsub("m", 30.5, dat_in$effortd)
    dat_in$effortd <- as.numeric(dat_in$effortd)
    dat_in$efforti <- dat_in$effortt * dat_in$effortd
    dat_in[is.na(dat_in$end_date),"end_date"] <- dat_in[is.na(dat_in$end_date),"start_date"] + dat_in[is.na(dat_in$end_date),"efforti"]
    dat_in$end_date  <- as.Date(dat_in$end_date, '1970-01-01')
    #   str(dat_in)
    dat_in <- dat_in[,c(nameslist, "efforti")]
    return(dat_in)
  }
  
#+end_src

#+RESULTS: timebox

*** get_gantt_data-code ETL
#+name:get_test_data
#+begin_src R :session *R* :tangle R/get_gantt_data.R :exports none :eval no
  
  library(sqldf)
  library(lubridate)
  library(swishdbtools)
  
  
  get_gantt_data <- function(
    dbname = 'gantt_todo'
    ,
    test_data = T
    ){
  if(test_data != TRUE){
  #### name:get_test_data####
  if(exists("ch"))  dbDisconnect(ch)
  ch <- connect2postgres2(dbname)
  
  datin  <- dbGetQuery(ch,
  "
  select t1.container_task_title, 
  t2.*
  from container_task t1
  join work_package t2
  on t1.id = t2.container_id
  where t2.status != 'DONTSHOW'
  order by container_task_title"
  )
  str(datin)
  datin_done  <- dbGetQuery(ch,
  "
  select t1.container_task_title, 
  t2.*
  from container_task_done t1
  join work_package_done t2
  on t1.id = t2.container_id
  where t2.status != 'DONTSHOW'
  "
  )
  str(datin_done)
  datin  <- rbind(datin, datin_done)
  } else {
  # or simpler
  datin  <- read.csv(textConnection("container_task_title, task_id, allocated, fte, blocker, start_date, effort, status, notes
    Container 1, task 0, jim,   1,   ,     2015-01-01, 1m  , DONE,  
    Container 1, task 1, jim,   1,   ,     2015-01-20, 1m  , DONE,  
    Container 1, task 2, bob,   1, task 1,           , 10d , TODO, This is a note 
    Container 2, task 3, sue,   1,   ,     2015-01-01, 2w  , TODO,  
    Container 2, task 4, jim,   1, task 3,           , 2d  , TODO,  
    Container 3, task 5, jimmy, 1, task 3,           , 10d , TODO,  
    Container 3, task 6, jimmy, 1,       , 2015-02-01, 10d , TODO,  
    Container 4, task 7, jimmy, 1, task 0,           , 10d , TODO,  
    Container 5, task 8, sue,   1,       , 2015-01-14, 5d  , TODO,  
    Container 5, task 9, sue,   1, task 8, , 2d            , TODO,  
    Container 5, task 10, sue,   1, task 9, , 2d           , TODO,  
    Container 5, task 11, sue,   1, task 10, , 2d          , TODO,  
    Container 5, task 12, sue,   1, task 11, , 2d          , TODO,  
    Container 5, task 13, sue,   1, task 12, , 2d          , TODO,  
    Container 5, task 14, sue,   1, task 13, , 2d          , TODO,  
    "),
    stringsAsFactor = F, strip.white = T)
    datin$start_date  <- as.Date(datin$start_date)
    str(datin)
    datin[datin$blocker == "","blocker"] <- NA
  # datin
  }
  
  return(datin)
  }

#+end_src

*** gantt_data_prep-code ETL
#+name:get_test_data
#+begin_src R :session *R* :tangle R/gantt_data_prep.R :exports none :eval no
  
  gantt_data_prep <- function(
    dat_in = datin
    ){
    dat_in <- timebox(dat_in)
    dat_in[1:5,c("task_id","start_date","end_date", "efforti")]
    str(dat_in)
    dat_in  <- dat_in[,c('container_task_title','task_id','allocated','fte','start_date','efforti','notes','status','blocker','end_date')]
    t(dat_in[1,])
    #dat_in
    # dbSendQuery(ch, "drop table indat")
    # dbWriteTable(ch, "indat", dat_in)
    
    indat <- dat_in
    dat_in_depends <- sqldf("
    select tab1.container_task_title, tab1.task_id, 
    'depends on ' || tab1.blocker || ' from Container ' || tab2.container_task_title as depends_on,
    tab2.end_date as start_date, 
    tab1.efforti, tab1.status
    from
    (
      select t1.container_task_title,
      t1.task_id, t1.blocker,
      t1.start_date,
      t1.end_date,
      t1.efforti, t1.status
      from indat t1
      where t1.blocker is not null
      ) tab1
    join
    indat tab2
    on tab1.blocker = tab2.task_id
    ", drv = "SQLite")
    # cast(tab2.end_date + (tab1.efforti || ' day')::INTERVAL as date) as
    # end_date
    dat_in_depends[1,]
    #dat_in_depends
    dat_in_depends$end_date  <- dat_in_depends$start_date + dat_in_depends$tab1.efforti
    names(dat_in_depends) <- gsub('tab1.', '', names(dat_in_depends))
    
    dat_in <- sqldf("
      select t1.container_task_title,
      t1.task_id, 
      t1.task_id as depends_on,  
      t1.start_date,
      t1.efforti,
      t1.status,
      t1.end_date
      from indat t1
      where t1.blocker is null or t1.blocker = ''
      order by container_task_title
    ", drv = 'SQLite')
    dat_in[,1]
    dat_in <- rbind(dat_in, dat_in_depends)
    dat_in[1,]
    #dat_in
    loc  <- sqldf("select container_task_title from dat_in group by container_task_title", drv = "SQLite")
    loc$loc  <- nrow(loc):1
    loc
    dat_in <- merge(loc, dat_in)
    str(dat_in)
    loc
    dat_out <- as.data.frame(matrix(NA, nrow = 0, ncol = ncol(dat_in) + 1))
    #names(qc) <- c(names(dat_in),"loc2")
    for(loci in loc$loc){
    # loci = loc$loc[1]
    qc <- dat_in[dat_in$loc == loci,]
    qc <- qc[order(qc$start_date),]
    loc2 <- seq(qc$loc[1]-1, qc$loc[1],  1/(length(qc$loc)))
    qc$loc2  <- loc2[(length(loc2)):2] 
    
    dat_out  <- rbind(dat_out, qc)
    
    }
    str(dat_out)
    return(dat_out)
    }
      
#+end_src

#+RESULTS: get_test_data

*** R-gantt_tufte PLOT
#+name:gantt_tufte
#+begin_src R :session *R* :tangle R/gantt_tufte.r :exports none :eval yes
  ################################################################
  # plot 
  
  gantt_tufte <- function(
    indat = dat_out
    ,
    smidge_lab = .15
    ,
    focal_date = '2015-01-18' # Sys.Date()
    , 
    show_today = TRUE
    ,
    time_box = 7 * 2.5
    ,
    end_task_ticks = F
    ,
    cex_context_ylab = 0.2
    ,
    cex_context_xlab = 0.5
    ,
    cex_context_points = 0.5
    ,
    min_context_xrange =  NA
    , 
    max_context_xrange = NA
    ,
    cex_detail_ylab = 0.7
    ,
    cex_detail_xlab = 1
    ,
    cex_detail_points = 0.7
    ,
    cex_detail_labels = 0.7
    ){
    focal_date <- as.Date(focal_date)
    m <- matrix(c(1,2), 2, 1)
    layout(m, widths=c(1), heights=c(.75,4))
    par(mar = c(3,16,2,1))
    # layout.show(2)
  
  
    yrange <- c((min(indat$loc2) - smidge_lab), (max(indat$loc2) + smidge_lab))
    if(!is.na(min_context_xrange)){
    xmin <- as.Date(min_context_xrange)    
    } else {
    xmin <- min(indat$start_date, na.rm = T)
    }
    if(!is.na(max_context_xrange)){
    xmax <- as.Date(max_context_xrange)    
    } else {
    xmax <- max(indat$start_date, na.rm = T)
    }
  
    xrange  <- c(xmin,xmax)
    
    # xrange
    #### context ####
    
    plot(xrange, yrange, type = 'n', xlab = "", ylab = "", axes = F )
    indat_lab  <- sqldf("select container_task_title, loc from indat group by container_task_title, loc", drv = "SQLite")
    mtext(c(indat_lab$container_task_title), 2, las =1, at = indat_lab$loc, cex = cex_context_ylab)
  
    polygon(c(focal_date, focal_date + time_box, focal_date + time_box, focal_date), c(rep(yrange[1],2), rep(yrange[2],2)), col = 'lightyellow', border = 'lightyellow')
  # DONE is grey
  indat_done <- indat[indat$status == 'DONE',]
    points(indat_done$start_date, indat_done$loc2, pch = 16, cex = cex_context_points, col = 'grey')
    #text(indat_done$start_date, indat_done$loc2 - smidge_lab, labels = indat_done$task_id, pos = 4)
    js <- indat_done$loc2
    for(i in 1:nrow(indat_done)){
    # = 1
      segments(indat_done$start_date[i] , js[i] , indat_done$start_date[i] , max(indat_done$loc2) + 1 , lty = 3, col = 'grey')
      segments(indat_done$start_date[i] , js[i] , indat_done$end_date[i] , js[i], col = 'grey')
    }
  # indat todo is black
  indat_todo <- indat[indat$status == 'TODO',]
    points(indat_todo$start_date, indat_todo$loc2, pch = 16, cex = cex_context_points)
    #text(indat_todo$start_date, indat_todo$loc2 - smidge_lab, labels = indat_todo$task_id, pos = 4)
    js <- indat_todo$loc2
    for(i in 1:nrow(indat_todo)){
    # = 1
      segments(indat_todo$start_date[i] , js[i] , indat_todo$start_date[i] , max(indat_todo$loc2) + 1 , lty = 3)
      segments(indat_todo$start_date[i] , js[i] , indat_todo$end_date[i] , js[i] )
    }  
    #segments(focal_date, yrange[1], focal_date, yrange[2], 'red')
    xstart_date <- ifelse(wday(xrange[1]) != 1, xrange[1] - (wday(xrange[1]) - 2), xrange[1])
    xend <- ifelse(wday(xrange[2]) != 7, xrange[2] + (5-wday(xrange[2])), xrange[2] )
    at_dates  <- seq(xstart_date, xend, 7)
    label_dates  <-
      paste(month(as.Date(at_dates, "1970-01-01"), label = T),
      day(as.Date(at_dates, "1970-01-01")),
      sep = "-")
  
    axis(1, at = at_dates, labels = label_dates, cex.axis = cex_context_xlab)
    #axis(3)
    if(show_today) segments(Sys.Date(), min(js), Sys.Date(), max(js), lty = 2, col = 'blue')
    
    #### detail ####
    js <- indat$loc2
    # todo
    plot(c(focal_date, focal_date + time_box), yrange, type = 'n', xlab = "", ylab = "", axes = F)
         
    mtext(c(indat_lab$container_task_title), 2, las =1, at = indat_lab$loc, cex = cex_detail_ylab)
    points(indat$start_date, indat$loc2, pch = 16, cex = cex_detail_points)
    text(indat$start_date, indat$loc2 - smidge_lab, labels = indat$task_id, pos = 4,
         cex = cex_detail_labels)
    for(i in 1:nrow(indat)){
    # = 1
      segments(indat$start_date[i] , js[i] , indat$start_date[i] , max(indat$loc2) + 1 , lty = 3,
        col = ifelse(indat$status[i] == "DONE", "grey","black"))
      segments(indat$start_date[i] , js[i] , indat$end_date[i] , js[i],
        col = ifelse(indat$status[i] == "DONE", "grey","black"))
    }
    # done
    indat_done  <- indat[indat$status == "DONE",]
    points(indat_done$start_date, indat_done$loc2, pch = 16, cex = cex_detail_points, col = "darkgrey")
    text(indat_done$start_date, indat_done$loc2 - smidge_lab, labels = indat_done$task_id, pos = 4,
         cex = cex_detail_labels, col = "darkgrey")  
    for(i in 1:nrow(indat_done)){
    # = 1
      segments(indat_done$start_date[i] , indat_done$loc2[i] , indat_done$start_date[i] , max(indat_done$loc2) + 1 , lty = 3, col = 'darkgrey')
      segments(indat_done$start_date[i] , indat_done$loc2[i] , indat_done$end_date[i] , indat_done$loc2[i], col = 'darkgrey' )
    }
  
    # continuing
  
    bumped_up <- indat[indat$start_date < focal_date & indat$status != 'DONE',]
    if(nrow(bumped_up) > 0){
    text(focal_date, bumped_up$loc2 - smidge_lab, labels = bumped_up$task_id, pos = 4,
         cex = cex_detail_labels, col = 'darkred')
    }

    bumped_up2 <- indat[indat$start_date < focal_date & indat$status == 'DONE' & indat$end_date >= focal_date,]
    if(nrow(bumped_up2) > 0){
    text(focal_date, bumped_up2$loc2 - smidge_lab, labels = bumped_up2$task_id, pos = 4,
         cex = cex_detail_labels, col = 'grey')
    }
    
    # overdue
    ## bumped_up <- indat[indat$end_date < focal_date & indat$status != 'DONE',]
    ## text(focal_date, bumped_up$loc2 - smidge_lab, labels = bumped_up$task_id, pos = 4,
    ##      cex = cex_detail_labels, col = 'darkorange')
    
    #segments(focal_date, yrange[1], focal_date, yrange[2], 'red')
    xstart_date <- ifelse(wday(focal_date) != 1, focal_date - (wday(focal_date) - 2), focal_date)
    xend <- ifelse(wday(focal_date + time_box) != 7, (focal_date + time_box) + (5-wday(focal_date + time_box)), (focal_date + time_box))
    at_dates  <- seq(xstart_date, xend, 1)
    at_dates2  <- seq(xstart_date, xend, 7)
    
    label_dates  <-
      paste(month(as.Date(at_dates2, "1970-01-01"), label = T),
      day(as.Date(at_dates2, "1970-01-01")),
      sep = "-")
  
    axis(1, at = at_dates, labels = F)
    axis(1, at = at_dates2, labels = label_dates,  cex = cex_detail_xlab)
    #segments(min(xrange), min(yrange) - .09, max(xrange), min(yrange) - .09)
    axis(3, at = at_dates, labels = F)
    axis(3, at = at_dates2, labels = label_dates)
    #segments(min(xrange), max(yrange) + .09, max(xrange), max(yrange) + .09)  
    if(show_today) segments(Sys.Date(), min(js), Sys.Date(), max(js) + 1, lty = 2, col = 'blue')
    
  }
  #ls()
  
#+end_src
*** man-gantt_tufte
#+name:gantt_tufte
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # arguments: gantt_tufte
  # this is a ploting function, depends on timebox and preprocessing 

  # args
  ## indat = datin4
  ## smidge_lab = .15
  ## focal_date = Sys.Date()
  ## time_box = 21
  ## end_task_ticks = F # this is the little tick marking the end of the tasks

#+end_src


      
*** test go
**** COMMENT test1-code

#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test2####
  source("R/timebox.R")
  source("R/get_gantt_data.R")
  source("R/gantt_data_prep.R")
  source("R/gantt_tufte.r")
  dat_in <- get_gantt_data(test_data = F)
  dat_out <- gantt_data_prep(dat_in)
  str(dat_out)
    svg("tests/AAPL.svg",height=26,width=18)
    gantt_tufte(dat_out, focal_date = "2015-01-10", cex_context_ylab = 0.45,
     min_context_xrange = "2015-01-01", time_box = 7 * 3.5, cex_detail_xlab = .4)
    dev.off()
  
  #### name:tat####
  #library(devtools)
  #install_github("ivanhanigan/disentangle")
  setwd("tests")
  require(knitr)
  require(markdown)
  opts_chunk$set(fig.align=”left”)
  knit2html("gantt_tufte_test.Rmd", options = c("toc", markdown::markdownHTMLOptions(TRUE)), stylesheet = "custom.css")
  setwd("..")
#+end_src

#+RESULTS:
: /home/ivan_hanigan/tools/disentangle/tests

: 
*** COMMENT test RMD
<section>
    <img style="float: left" src="AAPL.svg">
  </section>

#+name:make_html
#+begin_src R :session *R* :tangle tests/gantt_tufte_test.Rmd :exports none :eval yes
  Overview of Gantt Chart
  ===
  
  ivan.hanigan@anu.edu.au
  
  ```{r echo = F, eval=F, results="hide"}
  setwd("tests")
  require(knitr)
  require(markdown)
  opts_chunk$set(fig.align=”left”)
  knit2html("gantt_tufte_test.Rmd", options = c("toc", markdown::markdownHTMLOptions(TRUE)), stylesheet = "custom.css")
  ```
  
  ```{r}
  print(Sys.Date())
  ```
  
  Introduction
  ---
  
  This is a report of the TODO list broken down by LTERN Data Team member.
  
  
  ![aa](AAPL.svg)  
  
  ```{r}
  print(cat("\n"))
  ```
  
    
  ```{r echo = F, results = "hide", eval = T}
  #### name:test2####
  source("../R/timebox.R")
  source("../R/get_gantt_data.R")
  source("../R/gantt_tufte.r")
  dat <- get_gantt_data(test_data = F)
  # str(dat)
  
  
  datin  <- dbGetQuery(ch,
  "
  select t1.container_task_title, 
  t2.*
  from container_task t1
  join work_package t2
  on t1.id = t2.container_id
  where t2.status != 'DONTSHOW'
  order by container_task_title"
  )
  #str(datin)
  datin_done  <- dbGetQuery(ch,
  "
  select t1.container_task_title, 
  t2.*
  from container_task_done t1
  join work_package_done t2
  on t1.id = t2.container_id
  where t2.status != 'DONTSHOW'
  "
  )
  #str(datin_done)
  datin  <- rbind(datin, datin_done)
   str(datin)
  str(dat)
  as.data.frame(table(datin$name))
  
  
  ```
  
  ```{r echo = F, results = "asis", eval = T}
  library("xtable")
  
  for(i in names(table(datin$name))){
   #i <-names(table(datin$name))[1]
  
   cat(i)
   cat("\n")
   cat("===\nTODO\n---\n")
   # str(dat)
   xdat <- sqldf(
   sprintf("select name, t1.container_task_narrow as task_group , t1.task_id,
   t2.start_date, t2.end_date, t2.efforti as effort_days, t1.notes, t2.status,
   case when t2.depends_on = t1.task_id then '' else t2.depends_on end as depends_on
   from datin t1
   left join
   dat t2
   on t1.task_id = t2.task_id
   where t1.name = '%s'
   order by t2.start_date
   ", i),
   drv = "SQLite")
    xdat$start_date <- as.character(format(xdat$start_date, "%A, %d-%b-%Y"))
    xdat$end_date <- as.character(format(xdat$end_date, "%A, %d-%b-%Y"))
   xdat1 <- subset(xdat, status == 'TODO')
   print(xtable(xdat1), type = "html", include.rownames = F)
  
   xdat2 <- subset(xdat, status == 'DONE')
   xdat2 <- xdat2[which(as.Date(xdat2$end_date, format = "%A, %d-%b-%Y") > Sys.Date() - 7),]
   # xdat2
   if(nrow(xdat2) > 0){
     cat("DONE\n---\n")  
     cat("\n")
     print(xtable(xdat2), type = "html", include.rownames = F)
     }
   }
  
  ```
  
    
#+end_src

#+RESULTS: make_html
*** COMMENT test2-code

| container_task_title | task_id                      | allocated | fte | blocker               |       start_date | effort | status | notes |
| no 1                 | Start                        | ivan      |   1 | NA                    |       2015-03-15 |     1d | DONE   | NA    |
| no 2                 | Repeat MEDLINE search        | ivan      |   1 | Start                 |       2015-03-16 |     5d | TODO   | NA    |
| no 3                 | Retrieve articles            | ivan      |   1 | Repeat MEDLINE search |               NA |     5d | TODO   | NA    |
| no 4                 | Read articles                | ivan      |   1 |                       |       2015-03-26 |    10d | TODO   |       |
| no 5                 | Summarize articles           | ivan      |   1 |                       |       2015-04-06 |     5d | TODO   |       |
| no 6                 | Write introduction           | ivan      |   1 |                       |       2015-04-11 |     5d | TODO   |       |
| no 7                 | Write methods                | ivan      |   1 | Start                 |                  |    10d | TODO   |       |
| no 8                 | Write results                | ivan      |   1 | Start                 |                  |    10d | TODO   |       |
| no 9                 | Write discussion             | ivan      |   1 | Write results         |                  |    10d | TODO   |       |
| no 10                | Send to co-author for review | ivan      |   1 | Write discussion      |                  |     2d | TODO   |       |
| no 11                | Revise draft 1               | ivan      |   1 |                       |       2015-04-30 |    10d | TODO   |       |
| no 12                | Submit article 1             | ivan      |   1 |                       |   Revise draft 1 |     5d | TODO   |       |
| no 13                | Revise draft 2               | ivan      |   1 |                       |       2015-05-30 |    10d | TODO   |       |
| no 14                | Submit article 2             | ivan      |   1 |                       |   Revise Draft 2 |     5d | TODO   |       |
| no 15                | Accepted                     | ivan      |   1 |                       | Submit article 2 |     1d | TODO   |       |

*** code name:test2
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test2####
  library(disentangle)
  library(sqldf)
  library(lubridate)
  
  datin  <- read.csv(
  textConnection("
  container_task_title  , task_id                      , allocated , fte , blocker               ,       start_date , effort , status , notes 
  01 Start              , Start                        , ivan      ,   1 , NA                    ,       2015-03-15 ,     1d , DONE   , NA    
  02 Update Lit Review  , Repeat MEDLINE search        , ivan      ,   1 , Start                 ,       2015-03-16 ,     5d , DONE   , NA    
  02 Update Lit Review  , Retrieve articles            , ivan      ,   1 , Repeat MEDLINE search ,               NA ,     5d , DONE   , NA    
  02 Update Lit Review  , Read articles                , ivan      ,   1 ,                       ,       2015-03-26 ,    11d , DONE   ,       
  02 Update Lit Review  , Summarize articles           , ivan      ,   1 ,                       ,       2015-04-06 ,     9d , TODO   ,       
  03 Write Draft        , Write introduction           , ivan      ,   1 ,                       ,       2015-04-09 ,     6d , TODO   ,       
  03 Write Draft        , Write methods                , ivan      ,   1 , Start                 ,                  ,    15d , TODO   ,       
  03 Write Draft        , Write results                , ivan      ,   1 ,                       ,       2015-03-30 ,    10d , TODO   ,       
  03 Write Draft        , Write discussion             , ivan      ,   1 ,                       ,       2015-04-15 ,    10d , TODO   ,       
  04 Internal Review    , Send to co-author for review , ivan      ,   1 , Write discussion      ,                  ,     2d , TODO   ,        
  04 Internal Review    , Revise draft 1               , ivan      ,   1 ,                       ,       2015-04-19 ,    10d , TODO   ,       
  05 Peer Review        , Submit article 1             , ivan      ,   1 , Revise draft 1        ,                  ,     5d , TODO   ,       
  06 Revise and Resubmit, Revise draft 2               , ivan      ,   1 ,                       ,       2015-04-30 ,    10d , TODO   ,       
  06 Revise and Resubmit, Submit article 2             , ivan      ,   1 , Revise draft 2        ,                  ,     5d , TODO   ,       
  07 End                , Accepted                     , ivan      ,   1 ,                       ,       2015-05-15 ,     1d , TODO   ,       
  "),
  stringsAsFactor = F, strip.white = T)
  # or 
  # datin <- get_gantt_data("gantt_todo", test_data = T) # need to
  # adjust min_context_xrange to 2015-01-01 or something
  datin$start_date  <- as.Date(datin$start_date)
  str(datin)
  datin
  
  dat_out <- gantt_data_prep(dat_in = datin)
  str(dat_out)
  dat_out
  svg("tests/gantt_tufte_test.svg",height=10,width=8)
  gantt_tufte(dat_out, focal_date = "2015-04-13", time_box = 3*7,
              min_context_xrange = "2015-03-16",
              cex_context_ylab = 0.65, cex_context_xlab = .7,
              cex_detail_ylab = 0.9,  cex_detail_xlab = .4,
              show_today = F)
  dev.off()
  
#+end_src

#+RESULTS: test2
: 1

** 2015-09-13-task-management-like-an-open-science-hacker
#+name:task-management-like-an-open-science-hacker-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-09-13-task-management-like-an-open-science-hacker.md :exports none :eval no :padline no
---
name: task-management-like-an-open-science-hacker
layout: post
title: task-management-like-an-open-science-hacker
date: 2015-09-13
categories:
- disentangle
tags:
- project management
---

I just read this impressive paper and it has really given me a push toward making this open lab notebook 

#### Citation
    Nosek, B. A., et al. (2015). Promoting an open research
    culture. Science, 348(6242), 1422–1425. doi:10.1126/science.aab2374

<p></p>


#### Quote
    The situation is a classic collective action problem. Many individual researchers lack
    strong incentives to be more transparent, even though the credibility of science would 
    benefit if everyone were more transparent.
<p></p>

So I think I'll try to step up the pace of logging my daily scientific work.
One super easy thing to do is to publish my daily log from my task management in orgmode.
Indeed I am also reading at the moment this guy who says 

#### Quote
    The core of your documentation is the research log.
   
    Long, S. (2015). Reproducible Results and the Workflow of Data Analysis. 
    Retrieved from http://www.indiana.edu/~jslsoc/ftp/WIM/wf wim 2015 2015-08-21@3.pdf
<p></p>

Finally, I was struck by this reference [http://rich-iannone.github.io/about/2014/10/28/introduction.html](http://rich-iannone.github.io/about/2014/10/28/introduction.html) to something about 365+ day GitHub streaks. It was covered earlier by Geoff Greer, and by Dirk Eddelbuettel.

It seems the basic concept is that you can leverage off an obsessive tendency by making sure you do something toward ticking off items from the task list every day.  The impulse to not breaking the chain is supposed to give you inspiration to keep going.  I think this might work well for my temperatment.

## Emacs and orgmode

The set up of my daily log is pretty simple. After being set up by kjhealy's starter kit.
Then I modified the org-agenda-files which was set in the main el file that kjhealy provided  and then with the command C-c a a emacs will display my calendar.

When I open emacs in the morning I  open the agenda and this also opens research-log file.  I move to that buffer, then I use this key command to insert a new entry for todays date

#### CODE
     (define-skeleton org-journalentry
       "Template for a journal entry."
       "project:"
       "*** " (format-time-string "%Y-%m-%d %a") " \n"
       "**** TODO-list \n"
       "***** TODO \n"
       "**** timesheet\n"
       "#+begin_src txt :tangle work-log.csv :eval no :padline no\n"
       (format-time-string "%Y-%m-%d %a") ", " str ", 50\n" 
       "#+end_src\n"
     )
     (global-set-key [C-S-f5] 'org-journalentry)

<p></p>

This creates a new date, a stub of a TODO for anything ad hoc and a entry into my timesheet.csv file.

I then select from TODO items from a global list that I keep at the top of the file, and cut/paste them into todays list.


![img](/images/agenda.png)

Great so I just moved this research-log orgmode file into my blog github repo, and with the help of charlie park's bash script I am good to go

#### CODE

    alias build_blog="cd ~/projects/ivanhanigan.github.com.raw; jekyll b;
    cp -r ~/projects/ivanhanigan.github.com.raw/_site/* ~/projects/ivanhanigan.github.com;
    cd ~/projects/ivanhanigan.github.com;git add .;git commit -am 'Latest build.';git push"
    alias bb="build_blog"

<p></p>

So this will put the resulting changes onto my open lab book website here [https://raw.githubusercontent.com/ivanhanigan/ivanhanigan.github.com/master/work-log.org](https://raw.githubusercontent.com/ivanhanigan/ivanhanigan.github.com/master/work-log.org)


Things to note:

- I found this list of tips [http://natashatherobot.com/streak-github-mistakes/](http://natashatherobot.com/streak-github-mistakes/)
- In particular I realise I need to make my daily push by 4:50 PM in Canberra ACT as this is 11:50 PM the previous day for Github, Pacific Time (PT)
- I also will need to ensure I don't publish sensitive (or embarrasing entries).  
- I'll try to keep the identity of my collaborators private as well, so just use their initials rather than names.

#+end_src

** R-AdminTemplate
*** R-R-AdminTemplate
#+name:R-AdminTemplate
#+begin_src R :session *R* :tangle R/AdminTemplate.r :exports none :eval no
  ################################################################
  # name:R-AdminTemplate
  AdminTemplate <- function(rootdir = getwd()){
    if(!exists(rootdir)) dir.create(rootdir)
    dir.create(file.path(rootdir,'01_planning'))
    dir.create(file.path(rootdir,'01_planning','proposal'))
    dir.create(file.path(rootdir,'01_planning','scheduling'))
    dir.create(file.path(rootdir,'02_budget'))
    dir.create(file.path(rootdir,'03_communication'))
    dir.create(file.path(rootdir,'04_reporting_and_meetings'))
    file.create(file.path(rootdir,'contact_details.txt'))
    file.create(file.path(rootdir,'README.md'))
    }
  
#+end_src
*** test-R-AdminTemplate
#+name:R-AdminTemplate
#+begin_src R :session *R* :tangle tests/test-R-AdminTemplate.r :exports none :eval no
  ################################################################
  # name:R-AdminTemplate
  setwd("~/Dropbox/Z_New_T_Drive/Resource Management/IT Infrastructure/PhenocamsAndImageryServer")
  AdminTemplate()
#+end_src
*** man-R-AdminTemplate
#+name:R-AdminTemplate
#+begin_src R :session *R* :tangle no :exports none :eval no
################################################################
# name:R-AdminTemplate

#+end_src

*** blog
#+name:project-templates-that-initialize-a-new-project-with-a-skeleton-automatically-header
#+begin_src R :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2014-03-29-project-templates-that-initialize-a-new-project-with-a-skeleton-automatically.md :exports none :eval no :padline no
  ---
  name: project-templates-that-initialize-a-new-project-with-a-skeleton-automatically
  layout: post
  title: Project Templates That Initialize A New Project With A Skeleton Automatically
  date: 2014-03-29
  categories:
  - research methods
  ---
  
  - I have been using [John Myles Whites ProjectTemplate R package](http://projecttemplate.net/) for ages
  - I really like the ease with which I can get up and running a new project
  - and the ease with which I can pick up an old project and start adding new work
    
  #### Quote from John's first post
      My inspiration for this approach comes from the rails command from
      Ruby on Rails, which initializes a new Rails project with the proper
      skeletal structure automatically. Also taken from Rails is
      ProjectTemplate’s approach of preferring convention over
      configuration: the automatic data and library loading as well as the
      automatic testing work out of the box because assumptions are made
      about the directory structure and naming conventions that will be used
  
  <p></p>
    
  [http://www.johnmyleswhite.com/notebook/2010/08/26/projecttemplate/](http://www.johnmyleswhite.com/notebook/2010/08/26/projecttemplate/)
  
  - I dont know anything about RoR but this philosophy works really well for my R programming too
  
  #### R Code
      if(!require(ProjectTemplate)) install.packages(ProjectTemplate); require(ProjectTemplate)
      setwd("~/projects")
      create.project("my-project")
      setwd('my-project')
      dir()
      ##  [1] "cache"       "config"      "data"        "diagnostics" "doc"        
      ##  [6] "graphs"      "lib"         "logs"        "munge"       "profiling"  
      ## [11] "README"      "reports"     "src"         "tests"       "TODO"   
      ##### these are very sensible default directories to create a modular
      ##### analysis workflow.  See the project homepage for descriptions
       
      # now all you need to do whenever you start a new day 
      load.project()
      # and your workspace will be recreated and any new data automagically analysed in
      # the manner you want
  
  <p></p>
  
  #### Project Administration
  
  - I;ve found that these directories do not work so well for the administration of my projects and so I put together a different set of automatic defaults
  - Ive based it on the [University of Manitoba Centre for Health Policy](http://ivanhanigan.github.io/2013/12/research-protocol-for-manitoba-centre-for-health-policy/)- along with some other sources I can recall
  #### The full set
      # A.Background        
      # B.Proposals 
      # C.Approvals 
      # D.Budget    
      # E.Datasets  
      # F.Analysis  
      # G.Literature        
      # H.Communication             
      # I.Correspondance    
      # J.Meetings  
      # K.Completion        
      # ContactDetails.txt  
      # README.md
      # TODO.txt    
  
  <p></p>
    
  #### R Code: my subset
      AdminTemplate <- function(rootdir = getwd()){
        setwd(rootdir)
        dir.create(file.path(rootdir,'01_planning'))
        dir.create(file.path(rootdir,'01_planning','proposal'))
        dir.create(file.path(rootdir,'01_planning','scheduling'))
        dir.create(file.path(rootdir,'02_budget'))
        dir.create(file.path(rootdir,'03_communication'))
        dir.create(file.path(rootdir,'04_reporting_and_meetings'))
        file.create(file.path(rootdir,'contact_details.txt'))
        file.create(file.path(rootdir,'README.md'))
        }
  
  <p></p>
  
  
  #### Conclusion
  
  - hopefully by formalising some of these into my workflow I will find my projects easier to navigate through
  - and pick up or put down as needed
#+end_src

* Data Management 

** 2015-09-25-reproducible-research-and-managing-digital-assets part1
#+name:reproducible-research-and-managing-digital-assets-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-09-25-reproducible-research-and-managing-digital-assets.md :exports none :eval no :padline no
---
name: reproducible-research-and-managing-digital-assets
layout: post
title: Reproducible Research And Managing Digital Assets (1 of 3). Scott Long wrote a book and some great tutorial slides
date: 2015-09-25
categories:
- disentangle
tags:
- Data Management System
---

This will be a series of three posts that describe some key evidence based best practice methods that have helped me plan and organise files and folders for data analysis.  I have found these via books and on websites.

- Scott Long's Workflow for Data Analysis with Stata
- Josh Reich's Least Commonly Fouled up Data analysis (LCFD) framework
- John Myles White's ProjectTemplate

## Toward evidence based best-practice data management systems

It is important for open science to have effective management of digital assets across the different phases of the research pipeline.  The traditional research pipeline moves from steps of hypothesis and design, measured data, analytic data, computational results (for figures, tables and numerical results), and reports (text and formatted manuscript).  Reproducible research pipelines extend traditional research by encoding the steps in code from a computer ‘scripting’ language, and distributing the data and code with publications.  

In this research pipeline context there are a large number of potential ways to manage digital assets (documents, data and code).  There are also many different motivating drivers that will affect the way that a scientist or group of scientists choose to manage their data and code.  

To deal with in house data management issues before starting and during analysis/reporting is critical for reproducible research.  
I argue that more effective research pipelines can be achieved if scientists adopt the 'convention over configuration' paradigm and adopt best-practice systems based on evidence.



### Long, S. (2015). Workflow for Reproducible Results. 

For ages I was aware of the book from the Stata statistical program publishers [http://www.indiana.edu/~jslsoc/web_workflow/wf_home.htm](http://www.indiana.edu/~jslsoc/web_workflow/wf_home.htm):

#### Citation:
    Long, J. S. (2008). The Workflow of Data Analysis: 
    Principles and Practice. Stata publishing.

<p></p>

Recently I stumbled across more recent workshop slides and tutorial material which I will discuss briefly.

#### Citation:
    Long, S. (2015). Workflow for Reproducible Results. 
    IV : Managing digital assets Workflow for Tools for your WF. 
    
<p></p>

Retrieved from [http://txrdc.tamu.edu/documents/WFtxcrdc2014_4-digital.pdf](http://txrdc.tamu.edu/documents/WFtxcrdc2014_4-digital.pdf)

Long suggests a lot of practical things to do, but I will just focus here on the recommended file and folder structure:

#### Recommended project directory structure:
    \ProjectAcronym
        \- History starting YYYY-MM-DD
        \- Hold then delete 
        \Admin
        \Documentation 
        \Posted
             \Paper 1
                 \Correspondence 
                 \Text
                 \Analysis
        \PrePosted 
        \Resources 
        \Write 
        \Work
<p></p>    

- In another workshop report Long provides a useful tool to automatically create this structure on windows 
- Long, S. (2012). Principles of Workflow in Data Analysis. 
  Retrieved from [http://www.indiana.edu/~wim/docs/2012-long-slides.pdf](http://www.indiana.edu/~wim/docs/2012-long-slides.pdf)
- a bash version would be useful for linux and mac users, but also the R language can do this on all platforms with the `dir.create` command


#### Code: wfsetupsingle.bat
    # wfsetupsingle.bat 
    REM workflow talk 2 \ wfsetupsingle.bat jsl 2009-07-12 
    REM directory structure for single person.
    FOR /F "tokens=2,3,4 delims=/- " %%a in ("%DATE%") do set CDATE=%%c-%%a-%%b 
    md "- History starting \%cdate%" 
    md "- Hold then delete " 
    md "- Pre posted " 
    md "- To clean" 
    md "Documentation" 
    md "Posted" 
    md "Resources"
    md "Text\- Versions\" 
    md "Work\- To do"

## Critical reflections

- This recommendation is very sensible, especially the suggestion of moving things through the pipeline as they evolve from things being worked on (Write/Work) to later phases when they have been polished to a point that they can be put down while preparations for distrubuting them are made (Preposted) and then once they are sent off into downstream publication phases (Posted) they are locked for ever in a archival state.
- I am not particularly keen on the names that have been chosen (Resources, Write and Work are quite ambiguous terms).


#+end_src
** 2015-09-26-reproducible-research-and-managing-digital-assets-part-2
#+name:reproducible-research-and-managing-digital-assets-part-2-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-09-26-reproducible-research-and-managing-digital-assets-part-2.md :exports none :eval no :padline no
  ---
  name: reproducible-research-and-managing-digital-assets-part-2
  layout: post
  title: Reproducible Research And Managing Digital Assets Part 2 of 3. makeProject is simple
  date: 2015-09-26
  categories:
  - Data Management
  tags:
  - metadata
  ---
  
  This post is about an effective and simple data management framework for analysis projects.
  This post introduces Josh Reich's LCFD framework, originally introduced in this answer on the stack overflow website here [http://stackoverflow.com/a/1434424](http://stackoverflow.com/a/1434424), and encoded into the makeProject R package [http://cran.r-project.org/web/packages/makeProject/makeProject.pdf](http://cran.r-project.org/web/packages/makeProject/makeProject.pdf).
  
  ## Literature Review Approach

  This series of three posts is a summary of some of the most useful advice I have found based on my experience having implemented in my own work.
  
  This is the second post in a series of three entries regarding some evidence-based best practice approaches I have reviewed.  I have read many website articles and blog posts on a variety of approaches to the organisation of digital assets in a reporoducible research pipeline.
  The material I've gathered in my ongoing search and opportunistic readings regarding best practice in this area have been recommended by practitioners which provides some weight of evidence.  In addition I have implemented some aspects of the many techniques and the reproducibility of my own work has improved greatly.  

  ## Digital Assets Management for Reproducible Research
  
  The digital assets in a reproducible research pipeline include:
  
  1. Publication material (documents, figures, tables, literature)
  1. Data (raw measurements, data provided, data derived)
  1. Code (pre-processing, analysis and presentation)
  
  ## How to use the `makeProject` package
  
  - The `makeProject` R package is designed to create a folder and some R scripts that are useful for generic workflow tasks.
   - The theory is very similar to the approach described in the previous post about Scott Long's batch script: wfsetupsingle.bat [https://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets](https://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets)
    
  #### Code:
      # choose your project dir
      setwd("~/projects")   
      library(makeProject)
      makeProject("makeProjectDemo")
      #returns
      "Creating Directories ...
      Creating Code Files ...
      Complete ..."
      matrix(dir("makeProjectDemo"))
      #[1,] "code"       
      #[2,] "data"       
      #[3,] "DESCRIPTION"
      #[4,] "main.R"     
  
  <p></p>
  
  - This has set up some simple and sensible tools for a data analysis.
  - Let's have a look at the `main.R` script. This is the one file that is used to run all the modules of the project, found in the R scripts in the `code` folder.
  
  
  #### Code:
      # Project: makeProjectDemo
      # Author: Your Name
      # Maintainer: Who to complain to <yourfault@somewhere.net>
       
      # This is the main file for the project
      # It should do very little except call the other files
       
      ### Set the working directory
      setwd("/home/ivan_hanigan/projects/makeProjectDemo")
       
       
      ### Set any global variables here
      ####################
       
       
       
      ####################
       
       
      ### Run the code
      source("code/load.R")
      source("code/clean.R")
      source("code/func.R")
      source("code/do.R")
  
  
  <p></p>


  I think that is very self-explanatory, but it does need some demonstration.  The next instalment in this three part blog post will describe the ProjectTemplate approach.  After that I will demonstrate ways that each of the three approaches can be used.
  
#+end_src

** 2015-09-27-reproducible-research-and-managing-digital-assets-part-2

~/projects/ProjectTemplateDemo/ProjectTemplateDemo.org

#+name:reproducible-research-and-managing-digital-assets-part-2-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-09-27-reproducible-research-and-managing-digital-assets-part-3.md :exports none :eval no :padline no
  ---
  name: reproducible-research-and-managing-digital-assets-part-3
  layout: post
  title: Reproducible Research And Managing Digital Assets Part 3 of 3. ProjectTemplate is appropriate for large scale
  date: 2015-09-27
  categories:
  - Data Management
  tags:
  - R
  ---
  
  ## Recap on this series of three posts
  - The [first post](http://ivanhanigan.github.io/2015/09/reproducible-research-and-managing-digital-assets/) showed the recommended files and folders for a data analysis project from Scott Long
  - That recommendation was pretty complex, with a few folders that I felt did not jump out as super-useful
  - The [second post](http://ivanhanigan.github.io/2015/09/reproducible-research-and-managing-digital-assets-part-2/) showed a very simple template from the R community called makeProject
  - I really like that one as it seems to be the minimum amount of stuff needed to make things work.
  
  ## The ProjectTemplate framework
  
  - I have been using John Myles Whites ProjectTemplate R package [http://projecttemplate.net/](http://projecttemplate.net/) for ages
  - I really like the ease with which I can get up and running a new project
  - and the ease with which I can pick up an old project and start adding new work
    
  #### Quote from John's first post
      My inspiration for this approach comes from the rails command from
      Ruby on Rails, which initializes a new Rails project with the proper
      skeletal structure automatically. Also taken from Rails is
      ProjectTemplate’s approach of preferring convention over
      configuration: the automatic data and library loading as well as the
      automatic testing work out of the box because assumptions are made
      about the directory structure and naming conventions that will be used
  
  <p></p>
    
  [http://www.johnmyleswhite.com/notebook/2010/08/26/projecttemplate/](http://www.johnmyleswhite.com/notebook/2010/08/26/projecttemplate/)
  
  - I dont know anything about RoR but this philosophy works really well for my R programming too
  
  #### R Code
      if(!require(ProjectTemplate)) install.packages(ProjectTemplate); require(ProjectTemplate)
      setwd("~/projects")
      create.project("my-project")
      setwd('my-project')
      dir()
      ##  [1] "cache"       "config"      "data"        "diagnostics" "doc"        
      ##  [6] "graphs"      "lib"         "logs"        "munge"       "profiling"  
      ## [11] "README"      "reports"     "src"         "tests"       "TODO"   
      ##### these are very sensible default directories to create a modular
      ##### analysis workflow.  See the project homepage for descriptions
       
      # now all you need to do whenever you start a new day 
      load.project()
      # and your workspace will be recreated and any new data automagically analysed in
      # the manner you want
  
  <p></p>
  
  ## Advanced usage of ProjectTemplate
  
      
  <body>
  
  <div id="preamble">
  
  </div>
  
  <div id="content">
  <h1 class="title">ProjectTemplate Demo</h1>
  
  
  
  
  <div id="table-of-contents">
  <h2>Table of Contents</h2>
  <div id="text-table-of-contents">
  <ul>
  <li><a href="#sec-1">1 The Compendium concept</a></li>
  <li><a href="#sec-2">2 The R code that produced this report</a></li>
  <li><a href="#sec-3">3 Inititalise R environment</a></li>
  <li><a href="#sec-4">4 ProjectTemplate</a></li>
  <li><a href="#sec-5">5 Why?</a></li>
  <li><a href="#sec-6">6 The Reichian load, clean, func, do approach</a></li>
  <li><a href="#sec-7">7 The Peng NMMAPSlite approach</a></li>
  <li><a href="#sec-8">8 Init the project</a></li>
  <li><a href="#sec-9">9 dir()</a></li>
  <li><a href="#sec-10">10 The reports directory</a></li>
  <li><a href="#sec-11">11 Do the analysis</a></li>
  <li><a href="#sec-12">12 Get the projecttemplate tutorial data</a></li>
  <li><a href="#sec-13">13 Tools</a></li>
  <li><a href="#sec-14">14 Load the analysis data</a></li>
  <li><a href="#sec-15">15 check the analysis data</a></li>
  <li><a href="#sec-16">16 Develop munge code</a></li>
  <li><a href="#sec-17">17 To munge or not to munge?</a></li>
  <li><a href="#sec-18">18 Cache</a></li>
  <li><a href="#sec-19">19 Plot first and second letter counts</a></li>
  <li><a href="#sec-20">20 Do generate plots</a></li>
  <li><a href="#sec-21">21 First letter</a></li>
  <li><a href="#sec-22">22 Second letter</a></li>
  <li><a href="#sec-23">23 Report results</a></li>
  <li><a href="#sec-24">24 Produce final report</a></li>
  <li><a href="#sec-25">25 Personalised project management directories</a></li>
  </ul>
  </div>
  </div>
  
  <div id="outline-container-1" class="outline-2">
  <h2 id="sec-1"><span class="section-number-2">1</span> The Compendium concept</h2>
  <div class="outline-text-2" id="text-1">
  
  <p>\section{The Compendium concept}
  My goal is to develop data analysis projects along the lines of the Compendium concept of Gentleman and Temple Lang (2007) \cite{Gentleman2007}.
  Compendia are dynamic documents containing text, code and data.
  Transformations are applied to the compendium to view its various aspects.
  </p>
  <ul>
  <li>Code Extraction (Tangle): source code
  </li>
  <li>Export (Weave): LaTeX, HTML, etc
  </li>
  <li>Code Evaluation
  </li>
  </ul>
  
  
  <p>
  I'm also following the orgmode technique of Schulte et al (2012) \cite{Schulte}
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-2" class="outline-2">
  <h2 id="sec-2"><span class="section-number-2">2</span> The R code that produced this report</h2>
  <div class="outline-text-2" id="text-2">
  
  
  <p>
  I support the philosophy of Reproducible Research <a href="http://www.sciencemag.org/content/334/6060/1226.full">http://www.sciencemag.org/content/334/6060/1226.full</a>, and where possible I provide data and code in the statistical software R that will allow analyses to be reproduced.  This document is prepared automatically from the associated Emacs Orgmode file.  If you do not have access to the Orgmode file please contact me.
  </p>
  
  
  <pre class="src src-R">cat(<span style="color: #2aa198;">'</span>
  <span style="color: #2aa198;"> #######################################################################</span>
  <span style="color: #2aa198;"> ## The R code is free software; please cite this paper as the source.  </span>
  <span style="color: #2aa198;"> ## Copyright 2012, Ivan C Hanigan <a href="mailto:ivan.hanigan%40gmail.com">&lt;ivan.hanigan@gmail.com&gt;</a> </span>
  <span style="color: #2aa198;"> ## This program is free software; you can redistribute it and/or modify</span>
  <span style="color: #2aa198;"> ## it under the terms of the GNU General Public License as published by</span>
  <span style="color: #2aa198;"> ## the Free Software Foundation; either version 2 of the License, or</span>
  <span style="color: #2aa198;"> ## (at your option) any later version.</span>
  <span style="color: #2aa198;"> ## </span>
  <span style="color: #2aa198;"> ## This program is distributed in the hope that it will be useful,</span>
  <span style="color: #2aa198;"> ## but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
  <span style="color: #2aa198;"> ## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
  <span style="color: #2aa198;"> ## GNU General Public License for more details.</span>
  <span style="color: #2aa198;"> ## Free Software</span>
  <span style="color: #2aa198;"> ## Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA</span>
  <span style="color: #2aa198;"> ## 02110-1301, USA</span>
  <span style="color: #2aa198;"> #######################################################################</span>
  <span style="color: #2aa198;">'</span>)
  </pre>
  
  </div>
  
  </div>
  
  <div id="outline-container-3" class="outline-2">
  <h2 id="sec-3"><span class="section-number-2">3</span> Inititalise R environment</h2>
  <div class="outline-text-2" id="text-3">
  
  
  
  
  <pre class="src src-R"><span style="color: #586e75;">####</span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">MAKE SURE YOU HAVE THE CORE LIBS</span>
  <span style="color: #859900; font-weight: bold;">if</span> (!<span style="color: #268bd2; font-weight: bold;">require</span>(ProjectTemplate)) install.packages(<span style="color: #2aa198;">'ProjectTemplate'</span>, repos=<span style="color: #2aa198;">'http://cran.csiro.au'</span>); <span style="color: #268bd2; font-weight: bold;">require</span>(ProjectTemplate)
  <span style="color: #859900; font-weight: bold;">if</span> (!<span style="color: #268bd2; font-weight: bold;">require</span>(lubridate)) install.packages(<span style="color: #2aa198;">'lubridate'</span>, repos=<span style="color: #2aa198;">'http://cran.csiro.au'</span>); <span style="color: #268bd2; font-weight: bold;">require</span>(lubridate)
  <span style="color: #859900; font-weight: bold;">if</span> (!<span style="color: #268bd2; font-weight: bold;">require</span>(reshape)) install.packages(<span style="color: #2aa198;">'reshape'</span>, repos=<span style="color: #2aa198;">'http://cran.csiro.au'</span>); <span style="color: #268bd2; font-weight: bold;">require</span>(reshape)
  <span style="color: #859900; font-weight: bold;">if</span> (!<span style="color: #268bd2; font-weight: bold;">require</span>(plyr)) install.packages(<span style="color: #2aa198;">'plyr'</span>, repos=<span style="color: #2aa198;">'http://cran.csiro.au'</span>); <span style="color: #268bd2; font-weight: bold;">require</span>(plyr)
  <span style="color: #859900; font-weight: bold;">if</span> (!<span style="color: #268bd2; font-weight: bold;">require</span>(ggplot2)) install.packages(<span style="color: #2aa198;">'ggplot2'</span>, repos=<span style="color: #2aa198;">'http://cran.csiro.au'</span>); <span style="color: #268bd2; font-weight: bold;">require</span>(ggplot2)
  <span style="color: #859900; font-weight: bold;">if</span>(!<span style="color: #268bd2; font-weight: bold;">require</span>(mgcv)) install.packages(<span style="color: #2aa198;">'mgcv'</span>, repos=<span style="color: #2aa198;">'http://cran.csiro.au'</span>);<span style="color: #268bd2; font-weight: bold;">require</span>(mgcv);
  <span style="color: #268bd2; font-weight: bold;">require</span>(splines)
  <span style="color: #859900; font-weight: bold;">if</span>(!<span style="color: #268bd2; font-weight: bold;">require</span>(NMMAPSlite)) install.packages(<span style="color: #2aa198;">'NMMAPSlite'</span>, repos=<span style="color: #2aa198;">'http://cran.csiro.au'</span>);<span style="color: #268bd2; font-weight: bold;">require</span>(NMMAPSlite)
  rootdir <span style="color: #268bd2; font-weight: bold;">&lt;-</span> getwd()  
  </pre>
  
  </div>
  
  </div>
  
  <div id="outline-container-4" class="outline-2">
  <h2 id="sec-4"><span class="section-number-2">4</span> ProjectTemplate</h2>
  <div class="outline-text-2" id="text-4">
  
  <p>\section{ProjectTemplate}
  This is a simple demo of the R package \emph{ProjectTemplate} <a href="http://projecttemplate.net/">http://projecttemplate.net/</a> which is aimed at standardising the structure and general development of data analysis projects in R. 
  A primary aim is to allow analysts to quickly get a project loaded up and ready to:
  </p><ul>
  <li>reproduce or 
  </li>
  <li>create new data analyses.
  </li>
  </ul>
  
  
  
  </div>
  
  </div>
  
  <div id="outline-container-5" class="outline-2">
  <h2 id="sec-5"><span class="section-number-2">5</span> Why?</h2>
  <div class="outline-text-2" id="text-5">
  
  <p>It has been recognised on the R blogosphere that it 
  </p><ul>
  <li>is ``meant to handle very complex research projects'' (<a href="http://bryer.org/2012/maker-an-r-package-for-managing-document-building-and-versioning">http://bryer.org/2012/maker-an-r-package-for-managing-document-building-and-versioning</a>) and 
  </li>
  <li>is considered as being amongst the best approaches to the workflow for doing data analysis with R (<a href="http://blog.revolutionanalytics.com/2010/10/a-workflow-for-r.html">http://blog.revolutionanalytics.com/2010/10/a-workflow-for-r.html</a>)
  </li>
  </ul>
  
  
  </div>
  
  </div>
  
  <div id="outline-container-6" class="outline-2">
  <h2 id="sec-6"><span class="section-number-2">6</span> The Reichian load, clean, func, do approach</h2>
  <div class="outline-text-2" id="text-6">
  
  <p>\section{The Reichian load, clean, func, do approach}
  </p>
  <p>
  The already mentioned blog post <a href="http://blog.revolutionanalytics.com/2010/10/a-workflow-for-r.html">http://blog.revolutionanalytics.com/2010/10/a-workflow-for-r.html</a> also links to another`best' approach, the: 
  </p>
  <ul>
  <li>\emph{Reichian load, clean, func, do} approach <a href="http://stackoverflow.com/a/1434424">http://stackoverflow.com/a/1434424</a>.  
  </li>
  </ul>
  
  
  
  <p>
  By Josh Reich.  I've also followed to prepare this demo using the tutorial and data from the package website <a href="http://projecttemplate.net/getting_started.html">http://projecttemplate.net/getting_started.html</a>
  </p>
  
  </div>
  
  </div>
  
  <div id="outline-container-7" class="outline-2">
  <h2 id="sec-7"><span class="section-number-2">7</span> The Peng NMMAPSlite approach</h2>
  <div class="outline-text-2" id="text-7">
  
  <p>\section{The Peng NMMAPSlite approach}
  The other approach I followed was that of Roger Peng from Johns Hopkins and his NMMAPSlite R package \cite{Peng2004}.  Especially the function
  </p>
  
  
  <pre class="src src-R">readCity(name, collapseAge = <span style="color: #b58900;">FALSE</span>, asDataFrame = <span style="color: #b58900;">TRUE</span>)
  </pre>
  
  
  <p>
  Arguments
  </p><ul>
  <li>name  character, abbreviated name of a city
  </li>
  <li>collapseAge   logical, should age categories be collapsed?
  </li>
  <li>asDataFrame   logical, should a data frame be returned?)
  </li>
  </ul>
  
  
  <p>
  Description: Provides remote access to daily mortality, weather, and
          air pollution data from the National Morbidity, Mortality, and
          Air Pollution Study for 108 U.S. cities (1987&ndash;2000); data are
          obtained from the Internet-based Health and Air Pollution
          Surveillance System (iHAPSS)
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-8" class="outline-2">
  <h2 id="sec-8"><span class="section-number-2">8</span> Init the project</h2>
  <div class="outline-text-2" id="text-8">
  
  <p>\section{Init the project}
  First we want to initialise the project directory.
  </p>
  
  
  <pre class="src src-R"><span style="color: #586e75;">####</span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">init</span>
  <span style="color: #268bd2; font-weight: bold;">require</span>(<span style="color: #2aa198;">'ProjectTemplate'</span>)
  create.project(<span style="color: #2aa198;">'analysis'</span>,minimal=<span style="color: #b58900;">TRUE</span>)
  </pre>
  
  
  </div>
  
  </div>
  
  <div id="outline-container-9" class="outline-2">
  <h2 id="sec-9"><span class="section-number-2">9</span> dir()</h2>
  <div class="outline-text-2" id="text-9">
  
  
  
  
  <pre class="src src-R"><span style="color: #586e75;">####</span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">init dir</span>
  dir(<span style="color: #2aa198;">'analysis'</span>)
  </pre>
  
  
  <table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
  <colgroup><col class="left" />
  </colgroup>
  <tbody>
  <tr><td class="left">cache</td></tr>
  <tr><td class="left">config</td></tr>
  <tr><td class="left">data</td></tr>
  <tr><td class="left">munge</td></tr>
  <tr><td class="left">README</td></tr>
  <tr><td class="left">src</td></tr>
  </tbody>
  </table>
  
  
  </div>
  
  </div>
  
  <div id="outline-container-10" class="outline-2">
  <h2 id="sec-10"><span class="section-number-2">10</span> The reports directory</h2>
  <div class="outline-text-2" id="text-10">
  
  <p>  I've added the reports directory manually and asked the package author if this is generic enough to be in the defaults for 
  </p>
  
  
  <pre class="src src-R">minimal = <span style="color: #b58900;">TRUE</span> 
  </pre>
  
  
  <p>
  I believe it may be as the \emph{Getting Started} guidebook states:
  </p><blockquote>
  
  <p>`It's meant to contain the sort of written descriptions of the results of your analyses that you'd \textbf{publish in a scientific paper.}
  </p>
  <p>
  With that report written &hellip;, we've gone through \textbf{the simplest sort of analysis you might run with ProjectTemplate}. 
  </p>
  </blockquote>
  
  
  
  
  
  <pre class="src src-R"><span style="color: #586e75;">####</span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">init reports</span>
  dir.create(<span style="color: #2aa198;">'analysis/reports'</span>)
  </pre>
  
  </div>
  
  </div>
  
  <div id="outline-container-11" class="outline-2">
  <h2 id="sec-11"><span class="section-number-2">11</span> Do the analysis</h2>
  <div class="outline-text-2" id="text-11">
  
  <p>\section{Do the analysis: use load,clean,func,do}
  </p>
  
  
  <pre class="src src-R"><span style="color: #586e75;">####</span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">this is the start of the analysis, </span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">assumes the init.r file has been run</span>
  <span style="color: #859900; font-weight: bold;">if</span>(file.exists(<span style="color: #2aa198;">'analysis'</span>)) setwd(<span style="color: #2aa198;">'analysis'</span>)  
  Sys.Date()
  <span style="color: #586e75;"># </span><span style="color: #586e75;">keep a track of the dates the analysis is rerun</span>
  getwd()
  <span style="color: #586e75;"># </span><span style="color: #586e75;">may want to keep a reference of the directory </span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">the project is in so we can track the history </span>
  </pre>
  
  
  </div>
  
  </div>
  
  <div id="outline-container-12" class="outline-2">
  <h2 id="sec-12"><span class="section-number-2">12</span> Get the projecttemplate tutorial data</h2>
  <div class="outline-text-2" id="text-12">
  
  <p>Get the data from <a href="http://projecttemplate.net/letters.csv.bz2">http://projecttemplate.net/letters.csv.bz2</a> (I downloaded on 13-4-2012)
  Put it in the data directory for auto loading.
  </p>
  
  
  
  <pre class="src src-R"><span style="color: #586e75;">####</span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">analysis get tutorial data</span>
  download.file(<span style="color: #2aa198;">'http://projecttemplate.net/letters.csv.bz2'</span>, 
    destfile = <span style="color: #2aa198;">'data/letters.csv.bz2'</span>, mode = <span style="color: #2aa198;">'wb'</span>)
  
  </pre>
  
  </div>
  
  </div>
  
  <div id="outline-container-13" class="outline-2">
  <h2 id="sec-13"><span class="section-number-2">13</span> Tools</h2>
  <div class="outline-text-2" id="text-13">
  
  <p>Edit the \emph{config/global.dcf} file to make sure that the load_libraries setting is turned on
  </p>
  
  </div>
  
  </div>
  
  <div id="outline-container-14" class="outline-2">
  <h2 id="sec-14"><span class="section-number-2">14</span> Load the analysis data</h2>
  <div class="outline-text-2" id="text-14">
  
  <p>#\section{load}
  </p>
  
  
  <pre class="src src-R"><span style="color: #586e75;">####</span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">analysis load</span>
  <span style="color: #268bd2; font-weight: bold;">require</span>(ProjectTemplate)
  load.project()
  </pre>
  
  </div>
  
  </div>
  
  <div id="outline-container-15" class="outline-2">
  <h2 id="sec-15"><span class="section-number-2">15</span> check the analysis data</h2>
  <div class="outline-text-2" id="text-15">
  
  <p>#\section{clean}
  </p>
  
  
  <pre class="src src-R">tail(letters)
  </pre>
  
  
  <table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
  <colgroup><col class="left" /><col class="left" /><col class="left" />
  </colgroup>
  <tbody>
  <tr><td class="left">zyryan</td><td class="left">z</td><td class="left">y</td></tr>
  <tr><td class="left">zythem</td><td class="left">z</td><td class="left">y</td></tr>
  <tr><td class="left">zythia</td><td class="left">z</td><td class="left">y</td></tr>
  <tr><td class="left">zythum</td><td class="left">z</td><td class="left">y</td></tr>
  <tr><td class="left">zyzomys</td><td class="left">z</td><td class="left">y</td></tr>
  <tr><td class="left">zyzzogeton</td><td class="left">z</td><td class="left">y</td></tr>
  </tbody>
  </table>
  
  
  
  </div>
  
  </div>
  
  <div id="outline-container-16" class="outline-2">
  <h2 id="sec-16"><span class="section-number-2">16</span> Develop munge code</h2>
  <div class="outline-text-2" id="text-16">
  
  <p>#\section{load with processing (munge)}
  </p>
  <p>
  Edit the \emph{munge/01-A.R} script so that it contains the following two lines of code:
  </p>
  
  
  <pre class="src src-R"><span style="color: #586e75;"># </span><span style="color: #586e75;">For our current analysis, we're interested in the total </span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">number of occurrences of each letter in the first and </span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">second letter positions and not in the words themselves.</span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">compute aggregates</span>
  first.letter.counts <span style="color: #268bd2; font-weight: bold;">&lt;-</span> ddply(letters, c(<span style="color: #2aa198;">'FirstLetter'</span>), 
    nrow)
  second.letter.counts <span style="color: #268bd2; font-weight: bold;">&lt;-</span> ddply(letters, c(<span style="color: #2aa198;">'SecondLetter'</span>), 
    nrow)
  </pre>
  
  <p>
  Now if we run with 
  </p>
  
  
  
  <pre class="src src-R">load.project()
  </pre>
  
  
  <p>
  all munging will happen automatically.  However&hellip;
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-17" class="outline-2">
  <h2 id="sec-17"><span class="section-number-2">17</span> To munge or not to munge?</h2>
  <div class="outline-text-2" id="text-17">
  
  <p>As you'll see on the website, once the data munging is completed and outputs cached, load.project() will keep recomputing work over and over.  
  The author suggests we manually edit our configuration file.
  </p>
  
  
  <pre class="src src-R"> <span style="color: #586e75;"># </span><span style="color: #586e75;">edit the config file and turn munge on</span>
   <span style="color: #586e75;"># </span><span style="color: #586e75;">load.project()</span>
   <span style="color: #586e75;"># </span><span style="color: #586e75;">edit the config file and turn munge off</span>
   <span style="color: #586e75;"># </span><span style="color: #586e75;">or my preference</span>
   <span style="color: #268bd2; font-weight: bold;">source</span>(<span style="color: #2aa198;">'munge/01-A.r'</span>)
  <span style="color: #586e75;"># </span><span style="color: #586e75;">which can be included in our first analysis script</span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">but subsequent analysis scripts can just call load.project() </span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">without touching the config file</span>
  </pre>
  
  </div>
  
  </div>
  
  <div id="outline-container-18" class="outline-2">
  <h2 id="sec-18"><span class="section-number-2">18</span> Cache</h2>
  <div class="outline-text-2" id="text-18">
  
  <p>Once munging is complete we cache the results
  </p>
  
  
  <pre class="src src-R">cache(<span style="color: #2aa198;">'first.letter.counts'</span>)
  cache(<span style="color: #2aa198;">'second.letter.counts'</span>)
  
  <span style="color: #586e75;"># </span><span style="color: #586e75;">And need to keep an eye on the implications for our config file to avoid re-calculating these next time we </span>
  
  load.project()
  
  </pre>
  
  
  
  
  <p>
  #\section{do}
  </p></div>
  
  </div>
  
  <div id="outline-container-19" class="outline-2">
  <h2 id="sec-19"><span class="section-number-2">19</span> Plot first and second letter counts</h2>
  <div class="outline-text-2" id="text-19">
  
  <p>Produce some simple density plots to see the shape of the first and second letter counts. 
  </p><ul>
  <li>Create \emph{src/generate_plots.R}. Use the src directory to store any analyses that you run. 
  </li>
  <li>The convention is that every analysis script starts with load.project() and then goes on to do something original with the data.
  </li>
  </ul>
  
  
  </div>
  
  </div>
  
  <div id="outline-container-20" class="outline-2">
  <h2 id="sec-20"><span class="section-number-2">20</span> Do generate plots</h2>
  <div class="outline-text-2" id="text-20">
  
  <p>Write the first analysis script into a file in \textbf{src}
  </p>
  
  
  <pre class="src src-R"><span style="color: #268bd2; font-weight: bold;">require</span>(<span style="color: #2aa198;">'ProjectTemplate'</span>)
  load.project()
  plot1 <span style="color: #268bd2; font-weight: bold;">&lt;-</span> ggplot(first.letter.counts, aes(x = V1)) + 
    geom_density()
  ggsave(file.path(<span style="color: #2aa198;">'reports'</span>, <span style="color: #2aa198;">'plot1.pdf'</span>))
  
  plot2 <span style="color: #268bd2; font-weight: bold;">&lt;-</span> ggplot(second.letter.counts, aes(x = V1)) + 
    geom_density()
  ggsave(file.path(<span style="color: #2aa198;">'reports'</span>, <span style="color: #2aa198;">'plot2.pdf'</span>))
  dev.off()
  </pre>
  
  
  <p>
  And now run it (I do this from a main `overview' script).
  </p>
  
  
  
  <pre class="src src-R"><span style="color: #268bd2; font-weight: bold;">source</span>(<span style="color: #2aa198;">'src/generate_plots.r'</span>)
  </pre>
  
  </div>
  
  </div>
  
  <div id="outline-container-21" class="outline-2">
  <h2 id="sec-21"><span class="section-number-2">21</span> First letter</h2>
  <div class="outline-text-2" id="text-21">
  
  
  
  </div>
  
  </div>
  
  <div id="outline-container-22" class="outline-2">
  <h2 id="sec-22"><span class="section-number-2">22</span> Second letter</h2>
  <div class="outline-text-2" id="text-22">
  
  
  
  </div>
  
  </div>
  
  <div id="outline-container-23" class="outline-2">
  <h2 id="sec-23"><span class="section-number-2">23</span> Report results</h2>
  <div class="outline-text-2" id="text-23">
  
  <p>\section{Report results}
  We see that both the first and second letter distributions are very skewed. To make a note of this for posterity, we can write up our discovery in a text file that we store in the reports directory.
  </p>
  
  
  
  <pre class="src src-R">\documentclass[a4paper]{article}
  \title{Letters analysis}
  \author{Ivan Hanigan}
  \begin{document}
  \maketitle
  blah blah blah
  \end{document}
  
  
  </pre>
  
  
  
  
  
  </div>
  
  </div>
  
  <div id="outline-container-24" class="outline-2">
  <h2 id="sec-24"><span class="section-number-2">24</span> Produce final report</h2>
  <div class="outline-text-2" id="text-24">
  
  
  
  
  <pre class="src src-R"><span style="color: #586e75;"># </span><span style="color: #586e75;">now run LaTeX on the file in reports/letters.tex</span>
  </pre>
  
  </div>
  
  </div>
  
  <div id="outline-container-25" class="outline-2">
  <h2 id="sec-25"><span class="section-number-2">25</span> Personalised project management directories</h2>
  <div class="outline-text-2" id="text-25">
  
  
  <p>
  \section{Personalised project management directories}
  </p>
  
  
  
  
  
  <pre class="src src-R"><span style="color: #586e75;">####</span>
  <span style="color: #586e75;"># </span><span style="color: #586e75;">init additional directories for project management</span>
  analysisTemplate()
  </pre>
  
  
  <pre class="src src-R">dir()
  </pre>
  
  
  <table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
  <colgroup><col class="left" />
  </colgroup>
  <tbody>
  <tr><td class="left">admin</td></tr>
  <tr><td class="left">analysis</td></tr>
  <tr><td class="left">data</td></tr>
  <tr><td class="left">document</td></tr>
  <tr><td class="left">init.r</td></tr>
  <tr><td class="left">metadata</td></tr>
  <tr><td class="left">ProjectTemplateDemo.org</td></tr>
  <tr><td class="left">references</td></tr>
  <tr><td class="left">tools</td></tr>
  <tr><td class="left">versions</td></tr>
  </tbody>
  </table>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  </div>
  </div>
  </div>
  
  </body>

#+end_src

** part 3 stub
 the [ProjectTemplate R package](http://www.johnmyleswhite.com/notebook/2010/08/26/projecttemplate/) to organise my code sections into modules

** NOT POSTED Data analysis project organisation contribution to mozilla 
~/projects/ivanhanigan.github.com.raw/_posts/2015-10-06-aaa.md
#+name:aaa-header
#+begin_src markdown :tangle no :exports none :eval no :padline no
---
name: aaa
layout: post
title: aaa
date: 2015-10-06
categories:
-
---


how to keep data organized and easy to reuse at a later date (including in-house reuse)
data storage/archiving! It's frightening how hard it can be to find raw data from a project - or if you can find it at all?
Agreed! How to organize your data with informative naming scheme and minimize duplication or lots of sub-dir
maybe a data citation unit (DOI == link to data), + good names / layouts in section on clean data. 
 my concern is how to deal with in house data before you start your analysis 1/2
 this is critical for reproducible research - so that you could go back and reproduce your analysis... 
I recommend some 'convention over configuration' advice, and links to evidence based recommended filing systems.  My faves are:

##  a 2008 book recommended folder structure for statistical programmers 
- http://www.indiana.edu/~jslsoc/web_workflow/wf_home.html
- Recently updated with Long, S. (2015). Workflow for Reproducible Results. IV : Managing digital assets Workflow for Tools for your WF. http://txrdc.tamu.edu/documents/WFtxcrdc2014_4-digital.pdf

```
\ProjectAcronym
    \- History starting YYYY-MM-DD
    \- Hold then delete 
    \Admin
    \Documentation 
    \Posted
         \Paper 1
             \Correspondence 
             \Text
             \Analysis
    \PrePosted 
    \Resources 
    \Write 
    \Work
```
## Simple R analysis
This concept originally introduced by Josh Reich as the LCFD framework, on the stack overflow website here http://stackoverflow.com/a/1434424, and encoded into the makeProject R package http://cran.r-project.org/web/packages/makeProject/makeProject.pdf.

```r
# choose your project dir
setwd("~/projects")   
library(makeProject)
makeProject("makeProjectDemo")

# gives
/makeProjectDemo/
    /code/*.R
    /data/
    /DESCRIPTION
    /main.R

# in main.R you put
source("code/load.R")
source("code/clean.R")
source("code/func.R")
source("code/do.R")
```

## More complicated R framework for data analysis
- http://projecttemplate.net

```
/project/
    /cache/
    /config/
    /data/
    /diagnostics/
    /doc/
    /graphs/
    /lib/
        /helpers.R
    /logs/
    /munge/
    /profiling/
        /01_profile.R
    /reports/
    /src/
        /01_EDA.R
        /02_clean.R
        /03_do.R
    /tests/
        /01_tests.R
    /README
    /TODO
```

## For metadata I like EML
- https://github.com/ropensci/EML
- Ecological Metadata Language interface for R: synthesis and integration of heterogenous data     
#+end_src

** Data Management and Workflow for a Team
*** UCRH DMP including workflow ideas, based on Long
**** COMMENT TODO UCRH and Scott Long inspired files/folders for projects and analyses (EXTENDED ON Q)
****  UCRH and Scott Long inspired files/folders for projects and analyses
***** Background
Scott Long wrote a book on this in 2008, for UCRH need to Keep It Sensibly Simple (KISS) with documents and spreadsheets rather than databases and R scripts, need to showcase some use examples.

Reproducibility and documentation: Long says: "Replication should involve retrieving documentation, not trying to remember."

Challenge: remembering where things are.  Key is convention over configuration, which means following conventional rules rather than intuition and configuring things to suit a personal preference.

Also similar to 'Evidence-Based' methods, that require demonstration of efficacy before broad implementation.

***** Key components
****** Workplan 
******* Analysis Plan
******** Question
******** Methods
******* Data Plan
******** Potential data sources (collected or provided)
******** Potential data derivatives
****** Worklog
******* log of dates, tasks and notes.  for personal workdays or team meetings
****** Workflow
******* Steps, Inputs and Outputs
******* Code
****** Results
****** Reports
**** recommended folders
#+begin_src txt :tangle no :eval no

/data general/
  /Data_Management_Plan.doc
  /Data_Inventory.doc
  /project_or_data (PRJ)/
    /PRJ_workplan.doc 
    /PRJ_worklog.doc
    /PRJ_workflow.xls
    /data_provided
    /data_derived
    /code
    /report
    /result
  /references
#+end_src
**** folder organisation additional for a team
#+begin_src txt :tangle no :eval no

  /working_ivan/
    /inbox_for_ivan/
    /Ivan_work_plan.doc
    /Ivan_work_log.doc
    /Ivan_private/
  /working_jane/
    /Jane_work_plan.doc
    /Jane_private/

#+end_src

**** overview files
*****  dm plan.doc

# describe metadata Standard

# describe computing arrangements 
ie access, backup

# describe data management / analysis workflow
ie data provided, intermediary, code, derived, merging, sharing

*****  dat inv of holdings.doc
****** project
******* - dataset
******** -- entity
********* --- attributes? perhaps this is best stored with the file?

*****  data staging area/
*****  project folder/
******  project workplan.doc
 (one pager, could be printed as a hard copy)
#+begin_src txt :eval no
---
title: "Project Work Plan"
author: "Ivan Hanigan"
date: "16 September 2015"
output: word_document
---

# Urgent Actions
- taskname, notes, scheduled

# Analysis Plan for ENTIRE project
## Questions

## Methods
-  taskname, notes, scheduled
-  taskname, notes, scheduled
-  taskname, notes, scheduled

# Data Plan
- Planning data colleciton acquisition
- general summary stuff, specifics go into a data_inventory.doc 

# Work log (by date)
## oldest date
-  taskname, notes
-  taskname, notes

## date (personal notes)
-  taskname, notes

## date (team meeting notes)
-  taskname, notes
-  taskname, notes

## newest date
-  taskname, notes

#+end_src


******  project data inventory of prospective data to collect / aquire.doc

similar to data inventory in /data\_general, but this is more for doco of this project

******  project analysis method steps and workflow.xls

| Step  | Inputs | Outputs    | Notes                         | Status |
|-------+--------+------------+-------------------------------+--------|
| Step1 | in1    | out1, out2 | A description of why and what | DONE   |
| Step2 | out1   | out3       | why and what                  |        |
| StepN |        |            |                               |        |
|-------+--------+------------+-------------------------------+--------|

******  data provided/
******  data derived/
******  results/
******  report/
******  code/
******  versions/
****  working Ivan
*****  Ivan Work Plan.doc
#+begin_src txt :eval no
---
title: "Personal Work Plan"
author: "Ivan Hanigan"
date: "16 September 2015"
output: word_document
---

# Urgent Actions
- taskname, notes, scheduled

# TASK-list grouped by project
- project, taskname, notes, scheduled
- project, taskname, notes, scheduled
- project, taskname, notes, scheduled

# Work log (by date)
## oldest date
- project, taskname, notes
- project, taskname, notes

## date
- project, taskname, notes

## newest date
- project, taskname, notes

#+end_src

*****  private/

**** COMMENT deprecated version
***** folders
for UCRH need to KISS with word/excel use case (simple, accessible, widespread)


/data general/
  /Data_Management_Plan.doc
  /Data_Inventory.doc
  /project_or_data (PRJ)/
    /PRJ_workplan.doc
    /PRJ_worklog.xls
    /data_provided
    /data_derived
    /code
    /report
    /result
  /references
  /working_ivan/
    /Ivan_work_plan.doc
    /Ivan_work_log.xls
    /Ivan_private
  /working_margaret/
    /Margaret_work_plan.doc
    /??
***** overview files
****** TODO dm plan.doc

# describe metadata Standard

# describe computing arrangements 
ie access, backup

# describe data management / analysis workflow
ie data provided, intermediary, code, derived, merging, sharing

****** TODO dat inv of holdings.doc
project
-dataset
--entity
---attributes?
****** TODO data staging area/
****** TODO project folder/
******* TODO project workplan.doc


---
title: "Project Work Plan"
author: "Ivan Hanigan"
date: "16 September 2015"
output: word_document
---

# Urgent Actions
- taskname, notes, scheduled

# TODO-list grouped by project
- project, taskname, notes, scheduled
- project, taskname, notes, scheduled
- project, taskname, notes, scheduled

# Work log (by date)
## oldest date
- project, taskname, notes
- project, taskname, notes

## date
- project, taskname, notes

## newest date
- project, taskname, notes



******* TODO project data inventory of prospective data to collect / aquire.doc
******* TODO project analysis method steps and workflow.xls
| Step  | Inputs | Outputs    | Notes                         | Status |
|-------+--------+------------+-------------------------------+--------|
| Step1 | in1    | out1, out2 | A description of why and what | DONE   |
| Step2 | out1   | out3       | why and what                  | TODO   |
|       |        |            |                               |        |
|-------+--------+------------+-------------------------------+--------|

******* TODO data provided/
******* TODO data derived/
******* TODO results/
******* TODO report/
******* TODO code/
******* TODO versions/
****** TODO working Ivan
******* TODO Ivan Work Plan.doc
******* TODO private/
** 2015-10-05-naming-conventions-for-computer-files
*** blog
#+name:naming-conventions-for-computer-files-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-05-naming-conventions-for-computer-files.md :exports none :eval no :padline no
---
name: naming-conventions-for-computer-files
layout: post
title: Naming Conventions For Computer Files
date: 2015-10-05
categories:
- disentangle
tags:
- Data Management
---

I'm working on a new Data Management Plan for a research group who
merge data from air pollution, meteorology, population census and
health outcome datasets.  The folder organisation is pretty much under
control now, but the file names are challenging.

I'm searching for inspiration and was recommended this conversation:
[https://github.com/minisciencegirl/studyGroup/issues/20](https://github.com/minisciencegirl/studyGroup/issues/20)

```
File Organization and Naming:
...
I want to include details of what the settings were or what dataset I
started out with. Rather than saving a file name a mile long
"FlightHomLog2_av1_Euc_ArrayEucl_AvgLink", there must be many
different better ways to organizing my file space.
```
<p></p>
There is a bunch of good advice already here, and I recommend the slides [https://github.com/Reproducible-Science-Curriculum/rr-organization1/tree/master/slides/naming-slides](https://github.com/Reproducible-Science-Curriculum/rr-organization1/tree/master/slides/naming-slides) and the two PLOS articles, but I wanted to pull out the two things I think it is important to get right:

- ordering in lists
- substring chunks that can be extracted

I think that the substring chunks are explained well in the slides link above (summary, use '_' or '-' to split the string), but I think that the ordering problem needs some thinking.


## Tidy Data: 

This all reminds me of words from Hadley Wickham about tidy data, and the order that columns should be arranged in tabular data.  The principles are similar I think.

``` 
A good ordering makes it easier to scan the raw values. One way of
organizing variables is by their role in the analysis: are values
fixed by the design of the data collection, or are they measured
during the course of the experiment? Fixed variables describe the
experimental design and are known in advance. Computer scientists
often call fixed variables dimensions, and statisticians usually
denote them with subscripts on random variables. Measured variables
are what we actually measure in the study. Fixed variables should come
first, followed by measured variables, each ordered so that related
variables are contiguous. Rows can then be ordered by the first
variable, breaking ties with the second and subsequent (fixed)
variables. 
```    
<p></p>
<p></p>
Wickham, H. (2014). Tidy Data. JSS Journal of Statistical Software, 59(10). 
Retrieved from [http://www.jstatsoft.org/](http://www.jstatsoft.org/)

## One way that we did this:

Colleagues and I came up with the following protocol for an ecology and biodiversity database

1. Project name (optional sub-project name)
1. Data type (such as experimental unit, observational unit, and/or measurement methods)
1. Geographic location (State, Country)
1. Temporal frequency and coverge Annual or seasonal tranches

## Tidy data generalisable concepts are dimensions and variables

The concept of dimensions and variables can be useful here, and especially for deciding on filenames.  Dimensions are fixed or change slowly while variables change more quickly .  For example the project name is 'fixed', that is it does not change across the files, but the sub-project name does change, just more slowly (say there may be 2-3 different sub-projects within a project). Then there may be a set of data types, and these 'change' more quickly than the sub-project name (by change I mean, there are more of them).  Then the geographic and temporal variables might change quickest of all.

So a general rule for the order of things can be stated 
The more fixed variables should come first (those things that don't change, or don't change much), 
followed by the more fluid variables (or things that change more across the project). 
List elements can then be ordered so that the groups of things that are similar will always be contiguous, and vary sequentially within clusters.

Perhaps an example would be easier to understand.  Here is a set of file names that we constructed for one of our ecological field sites (project) and plots (sub-project or measurement location):

#### Notice we also had a controlled vocabulary of data types and their acronyms before starting this

```
| Filename                                                            | Title                                                                                                                                 |
|---------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------|
| asn_fnqr_soil_charact_robson_2011.csv                               | Soil Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2011                                                              |
| asn_fnqr_soil_pit_robson_2012.csv                                   | Soil Pit Data, Water Content and Temperature, Far North Queensland Rainforest SuperSite, Robson Creek, 2012                           |
| asn_fnqr_veg_seedling_robson_2010-2012.csv                          | Seedling Survey,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2012                                                  |
| asn_fnqr_veg_seedling_transect_coord_robson_2010-2012.csv           | Seedling Survey,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2012                                                  |
| asn_fnqr_core_1ha_robson_2014.csv                                   | Soil Pit Data, Soil Characterisation, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha plot, 2014                   |
| asn_fnqr_fauna_biodiversity_ctbcc_2012.csv                          | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, 2012                                      |
| asn_fnqr_fauna_biodiversity_ctbcc_2013.csv                          | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, 2013                                      |
| asn_fnqr_fauna_biodiversity_ctbcc_capetrib_2014.csv                 | Avifauna Monitoring, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2014                                                |
| asn_fnqr_fauna_biodiversity_ctbcc_lu11a_2014.csv                    | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2014                               |
| asn_fnqr_fauna_biodiversity_ctbcc_lu7a_2014.csv                     | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7A, 2014                                |
| asn_fnqr_fauna_biodiversity_ctbcc_lu7b_2014.csv                     | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7B, 2014                                |
| asn_fnqr_fauna_biodiversity_ctbcc_lu9a_2014.csv                     | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU9A, 2014                                |
| asn_fnqr_fauna_biodiversity_ctbcc-lu11a_2009-2011.csv               | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2009-2011                          |
| asn_fnqr_fauna_biodiversity_ctbcc-lu7a_2009-2011.csv                | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7A, 2009-2011                           |
| asn_fnqr_fauna_biodiversity_ctbcc-lu9a_2009-2011.csv                | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU9A, 2009-2011                           |
| asn_fnqr_fauna_biodiversity_habitat codes_ctbcc-lu11a_2009-2011.pdf | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU11A, 2009-2011                          |
| asn_fnqr_fauna_biodiversity_habitat codes_ctbcc-lu9a_2009-2011.pdf  | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU9A, 2009-2011                           |
| asn_fnqr_fauna_biodiversity_habitat_codes_ctbcc-lu7a_2009-2011.pdf  | Vertebrate Fauna Biodiversity Monitoring, Far North Queensland Rainforest SuperSite, CTBCC, LU7A, 2009-2011                           |
| asn_fnqr_fauna_birds_capture_robson_2011-2014.csv                   | Bird Capture Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2011-2014                                                 |
| asn_fnqr_fauna_birds_robson_2010-2014.csv                           | Bird Survey Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2014                                                  |
| asn_fnqr_fauna_invert_moth_robson_2009.csv                          | Moth Inventory at Canopy and Ground Level, Far North Queensland Rainforest SuperSite, Robson Creek, 2009                              |
| asn_fnqr_fauna_invert_moth_robson_2010.csv                          | Moth Inventory at Canopy and Ground Level, Far North Queensland Rainforest SuperSite, Robson Creek, 2010                              |
| asn_fnqr_fauna_invert_moth_robson_2011.csv                          | Moth Inventory at Canopy and Ground Level, Far North Queensland Rainforest SuperSite, Robson Creek, 2011                              |
| asn_fnqr_fauna_invert_robson_25ha_2013                              | Invertebrate Fauna Survey, Far North Queensland Rainforest SuperSite, Robson Creek, 25 Ha Plot, 2013                                  |
| asn_fnqr_geo_tracks_100m_grid_robson_2010-2013.kml                  | Base Geographical Data,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2013                                           |
| asn_fnqr_geo_tracks_robson_2010-2013.kml                            | Base Geographical Data,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2013                                           |
| asn_fnqr_geo_tracks_robson_2010-2013.mdb                            | Base Geographical Data,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2013                                           |
| asn_fnqr_geo_tracks_trees_robson_2010-2013.kml                      | Base Geographical Data,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2013                                           |
| asn_fnqr_soil_cosmos_robson_2011.csv                                | Soil Sampling for Calibration of Cosmic Ray Soil Moisture Sensor, Far North Queensland Rainforest SuperSite, Robson Creek, 2011       |
| asn_fnqr_soil_pit_robson_2012.csv                                   | Soil Pit Data, Soil Characterisation, Far North Queensland Rainforest SuperSite, Robson Creek, 2012                                   |
| asn_fnqr_soil_pit_robson_2013.csv                                   | Soil Pit Data, Water Content and Temperature, Far North Queensland Rainforest SuperSite, Robson Creek, 2013                           |
| asn_fnqr_soil_properties_ddc_2013.csv                               | Soil Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2013                                                 |
| asn_fnqr_soil_properties_ddc_2014.csv                               | Soil Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2014                                                 |
| asn_fnqr_soil_properties_robson_2014.csv                            | Soil Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2014                                                              |
| asn_fnqr_stream_chem_robson_201310.csv                              | Water Chemistry Data, Far North Queensland Rainforest SuperSite, Robson Creek, 201310-201311                                          |
| asn_fnqr_stream_chem_robson_201310-201405.csv                       | Water Chemistry Data, Far North Queensland Rainforest SuperSite, Robson Creek, 201310-201405                                          |
| asn_fnqr_stream_chem_robson_201311.csv                              | Water Chemistry Data, Far North Queensland Rainforest SuperSite, Robson Creek, 201310-201311                                          |
| asn_fnqr_stream_chem_std_methods_robson_2013.pdf                    | Water Chemistry Data, Far North Queensland Rainforest SuperSite, Robson Creek, 201310-201311                                          |
| asn_fnqr_stream_phys-chem_diagram_robson_2013.pdf                   | Stream Physico-Chemical Data,  Far North Queensland Rainforest SuperSite, Robson Creek, 201304-201305                                 |
| asn_fnqr_stream-phys-chem_robson_201304-201305.csv                  | Stream Physico-Chemical Data,  Far North Queensland Rainforest SuperSite, Robson Creek, 201304-201305                                 |
| asn_fnqr_veg_cwd_robson_core_1ha_2012.csv                           | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_dbh-h_capetrib_crane_plot_2001.csv                     | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 1 ha Crane Plot, 2001                               |
| asn_fnqr_veg_dbh-h_capetrib_crane_plot_2005.csv                     | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 1 ha Crane Plot, 2005                               |
| asn_fnqr_veg_dbh-h_capetrib_crane_plot_2010.csv                     | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 1 ha Crane Plot, 2010                               |
| asn_fnqr_veg_dbh-h_robson_25ha_2009-2015.csv                        | Vascular Plant Data ÔëÑ 10 cm DBH,  Far North Queensland Rainforest SuperSite, Robson Creek, 25 ha Plot, 2009-2015                    |
| asn_fnqr_veg_fruit_robson_25ha_2011-2015.csv                        | Fruit Phenology, Far North Queensland Rainforest SuperSite, Robson Creek, 25 ha Plot, 2011-2015                                       |
| asn_fnqr_veg_gentry_mid-stratum_robson_core_1ha_2012.csv            | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_gentry_sub-stratum_herbs_robson_core_1ha_2012.csv      | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_gentry_sub-stratum_shrubs_robson_core_1ha_2012.csv     | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_lai_robson_core_1ha_2012.pdf                           | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_leaf_phys_capetrib_20120831.csv                        | Leaf Level Physiology, Chemistry and Structural Traits, Far North Queensland Rainforest SuperSite, Cape Tribulation, Crane Site, 2012 |
| asn_fnqr_veg_leaf_phys_master_aci_curves_robson_2012.csv            | Leaf Level Physiology, Chemistry and Structural Traits, Far North Queensland SuperSite, Robson Creek, 2012                            |
|                                                                     |                                                                                                                                       |
| asn_fnqr_veg_leaf_phys_master_ai_curves_robson_2012.csv             | Leaf Level Physiology, Chemistry and Structural Traits, Far North Queensland SuperSite, Robson Creek, 2012                            |
|                                                                     |                                                                                                                                       |
| asn_fnqr_veg_seedling_species_robson_2010-2012.csv                  | Seedling Survey,  Far North Queensland Rainforest SuperSite, Robson Creek, 2010-2012                                                  |
| asn_fnqr_veg_species_capetrib_crane_plot_2001.csv                   | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 1 ha Crane Plot, 2001                               |
| asn_fnqr_veg_species_capetrib_crane_plot_2005.csv                   | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 1 ha Crane Plot, 2005                               |
| asn_fnqr_veg_species_capetrib_crane_plot_2010.csv                   | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 1 ha Crane Plot, 2010                               |
| asn_fnqr_veg_species_robson_2012.csv                                | Vegetation Species List, Far North Queensland Rainforest SuperSite, Robson Creek, 2012                                                |
| asn_fnqr_veg_struct_robson_core_1ha_2012.csv                        | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_centre_images_2012.zip          | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_centre_images_2014.zip          | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_ne_corner_images_2012.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_ne_corner_images_2014.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_nw_corner_images_2012.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_nw_corner_images_2014.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_se_corner_images_2012.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_se_corner_images_2014.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_sw_corner_images_2012.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_struct_robson_core_1ha_sw_corner_images_2014.zip       | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_structure_robson_core_1 ha.csv                         | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_veg_vascular plant list_robson_core_1 ha_2012.csv          | Vascular Plant Data, Far North Queensland Rainforest SuperSite, Robson Creek, Core 1 ha, 2012                                         |
| asn_fnqr_water_properties_robson_2013.csv                           | Stream Physico-Chemical Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2013                                           |
| asn_fnqr_water_properties_robson_2014.csv                           | Stream Physico-Chemical Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2014                                           |
| asn_fnqr_weather_capetrib_2006.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2006                                               |
| asn_fnqr_weather_capetrib_2007.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2007                                               |
| asn_fnqr_weather_capetrib_2008.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2008                                               |
| asn_fnqr_weather_capetrib_2009.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2009                                               |
| asn_fnqr_weather_capetrib_2010.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2010                                               |
| asn_fnqr_weather_capetrib_2011.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2011                                               |
| asn_fnqr_weather_capetrib_2012.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2012                                               |
| asn_fnqr_weather_capetrib_2013.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2013                                               |
| asn_fnqr_weather_capetrib_2014.csv                                  | Weather Station Data, Far North Queensland Rainforest SuperSite, Cape Tribulation, 2014                                               |
| asn_fnqr_weather_ddc_2008.csv                                       | Weather Station Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2008                                      |
| asn_fnqr_weather_ddc_2009.csv                                       | Weather Station Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2009                                      |
| asn_fnqr_weather_ddc_2010.csv                                       | Weather Station Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2010                                      |
| asn_fnqr_weather_ddc_2011.csv                                       | Weather Station Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2011                                      |
| asn_fnqr_weather_ddc_2012.csv                                       | Weather Station Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2012                                      |
| asn_fnqr_weather_ddc_2013.csv                                       | Weather Station Data, Far North Queensland Rainforest SuperSite, Daintree Discovery Centre, 2013                                      |
| asn_fnqr_weather_robson_2010.csv                                    | Weather Station Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2010                                                   |
| asn_fnqr_weather_robson_2011.csv                                    | Weather Station Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2011                                                   |
| asn_fnqr_weather_robson_2012.csv                                    | Weather Station Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2012                                                   |
| asn_fnqr_weather_robson_2013.csv                                    | Weather Station Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2013                                                   |
| asn_fnqr_weather_robson_2014.csv                                    | Weather Station Data, Far North Queensland Rainforest SuperSite, Robson Creek, 2014                                                   |

```

#+end_src
*** COMMENT tab-code
#+name:tab
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:tab ####
  dat <- read.csv("~/Dropbox/data/asn_data_inventory/listingnames.csv")
  dat
#+end_src

*** COMMENT filelist-code
#+name:filelist
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:filelist ####
  We avoided having zillions of files in a folder, but used folders too. The main repo we used is a data portal called 'Metacat', an open source portal designed to host all kinds of ecological data https://knb.ecoinformatics.org/#tools. This indexes files and the user can browse or search a catalogue.
  
  I think the consideration about people being uncomfortable with a zillion files in a folder is important (plus issues of disk I/O when backing up or copying such a beast). We were safeguarding against users downloading a stack of files, and then not knowing where they had come from... a potential issue I think you'll agree? This way it seems not to hard to figure out what these are regardless where they end up on the downstream users computer.
  
  In my own work, I do like the ability to use substrings and regular expressions on lists of zillions of files, but I also keep these in sensible folders. For eg:
  
  setwd("~/data/AWAP_GRIDS/")
  filelist <- dir(recursive=T, pattern="totals_2013")
  filelist
        [,1]                              
   [1,] "data/totals_2013010120130131.tif"
   [2,] "data/totals_2013020120130228.tif"
   [3,] "data/totals_2013030120130331.tif"
   [4,] "data/totals_2013040120130430.tif"
   [5,] "data/totals_2013050120130531.tif"
   [6,] "data/totals_2013060120130630.tif"
   [7,] "data/totals_2013070120130731.tif"
   [8,] "data/totals_2013080120130831.tif"
   [9,] "data/totals_2013090120130930.tif"
  [10,] "data/totals_2013100120131031.tif"
  [11,] "data/totals_2013110120131130.tif"
  [12,] "data/totals_2013120120131231.tif"
  
  setwd("~/data/AWAP_GRIDS/")
  filelist <- dir(recursive=T, pattern="totals_2013")
  filelist <- filelist[13:24]
  matrix(filelist)
  
#+end_src


** TODO Data Reference Syntax via Dr Climate (Damien Irving)
https://drclimate.wordpress.com/2015/09/04/managing-your-data/
https://github.com/DamienIrving/climate-analysis/blob/master/data_reference_syntax.md
** 2015-10-07-a-quick-review-of-a-quick-guide-to-organizing-computational-biology-projects
#+name:a-quick-review-of-a-quick-guide-to-organizing-computational-biology-projects-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-07-a-quick-review-of-a-quick-guide-to-organizing-computational-biology-projects.md :exports none :eval no :padline no
---
name: a-quick-review-of-a-quick-guide-to-organizing-computational-biology-projects
layout: post
title: A quick review of a quick guide to organizing computational biology projects
date: 2015-10-07
categories:
- disentangle
tags:
- Data Management
---

The organisation of material is a particularly vexatious topic.  For a data analysis project it is very important that the set of folders and files is logical and intuitive, as well as being well documented. The oft-heard exhortation by computer scientists to their users to 'Read The F-ing Manual' (RTFM) is perennial and rooted in the fundamental difficulty of readers to have the time required to read and digest the detailed information there-in.  

- In this post I review a paper that I was referred to by recent activity on the 
Mozillascience studyGroupLesson 'Open Science Utility Belt':
[https://github.com/mozillascience/studyGroupLessons/issues/7](https://github.com/mozillascience/studyGroupLessons/issues/7)
- That group also holds a journal club [https://github.com/minisciencegirl/studyGroup/issues/20#issuecomment-134750483](https://github.com/minisciencegirl/studyGroup/issues/20#issuecomment-134750483) and they reviewed this paper:
- Noble, W. S. (2009). A quick guide to organizing computational biology projects. PLoS Computational Biology, 5(7), 1–5. [http://dx.doi.org/10.1371/journal.pcbi.1000424](http://dx.doi.org/10.1371/journal.pcbi.1000424)

I missed out so I thought I'd put my notes up here for reference:

1. core guiding principle is simple: Someone unfamiliar with your project should be able to look at your computer files and understand in detail what you did and why
1. your future self may find it difficult to understand your current work.
1. Noble's law: 'Everything you do, you will probably have to do over again'
1. store all of the files relevant to one project in common root directory
1. The exception to this rule is data/code that are used in multiple projects, they are standalone projects
1. Within a given project, use a top-level organization that is logical first, then chronological  at the next level, and then logical organization next
1. Core folders are `data`, `results`, `doc` (versus Berndt Weiss' `dat`, `ana`, `doc`)
1. Chronological order?  'tempting to apply a similar, logical organization... this approach is risky, because the logical structure of your final set of experiments may look drastically different from the form you initially designed. This is particularly true under the results directory, where you may not even know in advance what kinds of experiments you will need to perform'

### Recommended folder and file structures [http://dx.doi.org/10.1371/journal.pcbi.1000424.g001](http://dx.doi.org/10.1371/journal.pcbi.1000424.g001)

```r
/projectname (eg msms)/
    /doc/
        /ms-analysis.html 
        /paper/
            /msms.tex
            /msms.pdf
    /data/
        /YYYY-MM-DD/
            /yeast/
                /README
                /yeast.sqt
            /worm/
                /README
                /worm.sqt
    /src/
        /ms-analysis.c
    /bin/
        /parse-sqt.py
    /results/
        /notebook.html 
        /YYYY-MM-DD-1/
            /runall
            /split1/
            /split2/
        /YYYY-MM-DD-2/
            /runall
```
<p></p>

1. Use a driver script to automate creation of a directory structure 
1. maintain a chronologically organized lab notebook (I have been calling this a work 'log' sensu Scott Long's 2008 'Workflow book')
1. create either a README file, or a command line driver script (he calls this `runall`, but see also `main.R` sensu the Reichian LCFD model)
1. you should end up with a file that is parallel to the lab notebook entry. The lab notebook contains a prose description of the exper- iment, whereas the driver script contains all the gory details
1. Version Control.  'Nuff said! But how to build capacity with Github when all my colleagues seem so confused by it?




    
#+end_src

** 2015-10-12-a-driver-script-to-set-up-a-data-analysis-pipeline
*** blog
#+name:a-driver-script-to-set-up-a-data-analysis-pipeline-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-12-a-driver-script-to-set-up-a-data-analysis-pipeline.md :exports none :eval no :padline no
---
name: a-driver-script-to-set-up-a-data-analysis-pipeline
layout: post
title: A Driver Script To Set Up A Data Analysis Pipeline
date: 2015-10-12
categories:
- disentangle
tags: 
- Data Management
---

- In my previous post I reviewed a paper by Noble 2009 that proposed recommendations for best practice ways to set up a data analysis pipeline [http://ivanhanigan.github.io/2015/10/a-quick-review-of-a-quick-guide-to-organizing-computational-biology-projects/](http://ivanhanigan.github.io/2015/10/a-quick-review-of-a-quick-guide-to-organizing-computational-biology-projects/)
- I was following up on a series of posts I made about other best practice recommendations [http://ivanhanigan.github.io/2015/09/reproducible-research-and-managing-digital-assets-part-3/](http://ivanhanigan.github.io/2015/09/reproducible-research-and-managing-digital-assets-part-3/)
- In the paper by Noble it is suggested that a one should use a 'driver script' to automate creation of a directory structure, this is the exact way that `ProjectTemplate` and `makeProject` work as I described them in the series of posts.
- I think Noble's framework offers something new to the recomendations I had canvassed, that is the idea of chronological order of the contents of the results directory.  I think this is an eminently sensible idea and thought that the R function `Sys.Date()` would be a great way to start off a project in this way.  
- so I have put together the following R function, as an alternative to the `makeProject` core function, that I thought I'd name so that there may be a family of makeProject functions, so that analysts have a range of to choose from.  The other candidate would be `makeProjectLong`, which I will also put up before long.

```
makeProjectNoble <- function(rootdir = getwd()){
  if(!exists(rootdir)) dir.create(rootdir)
  dir.create(file.path(rootdir,'doc'))
  dir.create(file.path(rootdir,'doc','paper'))
  sink(file.path(rootdir,'doc','workplan.Rmd'))
    cat(sprintf("---\ntitle: Untitled\nauthor: your name\ndate: %s\noutput: html_document\n---\n\n",
                Sys.Date()))
  sink()
  dir.create(file.path(rootdir,'data'))
  dir.create(file.path(rootdir,'data', Sys.Date()))
  dir.create(file.path(rootdir,'src'))
  dir.create(file.path(rootdir,'results')) 
  dir.create(file.path(rootdir,'results', Sys.Date())) 
  file.create(file.path(rootdir,'README.md'))
  }

```    

<p></p>

- Running this function will deploy the folders and files (I excluded the `bin` folder for compiled binaries, as I believe that many data analysts may not need that, and those who do are geeky enough to write their own driver scripts.

![/images/testProjectNoble.png](/images/testProjectNoble.png)

- The Rmarkdown script is waiting for the analysis plan to be pumped out, and work can begin

![/images/testProjectNobleMD.png](/images/testProjectNobleMD.png)


# References

- Noble, W. S. (2009). A quick guide to organizing computational biology projects. PLoS Computational Biology, 5(7), 1–5. [http://dx.doi.org/10.1371/journal.pcbi.1000424](http://dx.doi.org/10.1371/journal.pcbi.1000424)

#+end_src
*** COMMENT makeProjectNoble-code
#+name:makeProjectNoble
#+begin_src R :session *R* :tangle R/makeProjectNoble.R :exports none :eval no
  #### name:makeProjectNoble ####
  makeProjectNoble <- function(rootdir = getwd()){
    if(!exists(rootdir)) dir.create(rootdir)
    dir.create(file.path(rootdir,'doc'))
    dir.create(file.path(rootdir,'doc','paper'))
    sink(file.path(rootdir,'doc','workplan.Rmd'))
      cat(sprintf("---\ntitle: Untitled\nauthor: your name\ndate: %s\noutput: html_document\n---\n\n",
                  Sys.Date()))
    sink()
    dir.create(file.path(rootdir,'data'))
    dir.create(file.path(rootdir,'data', Sys.Date()))
    dir.create(file.path(rootdir,'src'))
    dir.create(file.path(rootdir,'results')) 
    dir.create(file.path(rootdir,'results', Sys.Date())) 
    file.create(file.path(rootdir,'README.md'))
    }
  
#+end_src

** COMMENT makeProjectBigger
#+name:makeProjectBigger
#+begin_src R :session *R* :tangle R/makeProjectBigger.R :exports none :eval yes
  #### name:makeProjectBigger ####
  makeProjectBigger <- function(name = "myProject", path = getwd(), force = FALSE){
  
    workplan  <- file.path(path, name, 'workplan.Rmd')
    if(!file.exists(workplan) | force == TRUE){
      txt <-     sprintf("---\ntitle: Project Workplan\nauthor: your name\ndate: %s\noutput:\n  html_document:\n    toc: true\n---\n\n```{r, eval = F, echo = F}\nsetwd('%s')\nrmarkdown::render('%s', 'html_document')\n```\n\n",
                  Sys.Date(),                  
                  dirname(workplan),
                  basename(workplan)
                  )
  tbl <- read.table(textConnection("metadata_field         description
  shortname       A concise name that describes the resource that is being documented. Example is vernal-data-1999.
  creator         The name of the person, organization, or position who created the data
  contact         A contact name for general enquiries. This field is COMPULSORY.
  contact_email   An email address for general enquiries.
  abstract        A brief overview of the resource that is being documented. The abstract should include basic information that summarizes the resource.
  title   Suggested structure is: [Project name (optional sub-project name)] [:] [Data type (such as experimental unit, observational unit, and/or measurement methods)] [,] [Geographic location] [,] [State] [,] [Country] [,] [Annual or seasonal tranches] 
  "), sep = "\t", stringsAsFactor = F, header = T)
      #str(tbl)
      txt2 <- paste(txt,
  sprintf("\n\n# Introduction\n
  ## Background
  - Introduce the study
  
  ## General stuff
  - Key contacts and phone numbers
  
  ## TODO-list
  - Task, Person, Timeframe, Notes
  
  ## Resources
  - Funding, people, data (highlevel)
  
  # Analysis plan
  - Questions
  - Literature base
  - Method type
  
  # Data plan
  
  %s\n\n", 
    print(xtable::xtable(tbl), type = 'html', comment = F, include.rownames = F)
               )
                    
                   , sep = "")
      sink(workplan)
      cat(txt2)
      sink()
  
    }
  }
  
#+end_src

#+RESULTS: makeProjectBigger


* Data Documentation
** 2015-03-19-r-eml-to-mitigate-risks-in-morpho-metacat-data-publishing


#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-03-19-r-eml-to-mitigate-risks-in-morpho-metacat-data-publishing.md :exports none :eval no :padline no
---
name: r-eml-to-mitigate-risks-in-morpho-metacat-data-publishing
layout: post
title: Using the R EML software to mitigate risks in Morpho and Metacat data publishing
date: 2015-03-19
categories:
- morpho
- data documentation
---

# Introduction

- Over the last few months I have used software called Metacat as a Data Portal and Repository.  Metacat is server software which has been
developed by the Knowledge Network for Biocomplexity (KNB). 
- Metacat
conforms to the Ecological Metadata Language (EML) Standard ([https://knb.ecoinformatics.org/#external//emlparser/docs/index.html](https://knb.ecoinformatics.org/#external//emlparser/docs/index.html)).  
- KNB also develop another software package called Morpho to be used by Ecologists to document their data ([https://knb.ecoinformatics.org/#tools/morpho](https://knb.ecoinformatics.org/#tools)).
- Morpho can be used to send the data and metadata documents to be published on a Metacat portal. 
- KNB’s software is used internationally by the Data
Observation Network for Earth (DataONE) nodes, the United States Long
Term Ecological Research (US LTER) network and the International Long
Term Ecological Research (ILTER) network. 
- Additionally, the Australian Long Term Ecological Research Network 
Data Portal ([www.ltern.org.au/knb/](http://www.ltern.org.au/knb/)), Australian SuperSites Network and
Australian Centre for Ecological Analysis and Synthesis used
the same underlying technology to publish data packages.
- The Metacat system is great for a data repository but unfortunately (in my experience) the Morpho software package has repeatedly hampered data processing and increased risks of inadvertently publishing data with errors. 
- My colleagues and I workaround these problems using a lot of different 'fixes' for the different problems.
- Fortunately there is an alternative to Morpho in the R statistical software environment called the R-EML package ([https://github.com/ropensci/EML](https://github.com/ropensci/EML)).  This provides a library of functions used in the R language to generate and parse EML files.
- This new workflow mitigates some of the risks of the Morpho software by ensuring the data related steps of the workflow are conducted in the R environment for statical computing.
- However, some Issues remain in that this requires a fairly specialised computing environment with various Linux libraries configured appropriately


# Results
 
- I generate EML metadata using REML in the workflow shown in the figure below.

![altext](/images/workflow-rmd-md.png)

<p></p>

- Image adapted from [http://kieranhealy.org/blog/archives/2014/01/23/plain-text/](http://kieranhealy.org/blog/archives/2014/01/23/plain-text/)


#+end_src

** TODO xxx-adding-value-labels-to-reml-boilerplate
*** adding-value-labels-to-reml-boilerplate-header
#+begin_src R :tangle no :exports none :eval no :padline no
  ---
  name: adding-value-labels-to-reml-boilerplate
  layout: post
  title: adding-value-labels-to-reml-boilerplate
  date: 2014-08-22
  categories:
  -
  ---
  
  #### Code:adding-value-labels-to-reml-boilerplate
  #For sake of argument, imagine a data.frame looking something like this:
  
  
  dat = data.frame(river=c("SAC", "SAC", "AM"),
                   spp = c("king", "king", "ccho"),
                   stg = c("smolt", "parr", "smolt"),
                   ct =  c(293, 410, 210L))
  
  xtable::xtable(dat)
  
  
  
  #In case our column header abbreviations are not obvious to others (or our future selves), we take a moment to define them:
  
  
  col_metadata = c(river = "http://dbpedia.org/ontology/River",
                   spp = "http://dbpedia.org/ontology/Species",
                   stg = "Life history stage",
                   ct = "count")
  
  
  
  # Define the units used in each column.  In the case of factors, we define the abbreviations in a named string.
  
  
  unit_metadata =
    list(river = c(SAC = "The Sacramento River", AM = "The American River"),
         spp = c(king = "King Salmon", ccho = "Coho Salmon"),
         stg = c(parr = "third life stage", smolt = "fourth life stage"),
         ct = "number")
  
  # automated?
  #dat <- dat[,-4]
  dat
  
#+end_src

*** COMMENT reml_boilerplate1-code
#+name:reml_boilerplate
# begin_src R :session *R* :tangle R/reml_boilerplate.r :exports reports :eval no
#+begin_src R :session *R* :tangle no :exports reports :eval no

  ################################################################
  # name:reml_boilerplate
   
  # func
  ## if(!require(EML)) {
  ##   require(devtools)
  ##   install_github("EML", "ropensci")
  ##   } 
  ## require(EML)
  
  reml_boilerplate <- function(data_set, outfile = NA, created_by = "Ivan Hanigan <ivanhanigan@gmail.com>", data_dir = getwd(), titl = NA)
  {
  
    # next create a list from the data
    unit_defs <- list()
    for(i in 1:ncol(data_set))
      {
        # i = 4
        if(is.numeric(data_set[,i])){
          unit_defs[[i]] <- "number"
        } else {
          unit_defs[[i]] <- names(data_set)[i]
        }
      }
  
  # print helpful comments
  cat(
  sprintf('
  # you just got a cheater\'s unit_defs
  # we can get the col names easily
  col_defs <- names(dat)
  # then create a dataset with metadata
  ds <- data.set(dat,
                 col.defs = col_defs,
                 unit.defs = unit_defs
                 )
  # now write EML metadata file
  eml_config(creator="%s")
  eml_write(ds,
            file = "%s",
            title = "%s"
            )
  
  # now your metadata has been created
  # if you want to add this to morpho and metacat it will needs something like
  </dataFormat>
    <distribution scope="document">
      <online>
        <url function="download">ecogrid://knb/hanigan.34.1</url>
      </online>
    </distribution>
  </physical>', created_by, outfile, titl)
  )
  
  
    return(unit_defs)
  
   }
#+end_src

*** COMMENT reml_boilerplate2-code
#+name:get_vals
# begin_src R :session *R* :tangle R/reml_boilerplate.r :exports none :eval no
#+begin_src R :session *R* :tangle no :exports none :eval no
  reml_boilerplate <- function(.dataframe){
  strng <- list()
  for(i in 1:ncol(.dataframe)){
    # i = 6
    .variable <- names(.dataframe)[i]
    #.dataframe[,.variable]
      if(is.character(.dataframe[ ,.variable])){
        .dataframe[,.variable]  <- factor(.dataframe[,.variable])
      }
  
    if(is.factor(.dataframe[,.variable])){
      x <- .dataframe[,.variable]
      vals <-  names(table(x))
      # symbols may pollute the string to parse
      vals <- make.names(vals)
      vals <- tolower(vals)  
      vals <- gsub("\\.","_",vals)
      vals <- gsub("_+","_",vals)    
      v <- .variable
      #v
      strng[[.variable]] <- paste(
      v, ' = c(',
      paste(vals, sep = '', collapse = ' = "TBA",')
      ,' = "TBA")', sep = '')
    } else if(is.numeric(.dataframe[,.variable])){
      v <- .variable
      strng[[.variable]] <- paste(v,' = "number"',sep='')
  #    strng[[.variable]] <- '"number"'
      
    } else if(
      !all(is.na(as.Date(as.character(na.omit(.dataframe[,.variable])), origin = "1970-01-01")))
      ){
      v <- .variable    
      strng[[.variable]] <- paste(v,' = "YYYY-MM-DD"',sep='')
  #    strng[[.variable]] <- '"YYYY-MM-DD"'
  
    } else if (all(is.na(.dataframe[ ,.variable]))){
      v <- .variable        
      strng[[.variable]] <- paste(v,' = "', names(.dataframe)[i], '"', sep='')
    }
  }  
  #strng
  strng2 <- ""
  for(n in 1:(length(strng)-1)){
    strng2 <- paste(strng2, strng[[n]], ",\n")
  }
  strng2 <- paste(strng2, strng[[length(strng)]], "\n")
  #cat(strng2)
  strng3 <- paste("
  unit_metadata =
    list(",strng2,")", sep = "")
  #cat(strng3)
  eval(parse(text = strng3))
  #unit_metadata
  return(unit_metadata)
  }
  
  #u1 <- get_vals(analyte)
  #u1
      
#+end_src
*** COMMENT reml_boilerplate3-code
#+name:get_vals
#+begin_src R :session *R* :tangle R/reml_boilerplate.r :exports none :eval no
  
  reml_boilerplate <- function(.dataframe, enumerated = NA){
  strng <- list()
  for(i in 1:ncol(.dataframe)){
  # i = 1
    .variable <- names(.dataframe)[i]
    #.dataframe[,.variable]
      if(is.character(.dataframe[ ,.variable])){
        .dataframe[,.variable]  <- factor(.dataframe[,.variable])
      }
  
    if(is.factor(.dataframe[,.variable])  & i %in% enumerated){
      x <- .dataframe[,.variable]
      vals <-  names(table(x))
      # symbols may pollute the string to parse
      vals <- make.names(vals)
      vals <- tolower(vals)  
      vals <- gsub("\\.","_",vals)
      vals <- gsub("_+","_",vals)    
      v <- .variable
      #v
      strng[[.variable]] <- paste(
      v, ' = c(',
      paste(vals, sep = '', collapse = ' = "TBA",')
      ,' = "TBA")', sep = '')
    } else if(is.factor(.dataframe[,.variable])){
      
      strng[[.variable]] <- paste(
        .variable, ' = "TBA"', sep = ''
        )
  
    } else if(is.numeric(.dataframe[,.variable])){
      v <- .variable
      strng[[.variable]] <- paste(v,' = "number"',sep='')
  #    strng[[.variable]] <- '"number"'
      
    } else if(
      !all(is.na(as.Date(as.character(na.omit(.dataframe[,.variable])), origin = "1970-01-01")))
      ){
      v <- .variable    
      strng[[.variable]] <- paste(v,' = "YYYY-MM-DD"',sep='')
  #    strng[[.variable]] <- '"YYYY-MM-DD"'
  
    } else if (all(is.na(.dataframe[ ,.variable]))){
      v <- .variable        
      strng[[.variable]] <- paste(v,' = "', names(.dataframe)[i], '"', sep='')
    }
  }  
  #strng
  strng2 <- ""
  for(n in 1:(length(strng)-1)){
    strng2 <- paste(strng2, strng[[n]], ",\n")
  }
  strng2 <- paste(strng2, strng[[length(strng)]], "\n")
  #cat(strng2)
  strng3 <- paste("
  unit_metadata =
    list(",strng2,")", sep = "")
  #cat(strng3)
  eval(parse(text = strng3))
  #unit_metadata
  return(unit_metadata)
  }
  
  #u1 <- get_vals(analyte)
  #u1
      
#+end_src


*** reml_boilerplate-test-code
#+name:reml_boilerplate-test
#+begin_src R :session *R* :tangle no :exports reports :eval no
  ################################################################
  # name:reml_boilerplate-test
  library(EML)
  require(devtools)
  load_all()
  #require(disentangle)
  fpath <- system.file("extdata/civst_gend_sector_additional_variables.csv", package = "disentangle")
  analyte <- read.csv(fpath, colClasses=c(ordinal_variable = "character"))
  analyte <- read.csv(fpath, stringsAsFactor = F)
  
  str(analyte)
  analyte$datevar <- as.Date(analyte$datevar)
  analyte$ordinal_variable <- as.character(analyte$ordinal_variable)
  analyte$fractions <- sample(rnorm(1000,0.1,0.1), nrow(analyte))
  
  str(analyte)
  head(analyte)
  unit_defs <- reml_boilerplate(
    data_set = analyte,
    created_by = "Ivan Hanigan <ivanhanigan@gmail.com>",
    titl = "civst_gend_sector_full",
    outfile = 'testing1234'
    )
  unit_defs
  analyte2 <- analyte
  names(analyte)
  #analyte <- analyte2[,-c(6)]
  unit_defs <- reml_boilerplate2(analyte)
  unit_defs
  
  unit_defs <- reml_boilerplate3(analyte, enumerated = c(1,2,3,8))
  unit_defs
  
  
  
  
    # we can get the col names easily
    col_defs <- names(analyte)
    # then create a dataset with metadata
    ds <- data.set(analyte,
                   col.defs = col_defs,
                   unit.defs = unit_defs
                   )
    # now write EML metadata file
    eml_config(creator="TBA <fakeaddress@gmail.com>")
  outfile <- "testingAbcd12234.csv"
  str(ds)
    ## eml_write(ds,
    ##           file = gsub(".csv", "_eml_skeleton.xml", outfile),
    ##           title = gsub(".csv", "", outfile)
    ##           )
  ## [1] "testingAbcd12234_eml_skeleton.xml"
  ## Warning message:
  ## In `[[<-.data.frame`(`*tmp*`, i, value = c("2013-09-09/01/13", "2013-09-09/01/13",  :
  ##   Setting class(x) to NULL;   result will no longer be an S4 object
  ## >
  eml_write(analyte,
            col.defs = col_defs,
            unit.defs = unit_defs,
            creator="TBA <fakeaddress@gmail.com>",
            file = gsub(".csv", "_eml_skeleton.xml", outfile)
            )
  
  tempfile <- dir(pattern="^data_table_")
  tempfile
  file.rename(tempfile, outfile)
  # rename the CSV file.
  
  # dir("data")
#+end_src

*** COMMENT reml_boilerplate new test-code
#+name:reml_boilerplate new test
#+begin_src R :session *R* :tangle no :exports none :eval no
#### name:reml_boilerplate new test####
install.packages("dlnm")
library(dlnm)
library(disentangle)
library(EML)

data(chicagoNMMAPS)
str(chicagoNMMAPS)
?chicagoNMMAPS
data_dictionary(chicagoNMMAPS)
unit_defs <- reml_boilerplate(chicagoNMMAPS)
col_defs <- names(chicagoNMMAPS)
ds <- eml_dataTable(chicagoNMMAPS,
              col.defs = col_defs,
              unit.defs = unit_defs,
              description = "Metadata documentation for chic.csv", 
              filename = "chic.csv")
# now write EML metadata file
eml_config(creator="Antonio Gasparrini
<antonio.gasparrini@lshtm.ac.uk>")
eml_write(ds,
          file = "chic.xml",
          title = "chicagoNMMAPS"
)

# now your metadata has been created

#+end_src

** R-data_dictionary
*** R-R-data_dict
#+name:R-data_dictionary
#+begin_src R :session *R* :tangle R/data_dict.r :exports none :eval no
  # name:data_dict
  data_dict <- function(.dataframe, .variable, .show_levels = -1)
  {
  
  summary2 <- function(x){
    summa <- summary(x, digits = nchar(max(x))+3)
    return(summa)
  }
  
    if(is.character(.dataframe[ ,.variable])){
      .dataframe[,.variable]  <- factor(.dataframe[,.variable])
    }
    if(all(is.na(.dataframe[ ,.variable]))){
      summa <- summary2(.dataframe[,.variable])
      summa <- as.data.frame(t(summa[2]))
      summa[,1]  <- as.numeric(as.character(summa[,1]))
    } else {
      summa <- summary2(.dataframe[,.variable])
    }
  # if there are some missing obs in a date var you get a malformed
  # summa with less names than levels
    if(length(as.character(summa)) != length(names(summa))){
      summa <- as.character(summa)
    }
  
    
    summa <- as.data.frame(
      cbind(
        c(.variable, rep("", length(summa) - 1)),
        names(summa)
        ,
        as.vector(summa)
        )
      )
    summa[,1]  <- as.character(summa[,1])
    summa[,2]  <- as.character(summa[,2])
    # summa
  
    # if char (factor)
    if(is.factor(.dataframe[,.variable])){
    summa$type <- c("character", rep("", nrow(summa) - 1))
    summa$summa  <- as.numeric(as.character(summa$V3))  
    summa$summa2 <- rep(NA, nrow(summa))
        # as.numeric(as.character(summa$V2)) ?
    summa$pct  <- round((summa$summa / sum(summa$summa)) * 100, 2)
    summa <- summa[,c(1,4,2,6,5,7)]
    if(.show_levels > 0){
      if(nrow(summa) > .show_levels){
        summa <- summa[1:.show_levels,]  
        summa <- rbind(summa, c("", "",
                                sprintf("more than %s levels. list truncated.", .show_levels),
                                "","", "")
                       )
        }
      }
    # summa
    } else if (
      is.numeric(.dataframe[,.variable])
      ){
    summa$type <- c("number", rep("", nrow(summa) - 1))
    summa$cnt <- NA
    summa$pct  <- NA
    summa <- summa[,c(1,4,2,3,5,6)]
    } else if (
     !all(
        is.na(as.Date(as.character(na.omit(.dataframe[,.variable])), origin = "1970-01-01"))
        )
      ){
    # if date
    ## datevar <- as.Date(as.character(
    ##   .dataframe[,.variable]
    ##   ), origin = "1970-01-01")
    # http://stackoverflow.com/questions/18178451/is-there-a-way-to-check-if-a-column-is-a-date-in-r
    # as.Date(as.character(.dataframe[,.variable]),format="%Y-%m-%d")
    ## if(!all(is.na(datevar))){
    ##   summa[,3] <- as.character(datevar)
    ## }
      
    summa$type <- c("date", rep("", nrow(summa) - 1))
    summa$cnt <- NA
    summa$pct  <- NA
    # summa
    if(
      length(which(is.na(.dataframe[,.variable]))) > 0
      ){
      summa$V3[-which(summa$V2 == "NA's")] <- as.character(
          as.Date(as.character(
          summa$V3[-which(summa$V2 == "NA's")]
          ), origin = "1970-01-01")
          )    
    } else {
      summa$V3 <- as.character(as.Date(as.numeric(as.character(summa$V3)), origin = "1970-01-01"))    
    }
    summa <- summa[,c(1,4,2,3,5,6)]
    } else if (all(is.na(.dataframe[ ,.variable]))){
      
    summa$type <- c("missing", rep("", nrow(summa) - 1))
    summa$fill  <- NA
    summa$pct  <- 100
    summa <- summa[,c(1,4,2,5,3,6)]
    
    } else {
        stop(sprintf("variable '%s' type is not character, factor, date, numeric or missing", .variable))
    }
    names(summa)  <- c("Variable","Type","Attributes", "Value", "Count", "Percent")
    # summa
    return(summa)
  }
  
#+end_src

#+RESULTS: R-data_dictionary

*** data_dictionary-code
#+name:data_dictionary
#+begin_src R :session *R* :tangle R/data_dictionary.r :exports none :eval no
  ################################################################
  data_dictionary <- function(dataframe, show_levels = -1){
    out <- matrix(NA, nrow = 0, ncol = 3)
    for(i in 1:ncol(dataframe)){
    #  i = 1
    #  print(i)
    out2 <- data_dict(
      .dataframe = dataframe
      ,
      .variable = names(dataframe)[i]
      ,
      .show_levels = show_levels
      )
    out <- rbind(out, out2)
    }
    row.names(out) <- NULL
    return(out)
  }
#+end_src

*** test-R-data_dictionary
#+name:R-data_dictionary
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:R-data_dictionary
  require(devtools)
  install_github("ivanhanigan/disentangle")
  # or
  load_all()
  require(disentangle)
  fpath <- system.file("extdata/civst_gend_sector.csv", package = "disentangle")
  fpath
  civst_gend_sector <- read.csv(fpath)
  civst_gend_sector$datevar <- as.Date(round(rnorm(nrow(civst_gend_sector), Sys.Date(),10)), origin = "1970-01-01")
  civst_gend_sector$missing_variable  <- NA
  civst_gend_sector$Survey.year  <- as.integer(rep(2012, nrow(civst_gend_sector)))
  str(civst_gend_sector)
  #data_dict
  data_dict(civst_gend_sector, "civil_status")
  class(civst_gend_sector$datevar)
  data_dict(civst_gend_sector, "datevar")
  data_dict(civst_gend_sector, "number_of_cases")
  data_dict(civst_gend_sector, "missing_variable")
  data_dict(civst_gend_sector, "Survey.year")
  dataDictionary <- data_dictionary(civst_gend_sector,
                                    show_levels = -1)
  
  dataDictionary
  #write.csv(dataDictionary, "~/dataDictionary.csv", row.names = F)
  
  # with randoms in date variable
  civst_gend_sector$datevar[sample(1:nrow(civst_gend_sector), 5)] <- NA
  # and civil status
  civst_gend_sector$civil_status[sample(1:nrow(civst_gend_sector), 7)] <- NA
  # and number of cases
  civst_gend_sector$number_of_cases[sample(1:nrow(civst_gend_sector), 2)] <- NA
  civst_gend_sector$Survey.year[sample(1:nrow(civst_gend_sector), 4)] <- NA
  # some integer codes
  civst_gend_sector$ordinal_variable <-  as.character(round(sample(rnorm(10000,2.5,1), nrow(civst_gend_sector))))
  civst_gend_sector$alphavar <- sample( letters, nrow(civst_gend_sector))
  str(civst_gend_sector)
  #debug(data_dict)
  data_dict(
            .dataframe=civst_gend_sector
            ,
            .variable="datevar"
            ,
            .show_levels = -1
            )
  #undebug(data_dict)
  
  dataDictionary <- data_dictionary(civst_gend_sector,
                                    show_levels = -1)
  
  dataDictionary
  write.csv(civst_gend_sector,
            file.path("inst/extdata", gsub(".csv","_additional_variables.csv", basename(fpath)))
            , row.names = F)
#+end_src
*** man-R-data_dictionary
#+name:R-data_dictionary
#+begin_src R :session *R* :tangle no :exports none :eval no
################################################################
# name:R-data_dictionary

#+end_src

** variable_names_and_labels
*** R-variable_names_and_labels
#+name:variable_names_and_labels
#+begin_src R :session *R* :tangle R/variable_names_and_labels.r :exports none :eval no
  #' @title variable_names_and_labels
  #' @name variable names and labels
  #' @param datadict a dd object (optional)
  #' @param infile the full pathname (optional)
  #' @param orig_names the original names (optional)
  #' @param insert_labels T/F if the output should summarise the value labels
  
  variable_names_and_labels <- function(datadict = NULL, infile = NULL, orig_names=NULL, insert_labels = FALSE){
    
  if(is.null(datadict)){  
  dat <- read.csv(infile, stringsAsFactors =  F)
  datadict <- data_dictionary(dat)
  }
  
  #### now get variable names as they appear in the dd ####
  datadict$ordering <- 1:nrow(datadict)
  col_defs <- sqldf::sqldf("
  select Variable, Type
  from datadict
  group by Variable, Type
  order by ordering
  ", drv = "SQLite")
  col_defs <- col_defs[col_defs$Variable != "",]
  # col_defs
  # orig_names
  if(!is.null(orig_names)){
    vl <- as.data.frame(cbind(col_defs,orig_names))
    names(vl) <- c("variable_name", "simple_type", "original_name")
  } else {
    vl <- col_defs
  }
  # vl
  vl$description <- ""
  vl$nominal_ordinal_interval_ratio_datetime <- ""
  vl$unit_of_measurement <- ""
  vl$value_labels <- ""
  vl$issue_description <- ""
  vl$depositor_response <- ""
  # it is easy in a spreadsheet to add the value labels but an
  # automation approach is here
  if(insert_labels){
    dat <- read.csv(infile, stringsAsFactors =  F)
    enums <- NA
    for(j in 1:ncol(dat)){
      if(is.character(dat[,j])){
        if(j == 1){
          enums <- 1
        } else {
          enums <- c(enums, j)
        }
      }
    }
    #enums
    #str(dat)
    lablist  <- reml_boilerplate(dat, enumerated = enums)
    lablist <- lapply(lablist, names)
    maxnlabs <- max(sapply(lablist, length))
    if(maxnlabs > 10) stop("more than 10 labels, try a different approach")
    lablist <- lapply(lablist, paste, sep = "", collapse = " = ?; ")  
    lablist  <- do.call(rbind.data.frame, lablist)
    vl$value_labels <- lablist[,1]
  }
    
  # vl
  return(vl)
  }
#+end_src
*** test-variable_names_and_labels
#+name:variable_names_and_labels
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:variable_names_and_labels
  require(devtools)
  install_github("ivanhanigan/disentangle")
  require(disentangle)
  # or
  load_all()
  
  fpath <- "inst/extdata/civst_gend_sector_additional_variables.csv"
  # or
  fpath <- "inst/extdata/civst_gend_sector.csv"
  fpath
  dat <- read.csv(fpath)
  # store orig names
  namlist <- names(dat)
  dat$datevar <- as.Date(dat$datevar)
  
  # changing names is something that we want to track
  names(dat) <- gsub("_", " ", names(dat))
  names(dat)  <- sapply(names(dat), upcase_string)
  str(dat)
  civst_dd <- data_dictionary(dat)
  civst_dd
  fpath
  vl <- variable_names_and_labels(
    datadict = civst_dd
    ,
    infile = fpath
    ,
    orig_names = namlist
    ,
    insert_labels = T
    )
  vl
  vl <- variable_names_and_labels(
    datadict = civst_dd
    ,
    infile = fpath
    ,
    insert_labels = T
    )
  vl
  
#+end_src

* Diagrams


** diagrammer tests
*** first cuts
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:diagrammer
  install.packages('DiagrammeR')
  library(DiagrammeR)
  library(disentangle)
  ###
  # Create a graph with both nodes and edges
  # defined, and, add some default attributes
  # for nodes and edges
  ###
  
  library(DiagrammeR)
  # modified eg from http://rich-iannone.github.io/DiagrammeR/graphs.html
  # Create a node data frame
  nodes <-
    create_nodes(nodes = c("a", "b", "c", "d"),
                 label = c("one", "two", "three", "{{ { Name | Description } | { four | thing } }}"),
                 type = "lower",
                 style = c("filled","", "", ""),
                 color = "aqua",
                 shape = c("circle", "circle",
                           "rectangle", "record"),
                 data = c(3.5, 2.6, 9.4, 2.7),
                 cluster = c("A", "A", "B", "B"))
  nodes
  nodes1 <- subset(nodes, cluster == "A")
  nodes1
  nodes2 <- subset(nodes, cluster == "B")
  nodes2
  edges <-
    create_edges(from = c("a", "b", "c"),
                 to = c("d", "c", "a"),
                 relationship = "leading_to",
                 label = c("+", "-", "?"))
  edges
  edges2 <-
    create_edges(from = c("e", "a"),
                 to = c("f", "e"),
                 relationship = "leading_to",
                 label = c("+", "-"))
  edges2
  graph1 <-
    create_graph(nodes_df = nodes1,
                 edges_df = edges,
                 node_attrs = "fontname = Helvetica",
                 edge_attrs = c("color = blue",
                                "arrowsize = 2"))#,
  #               graph_attrs = "rankdir = LR")
  graph2 <-
    create_graph(nodes_df = nodes2,
                 edges_df = edges2,
                 node_attrs = "fontname = Helvetica",
                 edge_attrs = c("color = blue",
                                "arrowsize = 2"))
      # use create_graph on seperate nodes and edges data frames, one for
      # each cluster
      gr1 <- graph1$dot_code
      gr2 <- graph2$dot_code
      gr1 <- gsub("digraph", "subgraph cluster1", gr1)
      gr2 <- gsub("digraph", "subgraph cluster2", gr2)
      gr3 <- sprintf("digraph{\n%s\n\n %s\n}", gr1, gr2)
      grViz(gr3)
  
  
  # View the graph in the RStudio Viewer
  #render_graph(graph)
  #graph <- add_edges(graph, from = "b", to = "a",
  #                   relationship = "leading_to")
  #get_predecessors(graph, node = "a")
  #graph <- add_edges(graph, from = "d", to = "b",
  #                   relationship = "leading_to")
  #render_graph(graph)
  
#+end_src
*** COMMENT clusters-code
#+name:clusters
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:clusters ####
  #devtools::install_github("rich-iannone/DiagrammeR")
  library(DiagrammeR)
  
  # Group 1
  nodes_1 <- 
    create_nodes(nodes = c("a", "b", "c", "d",
                           "e", "f", "g", "h",
                           "i"),
                 label = TRUE,
                 type = c("X", "X", "X", "X",
                          "Y", "Y", "Y", "Z",
                          "Z"),
                 color = "blue",
                 value = c(182, 98, 182, 112,
                           154, 126, 252, 154,
                           168))
  
  edges_1 <- create_edges(from = c("a", "b", "i", "e", "g", "f", "h", "d", "d"),
                          to =   c("b", "i", "e", "h", "h", "h", "c", "c", "a"),
                          rel = c("rel_a", "rel_a", "rel_b", "rel_c",
                                  "rel_b", "rel_a", "rel_b", "rel_c",
                                  "rel_c"))
  
  # Group 2
  nodes_2 <- 
    create_nodes(nodes = c("j", "k", "l", "m",
                           "n", "o", "p", "q",
                           "r"),
                 label = TRUE,
                 type = c("A", "A", "A", "A",
                          "B", "B", "B", "C",
                          "C"),
                 color = "red",
                 value = c(182, 98, 182, 112,
                           154, 126, 252, 154,
                           168))
  
  edges_2 <- create_edges(from = c("j", "m", "q", "r", "k", "m", "r", "q", "o", "n", "c"),
                          to =   c("k", "q", "o", "n", "o", "n", "j", "l", "r", "p", "p"),
                          rel = c("rel_a", "rel_a", "rel_b", "rel_c",
                                  "rel_b", "rel_a", "rel_b", "rel_c",
                                  "rel_c"))
  
  # Combine graphs
  nodes <- combine_nodes(nodes_1, nodes_2)
  edges <- combine_edges(edges_1, edges_2)
  
  # Render graph
  graph <- create_graph(nodes_df = nodes,
                        edges_df = edges)
  
  render_graph(graph, output = "visNetwork")
  render_graph(graph)
#+end_src

** reference causal DAGs with R
http://donlelek.github.io/2015/03/31/dags-with-r/
*** COMMENT test-code
#+name:test
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:test ####
  library(DiagrammeR)
  grViz("
  digraph causal {
  
    # Nodes
    node [shape = plaintext]
    A [label = 'Age']
    R [label = 'Retained\n Placenta']
    M [label = 'Metritis']
    O [label = 'Cistic ovarian\n disease']
    F [label = 'Fertility']
    
    # Edges
    #edge [color = black,
    #      arrowhead = vee]
    rankdir = LR
    A->F [headlabel='+']
    A->O [headlabel='+']
    A->R
    R->O [headlabel='+']
    R->M
    M->O
    O->F
    M->F
    
    # Graph
    graph [overlap = true, fontsize = 10]
  }")
#+end_src
** R-causal_dag
*** TODO R-causal_dag
#+name:causal_dag
#+begin_src R :session *R* :tangle no :exports none :eval no
  #' @title causal_dag
  #' @name causal_dag
  #' @param indat the input data.frame
  #' @param names_col the name of each edge (the boxes)
  #' @param in_col the name of the nodes that are inputs to each edge with comma seperated vals.  whitespace will be stripped
  #' @param out_col the nodes that are outputs of each edge.
  #' @param desc_col description
  #' @param clusters_col optional column identifying clusters
  #' @param todo_col optional column with TODO status (DONE and WONTDO will be white, others are red)
  #' @return character string object that has the DOT language representatio of the input
  causal_dag <- function(indat, names_col = NULL, in_col = NULL, out_col = NULL, desc_col = NULL, clusters_col = NULL, todo_col = NULL, nchar_to_snip = 40){
  if (is.null(names_col)) stop("Names of the edges are needed")
  if (is.null(in_col)) stop("Inputs are needed")
  if (is.null(out_col)) stop("Outputs are needed")
  
  }
#+end_src
*** test-causal_dag
#+name:causal_dag
#+begin_src R :session *R* :tangle tests/test-causal_dag.r :exports none :eval no
################################################################
# name:causal_dag

#+end_src

*** COMMENT a-code
#+name:a
#+begin_src R :session *R* :tangle a.R :exports none :eval no
  #### name:a ####
  # read in the sheet
  library(disentangle)
  library(stringr)
  causes <- readxl::read_excel("causal-ssheet.xlsx")
  causes
  nodes <- newnode(causes, "edges", "inputs", "outputs")
  cat(nodes)
  # The result is a formated graph in the dot language with some of my
  # preferred settings such as edges showing as 'records' and a spot to
  # write a description or include literature about each process
  
  #####################################################################
  #digraph transformations {
  # 
  #"Metritis" -> "Fertility effects"
  #"Cistic Ovarian Disease" -> "Fertility effects"
  #"Age" -> "Fertility effects"
  #"Fertility effects"  [ shape=record, label="{{ { Name | Description } | { Fertility effects |  } }}"]
  #"Fertility effects" -> "Fertility"
  # 
  # 
  #"Metritis" -> "Cistic Ovarian effects"
  #"Retained Placenta" -> "Cistic Ovarian effects"
  #"Age" -> "Cistic Ovarian effects"
  #"Cistic Ovarian effects"  [ shape=record, label="{{ { Name | Description } | { Cistic Ovarian effects |  } }}"]
  #"Cistic Ovarian effects" -> "Cistic Ovarian Disease"
  # 
  # 
  #"Retained Placenta" -> "Metritis effects"
  #"Metritis effects"  [ shape=record, label="{{ { Name | Description } | { Metritis effects |  } }}"]
  #"Metritis effects" -> "Metritis"
  # 
  # 
  # "Age" -> "Retained Placenta effects"
  #"Retained Placenta effects"  [ shape=record, label="{{ { Name | Description } | { Retained Placenta effects |  } }}"]
  #"Retained Placenta effects" -> "Retained Placenta"
  # 
  # 
  # }
  
  # now DiagrammeR can render this to SVG
  grViz(nodes)
  
  # But I also use graphviz directly to produce a publishable image in
  # pdf or png
  sink("reproduce-donlelek.dot")
  cat(nodes)
  sink()# If graphviz is installed and on linux call it with a shell command
  #system("dot -Tpdf reproduce-donlelek.dot -o reproduce-donlelek.pdf")
  system("dot -Tpng reproduce-donlelek.dot -o reproduce-donlelek.png")
  
#+end_src
** 2015-09-19-my-r-function-for-causal-directed-acyclic-graphs
#+name:my-newnode-r-function-useful-for-causal-directed-acyclic-graphs-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-09-19-my-newnode-r-function-useful-for-causal-directed-acyclic-graphs.md :exports none :eval no :padline no
---
name: my-newnode-r-function-useful-for-causal-directed-acyclic-graphs
layout: post
title: My Newnode R Function Useful For Causal Directed Acyclic Graphs (DAGs)
date: 2015-09-19
categories:
- disentangle
tags:
- diagrams
---
  
# Aims

I have worked on a function that turns a `data.frame` into a graphviz code in the dot language, with some of my preferred settings.  I realised that it might be useful for causal directed acyclic graphs.
  
Causal diagrams are useful for conceptualising the pathways of cause and effect.  These diagrams are sometimes simplly informal pictures but have also been developed in a more formal way to be used in modelling.  These formal developments use concepts derived from the mathmatical abstraction of Graphs (fundamentally Graphs  are networks of linked 'nodes', with the links being termed 'edges').  Causal diagrams can either be constructed to depict two things: first are feedback loops (a vexatious property of complex systems that confounds modelling) while second are more simple chain-of-events type pathways which proceed from an upstream cause to a downstream effect in a single direction, without cycles, called 'Directed Acyclic Graphs or DAGs.  The loop diagrams are out of the scope of this present blog post because the DAGs are much more easily addressed by the tool that I am describing.    
  
To begin I am going to build on this other guy's blog post on causal DAGs with R
  [http://donlelek.github.io/2015/03/31/dags-with-r/](http://donlelek.github.io/2015/03/31/dags-with-r/)
I wanted to add an interface for building these.
  
Some background to the concepts that I use are provided in the references below. 
  
# Materials and Methods
  
The DiagrammeR package which has been integrated within R-Studio has made access to the graphing tool `graphviz` much easier than it used to be.  My function `causal_dag` (avaiable in my `disentangle` github package) essentially constructs the required `nodes` and `edges` for that package to use.  Optionally we can also include `labels` to indicate the direction of the effect.
  
To use the tool all you need to do is create a list of `edges` and their associated `inputs` nodes and `outputs` nodes (as a comma separated values string) shown in the picture below.
  
![causal-ssheet.png](/images/causal-ssheet.png)

#### Code:
    # read in the sheet
    library(disentangle)
    library(stringr)
    causes <- readxl::read_excel("causal-ssheet.xlsx")
    causes
    nodes <- newnode(causes, "edges", "inputs", "outputs")
    cat(nodes)
    # The result is a formated graph in the dot language with some of my
    # preferred settings such as edges showing as 'records' and a spot to
    # write a description or include literature about each process
  
<p></p>

- See the DOT code in the Appendix
- to render the graph now DiagrammeR can use this text string R object to render this to SVG
- I think it does not do PNG or PDF though so I still use graphviz and dot directly

#### Code:
    grViz(nodes)
    
    # But I also use graphviz directly to produce a publishable image in
    # pdf or png
    sink("reproduce-donlelek.dot")
    cat(nodes)
    sink()# If graphviz is installed and on linux call it with a shell command
    #system("dot -Tpdf reproduce-donlelek.dot -o reproduce-donlelek.pdf")
    system("dot -Tpng reproduce-donlelek.dot -o reproduce-donlelek.png")

<p></p>


# Results
  
Here I have reproduced the work of donlelek
    
![reproduce-donlelek.png](/images/reproduce-donlelek.png)

# Future directions

- I'd like to make the edges implicit, so that the spreadsheet keeps track of the information about the causal process, but the graph just shows the lines connecting the nodes
- The edges are where the action is, so I need to add a direction of effect.  This would be in a `label` column and added in a [ label = 'abc' ] tag for each edge
- the rankdir option is LR to make this go sideways, which seems more the norm for causal DAGs, left to right.
  
#### References
    Greenland, S., Pearl, J., & Robins, J. M. (1999). Causal diagrams for
    epidemiologic research. Epidemiology (Cambridge, Mass.), 10(1),
    37–48. doi:10.1097/00001648-199901000-00008
     
    Reid, C. E., Snowden, J. M., Kontgis, C., & Tager, I. B. (2012). The
    role of ambient ozone in epidemiologic studies of heat-related
    mortality. Environmental Health Perspectives, 120(12),
    1627–30. doi:10.1289/ehp.1205251
       
    Newell, B., & Wasson, R. (2001). Social System vs Solar System: Why
    Policy Makers Need History. In: Conflict and Cooperation related to
    International Water Resources : Historical Perspectives. In World
    Water (Vol. 2002).
<p></p>
  
# Appendix

#### Code:
    #####################################################################
    # The following output is automatically created by newnode()
    # NOTE for some reason, to show on the blog, I had to replace all { braces with normal (
    #####################################################################
    digraph transformations (
     
    "Metritis" -> "Fertility effects"
    "Cistic Ovarian Disease" -> "Fertility effects"
    "Age" -> "Fertility effects"
    "Fertility effects"  [ shape=record, label="(( ( Name | Description ) | ( Fertility effects |  ) ))"]
    "Fertility effects" -> "Fertility"
     
     
    "Metritis" -> "Cistic Ovarian effects"
    "Retained Placenta" -> "Cistic Ovarian effects"
    "Age" -> "Cistic Ovarian effects"
    "Cistic Ovarian effects"  [ shape=record, label="(( ( Name | Description ) | ( Cistic Ovarian effects |  ) ))"]
    "Cistic Ovarian effects" -> "Cistic Ovarian Disease"
     
     
    "Retained Placenta" -> "Metritis effects"
    "Metritis effects"  [ shape=record, label="(( ( Name | Description ) | ( Metritis effects |  ) ))"]
    "Metritis effects" -> "Metritis"
     
     
     "Age" -> "Retained Placenta effects"
    "Retained Placenta effects"  [ shape=record, label="(( ( Name | Description ) | ( Retained Placenta effects |  ) ))"]
    "Retained Placenta effects" -> "Retained Placenta"
     
     
     )
  
      
#+end_src

** 2015-09-23-organising-graph-nodes-and-edges-in-a-dataframe
#+name:getting-nodes-and-edges-from-a-dataframe-for-diagrammer-package-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-09-23-organising-graph-nodes-and-edges-in-a-dataframe.md :exports none :eval no :padline no
  ---
  name: organising-graph-nodes-and-edges-in-a-dataframe
  layout: post
  title: Organising Graph Nodes And Edges In A Dataframe
  date: 2015-09-23
  categories:
  - disentangle
  tags:
  - diagrams
  ---
    
  I use the R package `DiagrammeR` for creating graphs (the formal kind, connecting nodes with edges)
  - This package is great and I like how it interacts with the Graphviz program
  - One thing that I like to do in planning and organising data analysis projects is to make graphs and lists of the methods steps, inputs and Outputs
  - A simple way to organise these things is in a dataframe (table) with a column for each step (node) and two others for inputs and outputs (edges)
  - In my utilities R package `github.com/ivanhanigan/disentangle` I have written functions that turn this table into a graphiviz DOT language script
  - Recently I have needed to unpack the list for a more itemized view
  - Both these functions are showcased below

  
  #### Code: newnode
      # First create some test data, each step is a collection of edges 
      # with inputs or outputs simple comma seperated lists
      dat <- read.csv(textConnection('
      cluster ,  step    , inputs                    , outputs                                , description                      
      A  ,  siteIDs      , "GPS, helicopter"          , "spatial, site doco"                 , latitude and longitude of sites  
      A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
      B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
      B  ,  biomass      , spatial                   , biomass_g                              ,                                  
      B  ,  correlations , "exposures,trapped_no,biomass_g" , report1                         , A study we published             
      C  ,  paper1       , report1                   , "open access repository, data package" ,                                  
      D  ,  biomass revision, new estimates          , biomass_g                              , this came late
      '), stringsAsFactors = F, strip.white = T)    
      str(dat)
      # dat
            
      # Now run the function and create a graph
      nodes <- newnode(
        indat = dat,
        names_col = "step",
        in_col = "inputs",
        out_col = "outputs",
        desc_col = "description",
        clusters_col = "cluster",
        nchar_to_snip = 40
        )  
      sink("Transformations.dot")
      cat(nodes)
      sink()
      #DiagrammeR::grViz("Transformations.dot")
      system("dot -Tpng Transformations.dot -o Transformations.png")
      browseURL("Transformations.png")
      
  <p></p>    
  
  That creates this diagram
  
  ![/images/Transformations.png](/images/Transformations.png)
  
  ## Now to showcase the tool that itemizes this list of inputs and outputs
   
  - The original table has no capacity to add detail about each node as they are held as a list of inputs and outputs
  - To add detail for each we need to unpack each list and create a new table with one row per node
  - I decided to make this a long table with an identifier for each node about which step (edge) the node is an input or an output
  
  #### Code: newnode_csv  
      nodes_graphy <- newnode_csv(
        indat = dat,
        names_col = "step",
        in_col = "inputs",
        out_col = "outputs",
        clusters_col = 'cluster'
        ) 
      # which creates this table
      knitr::kable(nodes_graphy)
      |cluster |name             |in_or_out |value                  |
      |:-------|:----------------|:---------|:----------------------|
      |A       |siteIDs          |input     |GPS                    |
      |A       |siteIDs          |input     |helicopter             |
      |A       |siteIDs          |output    |spatial                |
      |A       |siteIDs          |output    |site doco              |
      |A       |weather          |input     |BoM                    |
      |A       |weather          |output    |exposures              |
      |B       |trapped          |input     |spatial                |
      |B       |trapped          |output    |trapped_no             |
      |B       |biomass          |input     |spatial                |
      |B       |biomass          |output    |biomass_g              |
      |B       |correlations     |input     |exposures              |
      |B       |correlations     |input     |trapped_no             |
      |B       |correlations     |input     |biomass_g              |
      |B       |correlations     |output    |report1                |
      |C       |paper1           |input     |report1                |
      |C       |paper1           |output    |open access repository |
      |C       |paper1           |output    |data package           |
      |D       |biomass revision |input     |new estimates          |
      |D       |biomass revision |output    |biomass_g              |
  
  <p></p>
  
  This can now be useful for making a 'shopping list' of the data to aquire, transform, analyse or archive.
    
#+end_src

** 2015-10-03-complexity-of-graphs-obfuscates-but-visual-grouping-helps-disentangle-things
*** COMMENT -code
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name: ####
  library(DiagrammeR)
  #### First create the outcome
  nodes_outcome <- create_nodes(nodes = c('suicide','depression','anxiety'),
                          label = TRUE,
                          colour = "black")
  
  edges_outcome <- create_edges(from = c('depression','anxiety'),
                          to =   c("suicide", "suicide")
                          )
  
  graph_outcome <- create_graph(nodes_df = nodes_outcome,
                         edges_df = edges_outcome)
  # just test this out
  ## render_graph(graph_outcome)
                          
  #### now the social capital factors  
  nodes_social <- 
    create_nodes(nodes =  c("stress", "decreased community support", "migration"),
                 label = TRUE,
                 color = "blue")
  
  edges_social <- create_edges(from =  c("stress", "stress", "decreased community support",
                            "migration"),
                          to =   c("anxiety", "depression", "anxiety",
                            "decreased community support")
                          )
  graph_social <- create_graph(nodes_df = nodes_social,
                         edges_df = edges_social)
  # render_graph(graph_social)
  
  #### now the financial capital factors
  nodes_financial <- 
    create_nodes(nodes = c("employment", "debt"),
                 label = TRUE,
                 color = "green")
  
  edges_financial <- create_edges(from = c("employment", "employment", "debt"),
                          to =   c("stress", "debt", "stress")
                          )
  graph_financial <- create_graph(nodes_df = nodes_financial,
                         edges_df = edges_financial)
  # render_graph(graph_financial)
  
  
  #### now the natural capital factors
  nodes_natural <- 
    create_nodes(nodes = c("drought", "declined agricultural productivity", "decreased food security"),
                 label = TRUE,
                 color = "red")
  
  edges_natural <- create_edges(from = c("drought", "drought", "declined agricultural productivity",
                            "declined agricultural productivity", "declined agricultural productivity",
                            "decreased food security",
                            "drought"),
                          to =   c("declined agricultural productivity", "decreased food security", "decreased food security",
                            "anxiety", "employment",
                            "anxiety",
                            "migration")
                          )
  graph_natural <- create_graph(nodes_df = nodes_natural,
                         edges_df = edges_natural)
  ## render_graph(graph_natural)
  
  # use create_graph on separate nodes and edges data frames, one for each cluster
  # then access the dot codes for each
  gr0 <- graph_outcome$dot_code
  gr1 <- graph_social$dot_code
  gr2 <- graph_financial$dot_code
  gr3 <- graph_natural$dot_code
  # then replace the graph with subgraph
  gr0 <- gsub("digraph", "subgraph cluster0", gr0)
  gr1 <- gsub("digraph", "subgraph cluster1", gr1)
  gr2 <- gsub("digraph", "subgraph cluster2", gr2)
  gr3 <- gsub("digraph", "subgraph cluster3", gr3)
  # and then combine the subgraphs into one graph
  gr_out <- sprintf("digraph{\n%s\n\n %s\n%s\n%s\n}", gr0, gr1, gr2, gr3)
  cat(gr_out)
  grViz(gr_out)
  # If graphviz is installed and on linux call it with a shell command
  sink("suicide-drought.dot")
  cat(gsub("'",'"', gr_out))
  sink()
  system("dot -Tpng suicide-drought.dot -o suicide-drought.png")
  
#+end_src

#+RESULTS:
*** visNetwork
#+name:a
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:a ####
  nodes <- combine_nodes(nodes_outcome, nodes_social, nodes_natural, nodes_financial)
  edges <- combine_edges(edges_outcome, edges_social, edges_natural, edges_financial)
  
  # Render graph
  graph <- create_graph(nodes_df = nodes,
                        edges_df = edges)
  
  render_graph(graph, output = "visNetwork")
    
#+end_src

#+RESULTS: a

*** blog
#+name: complexity-of-graphs-obfuscates-but-visual-grouping-helps-disentangle-things-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-03-complexity-of-graphs-obfuscates-but-visual-grouping-helps-disentangle-things.md :exports none :eval no :padline no
---
name:  complexity-of-graphs-obfuscates-but-visual-grouping-helps-disentangle-things
layout: post
title:  Complexity of Graphs Obfuscates, but Visual Grouping Helps Disentangle Things
date: 2015-10-03
categories:
- disentangle
tags:
- diagrams
---

Using diagrams has long been a technique used in science to describe the relationships (edges) between things (nodes).  Mathematics and geometry tools have been applied to ameliorate the problem of laying out the diagram for the most efficient use of space.  It is desirable to minimise the gaps between the nodes and also to ensure that lines do not overlap too much.  This is because the complexity of graphs obfuscates the details that we are trying to show. Visual grouping helps to disentangle the relationships.

As an example, the relationship between drought and suicide is a complex system
where the effects are indirect. The focus is on a chain
of intermediary causal factors. These questions are usually explored
in the context of many other factors that describe human biological
variables and the socio-economic milieu.

In this post I utilise the R package `DiagrammeR` to construct a causal directed acyclic graph (DAG) of the putative effects of a set of selected causal factors from both natural and social capital theories.

The following code produces graph 
![/images/suicide-drought.png](/images/suicide-drought.png)

Or this interactive version [/viewhtml21f36d8c5d7d/index.html](/viewhtml21f36d8c5d7d/index.html)

<iframe style="border: none;" height="400" width="600" src="/viewhtml21f36d8c5d7d/index.html"></iframe>



```r
library(DiagrammeR)
#### First create the outcome
nodes_outcome <- create_nodes(nodes = c('suicide','depression','anxiety'),
                        label = TRUE,
                        colour = "black")

edges_outcome <- create_edges(from = c('depression','anxiety'),
                        to =   c("suicide", "suicide")
                        )

graph_outcome <- create_graph(nodes_df = nodes_outcome,
                       edges_df = edges_outcome)
# just test this out
## render_graph(graph_outcome)
                        
#### now the social capital factors  
nodes_social <- 
  create_nodes(nodes =  c("stress", "decreased community support", "migration"),
               label = TRUE,
               color = "blue")

edges_social <- create_edges(from =  c("stress", "stress", "decreased community support",
                          "migration"),
                        to =   c("anxiety", "depression", "anxiety",
                          "decreased community support")
                        )
graph_social <- create_graph(nodes_df = nodes_social,
                       edges_df = edges_social)
# render_graph(graph_social)

#### now the financial capital factors
nodes_financial <- 
  create_nodes(nodes = c("employment", "debt"),
               label = TRUE,
               color = "green")

edges_financial <- create_edges(from = c("employment", "employment", "debt"),
                        to =   c("stress", "debt", "stress")
                        )
graph_financial <- create_graph(nodes_df = nodes_financial,
                       edges_df = edges_financial)
# render_graph(graph_financial)


#### now the natural capital factors
nodes_natural <- 
  create_nodes(nodes = c("drought", "declined agricultural productivity", "decreased food security"),
               label = TRUE,
               color = "red")

edges_natural <- create_edges(from = c("drought", "drought", "declined agricultural productivity",
                          "declined agricultural productivity", "declined agricultural productivity",
                          "decreased food security",
                          "drought"),
                        to =   c("declined agricultural productivity", "decreased food security", "decreased food security",
                          "anxiety", "employment",
                          "anxiety",
                          "migration")
                        )
graph_natural <- create_graph(nodes_df = nodes_natural,
                       edges_df = edges_natural)
## render_graph(graph_natural)

# use create_graph on separate nodes and edges data frames, one for each cluster
# then access the dot codes for each
gr0 <- graph_outcome$dot_code
gr1 <- graph_social$dot_code
gr2 <- graph_financial$dot_code
gr3 <- graph_natural$dot_code
# then replace the graph with subgraph
gr0 <- gsub("digraph", "subgraph cluster0", gr0)
gr1 <- gsub("digraph", "subgraph cluster1", gr1)
gr2 <- gsub("digraph", "subgraph cluster2", gr2)
gr3 <- gsub("digraph", "subgraph cluster3", gr3)
# and then combine the subgraphs into one graph
gr_out <- sprintf("digraph{\n%s\n\n %s\n%s\n%s\n}", gr0, gr1, gr2, gr3)
cat(gr_out)
grViz(gr_out)
# If graphviz is installed and on linux call it with a shell command
sink("suicide-drought.dot")
cat(gsub("'",'"', gr_out))
sink()
system("dot -Tpng suicide-drought.dot -o suicide-drought.png")
# Interactive
nodes <- combine_nodes(nodes_outcome, nodes_social, nodes_natural, nodes_financial)
edges <- combine_edges(edges_outcome, edges_social, edges_natural, edges_financial)
  
# Render graph
graph <- create_graph(nodes_df = nodes,
                      edges_df = edges)
  
render_graph(graph, output = "visNetwork")    
```
   
<p></p>




    
#+end_src

** transformations tracker
see the file at [[transformations-tracker.org][transformations-tracker.org]]

* Exploratory Data Analysis
** COMMENT R-csv-fingerprint-code

#+name:R-csv-fingerprint
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:R-csv-fingerprint####
  http://flowingdata.com/2014/08/14/csv-fingerprint-spot-errors-in-your-data-at-a-glance/
  
  OR
  
  >>
  >> Code below goes some way towards doing what you may want...takes a
  >> dataframe containing T/F values for missing or present data, and plots
  >> it as a graphical grid.  Automatically scales depending on the size of
  >> the dataframe - however, it doesn't scale the font size yet, I'll have
  >> to look into that.
  >>
    

#+end_src

** 2015-10-14-a-set-of-guidelines-for-exploratory-data-analysis-and-cleaning
*** copy1
The New York Times ran a piece on August 17, 2014: “For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights” [1].  The article bemoaned the need for too much of what data scientists call “data wrangling”, “data munging” and “data janitor work”.  In essence this means data quality control processes.
The LTERN Data Team performs Exploratory Data Analysis (EDA) to review data deposited for consistency, quality and its compliance with standards to ensure that reusability of data published in the portal is maximised.   To do this, we use relevant data formatting standards (for example the International Standards from ISO), undertake thorough taxonomic reviews of each dataset and have well documented procedures for dealing with miscellaneous errors such as data inconsistencies, duplicate variable names and reformatting numeric or character strings. We sometimes make changes to the data or we make recommendations and ask for approval. 
We follow a set of guidelines for data structure that are recommended [2-5] to make data as re-usable as possible. Here is a list of standard amendments undertaken by the LTERN data portal team:

- identify any out-of-range values (based on the specified units), or questionable data in general;
- rename all files and variables using lower_case_with_underscores naming convention;
- tabulate frequencies and variable distributions, note any outliers for review;
- identify any opportunities to make wide data longer, or many files that can be merged;
- If you have multiple linked tables, each table should include a column that allows those tables to be linked unambiguously (such as the site_ID variables); check that linking variables that link two or more data tables are identical in each table
- check that values in linked files marry up to values in other files (eg a site code in one file that is missing from the spatial data file);
- write as CSV with quote encapsulated strings (for archival purposes);
- code missing data as NA, or identify if these were actually censored;
- coerce dates to ISO 8601 to be in the the YYYY-MM-DD format, or MMM-YYYY;
- cast nominal variables that use integer codes as character;
- check that all value labels in enumerated lists are described (ie codes for “1” = “low”, “2” = “mid” and “3” = “high”);
- attempt to identify and split any combined variables (like season AND year like “winter-97” or species and comments ie “Banksia Dead”);
- review any species lists against current scientific name conventions, recommend any modifications;
- rename any non-conformant species lists (for instance including comments such as Alive/Dead) to “fauna_descriptor” or “flora_descriptor”;
- identify any characters in numeric or date variables and replace with NA, (add to a comments variable if possible);
- identify any values that Excel may try to convert to date type (for eg. site code “1-5” will appear as 5-Jan and should be rewritten as “site_1-5”); 
- use a GIS to confirm spatial coordinates and add geographical coordinates in decimal degrees (GDA94) if only supplied in metres UTM or AMG (always request the datum and the zone);
- check what the coordinates refer to (e.g. approximate location of SW corner of 1ha plot).
- rename files to be consistent with all data in the LTERN Data Portal.  Our standardised names have been created using controlled vocabularies.  Packages and files are designated tracking numbers – this is denoted by a plot network code and a unique “Package” number ascribed to each data package.  Each data package contains one or more data table which is the smallest trackable unit (denoted by a unique “Table” number).
- while we prefer deposit of plain text CSV files, if we receive Excel spreadsheets we check for hidden rows or columns that might not be intended for publication (and  may have been deposited as an oversight).
- it is always best to open Excel workbooks and use the in-built export function to save as CSV files for further re-use.  While R packages and other tools exist to programmatically extract the data from Excel, few tools that interoperate with Excel actually get the all the bug/feature cases right.  It has been noted that
"because working with data that has passed through Excel is hard to get right, data that has passed through Excel is often wrong." [6]

References:
1. Lohr, S. http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0 
2. White, E., Baldridge, E., Brym, Z., Locey, K., McGlinn, D., & Supp, S. (2013). Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution, 6(2), 1–10. doi:10.4033/iee.2013.6b.6.f 
3. Wickham, H. (Under Review). Tidy data. Journal of Statistical Software, VV(Ii).
4. Leek, J. 2014. https://github.com/jtleek/datasharing
5. Borer, E., Seabloom, E., Jones, M., and Schildhauer, M. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America 90:205–214. http://dx.doi.org/10.1890/0012-9623-90.2.205 
5. Campbell, J. L., Rustad, L. E., Porter, J. H., Taylor, J. R., Dereszynski, E. W., Shanley, J. B., Gries, C., Henshaw, D. L., Martin, M. E., Sheldon, W. M., and Boose, E. R. 2013. Quantity is Nothing
without Quality: Automated QA/QC for Streaming Environmental Sensor Data. BioScience, 63,
574-585. http://dx.doi.org/10.1525/bio.2013.63.7.10
6. Mount, J. 2014. Excel spreadsheets are hard to get right. http://www.win-vector.com/blog/2014/11/excel-spreadsheets-are-hard-to-get-right/

*** copy2
4.5
Exploratory Data Analysis
Exploratory Data Analysis and Quality Control
The Data Preparation Guidelines contained in the LTERN Data Deposit Form are provided to Data Provider
Contacts (and/or Nominated Representatives). When the data are received the data analyst makes a series
of checks to ensure the data meet the expectations of data structure and content. To do this, Exploratory
Data Analysis (EDA) and Quality Control (QC) checks of the data deposited are performed. This ensures data
consistency, quality, and compliance with relevant standards to maximise its reusability. For major issues
recommendations are provided and consultation is sought, however minor corrections are made routinely
in accordance with the Data Preparation Guidelines (see Section 16, LTERN Data Deposit Form) as part of
the LTERN Data Portal Team’s workflow. Data owners are informed of all changes. For more information
regarding this work flow see the LTERN Data Publication and Archiving Overview document (see B-1—B-9
of the LTERN Data Portal Project Plan 2014-2015).
Summary of Data Analysis Steps Performed By Data Analyst during EDA
A set of guidelines are followed for data structure which have been recommended [see references 1-5
below] to make data as re-usable as possible. Here is a list of standard amendments undertaken by the
LTERN Data Portal Team:
 Conversion to a range of data formatting standards (for example the International Standards from
   ISO).
 Thorough taxonomic reviews of each dataset.
 Identification of miscellaneous errors such as data inconsistencies and duplicate variable names.
 Reformatting numeric or character strings where appropriate.
 Identify any out-of-range values (based on the specified units), or questionable data in general.
 Rename all files and variables using lower_case_with_underscores naming convention.
 Tabulate frequencies and variable distributions, note any outliers for review.
 Identify any opportunities to make wide data longer, or many files that can be merged.
 If you have multiple linked tables, each table should include a key column that allows those tables
   to be linked unambiguously (such as the site_ID variables).
 Check that values in linked files correspond up to values in related files (e.g. a site_ID in one file
   that is missing from the spatial data file).
E-14
LTERN Data Portal Procedures Manual
 Write as CSV with quote encapsulated strings (for archival purposes).
 Code missing data as NA, or identify if these were actually censored.
 Coerce dates to ISO 8601 to be in the YYYY-MM-DD format, or MMM-YYYY.
 Cast nominal variables that use integer codes as character.
 Check that all value labels in enumerated lists are described (i.e. codes for “1” = “low”, “2” = “mid”
                   and “3” = “high”).
 Attempt to identify and split any combined variables (like season AND year like “winter-97” or
       species and comments i.e. “Banksia Dead”).
 Review any species lists against current scientific name conventions, recommend any
   modifications.
 Rename any non-conformant species lists (for instance including comments such as Alive/Dead) to
   “fauna_descriptor” or “flora_descriptor”.
 Identify any characters in numeric or date variables and replace with NA, (add to a comments
   variable if possible).
 Identify any values that Excel may try to convert to date type (for e.g. Site code “1-5” will appear as
       5-Jan and should be rewritten as “site_1-5”).
 Use a GIS to confirm the distribution of spatial coordinates, and transform geographical
   coordinates in decimal degrees (GDA94) if supplied in metres UTM or AMG (we always request the
  datum and the zone).
 Separate spatial data into a standalone file in a consistent format Latitude as Y coordinate in a
   separate field to Longitude/X coordinates
 Check what the coordinates refer to (e.g. “Approximate location of SW corner of 1ha plot”).
 Rename files to be consistent with all data in the LTERN Data Portal. Our standardised names have
   been created using controlled vocabularies. Packages and files are designated tracking numbers –
    this is denoted by a plot network code and a unique “Package” number ascribed to each data
       package. Each data package contains one or more data tables which constitutes the smallest
      trackable unit (denoted by a unique “Table” number).
 Whilst we prefer deposit of plain text CSV files, if we receive spreadsheets we check for hidden
   rows or columns that might not be intended for publication (and may have been deposited as an
  oversight).
References:
1. White, E., Baldridge, E., Brym, Z., Locey, K., McGlinn, D., & Supp, S. 2013. Nine simple ways to make
it easier to (re)use your data. Ideas in Ecology and Evolution, 6(2), 1–10.
doi:10.4033/iee.2013.6b.6.f
2. Wickham, H., 2014. Tidy Data. Journal of Statistical Software, VV (Ii). Available at:
http://vita.had.co.nz/papers/tidy-data.pdf.
3. Leek, J. 2014. https://github.com/jtleek/datasharing
E-15
LTERN Data Portal Procedures Manual
4. Borer, E., Sea bloom, E., Jones, M., and Schildhauer, M. 2009. Some Simple Guidelines for Effective
Data Management. Bulletin of the Ecological Society of America 90:205–214.
http://dx.doi.org/10.1890/0012-9623-90.2.205
5. Campbell, J. L., Rustad, L. E., Porter, J. H., Taylor, J. R., Dereszynski, E. W., Shanley, J. B., Gries, C.,
Henshaw, D. L., Martin, M. E., Sheldon, W. M., and Boose, E. R. 2013. Quantity is Nothing
without Quality: Automated QA/QC for Streaming Environmental Sensor Data. BioScience, 63,
574-585. http://dx.doi.org/10.1525/bio.2013.63.7.10

*** blog
#+name:a-set-of-guidelines-for-exploratory-data-analysis-and-cleaning-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-14-a-set-of-guidelines-for-exploratory-data-analysis-and-cleaning.md :exports none :eval no :padline no
---
name: a-set-of-guidelines-for-exploratory-data-analysis-and-cleaning
layout: post
title: A Set Of Guidelines For Exploratory Data Analysis And Cleaning
date: 2015-10-14
categories:
- disentangle
tags:
- Exploratory Data Analysis
---


The New York Times ran a piece on August 17, 2014: “For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights” [1].  The article bemoaned the need for too much of what data scientists call “data wrangling”, “data munging” and “data janitor work”.  In essence this means data quality control processes.
A key task of my role as data manager/analyst is to perform Exploratory Data Analysis (EDA) to review data deposited for consistency, quality and its compliance with standards to ensure that reusability of data published in the portal is maximised.   To do this, I use relevant data formatting standards (for example the International Standards from ISO), undertake thorough taxonomic reviews of each dataset and have well documented procedures for dealing with miscellaneous errors such as data inconsistencies, duplicate variable names and reformatting numeric or character strings. I sometimes make changes to the data and give no further thoughts to it, but at  other times I need to make recommendations to the data provider and ask for their decisions/approvals on what to change.
 
I have put down this set of guidelines for my procedures to create standardised data structures, based on things that have been recommended in the literature [2-6] to make data as re-usable as possible. Here is a list of standard amendments undertaken during my EDA process:

- identify any out-of-range values (based on the specified units), or questionable data in general;
- rename all files and variables using lower_case_with_underscores naming convention;
- tabulate frequencies and variable distributions, note any outliers for review;
- identify any opportunities to make wide data longer, or many files that can be merged;
- If you have multiple linked tables, each table should include a column that allows those tables to be linked unambiguously (such as the site_ID variables); check that linking variables that link two or more data tables are identical in each table
- check that values in linked files marry up to values in other files (eg a site code in one file that is missing from the spatial data file);
- write as CSV with quote encapsulated strings (for archival purposes);
- code missing data as NA, or identify if these were actually censored;
- coerce dates to ISO 8601 to be in the the YYYY-MM-DD format, or MMM-YYYY;
- cast nominal variables that use integer codes as character;
- check that all value labels in enumerated lists are described (ie codes for “1” = “low”, “2” = “mid” and “3” = “high”);
- attempt to identify and split any combined variables (like season AND year like “winter-97” or species and comments ie “Banksia Dead”);
- review any species lists against current scientific name conventions, recommend any modifications;
- rename any non-conformant species lists (for instance including comments such as Alive/Dead) to “fauna_descriptor” or “flora_descriptor”;
- identify any characters in numeric or date variables and replace with NA, (add to a comments variable if possible);
- identify any values that Excel may try to convert to date type (for eg. site code “1-5” will appear as 5-Jan and should be rewritten as “site_1-5”); 
- use a GIS to confirm spatial coordinates and add geographical coordinates in decimal degrees (GDA94) if only supplied in metres UTM or AMG (always request the datum and the zone);
- check what the coordinates refer to (e.g. approximate location of SW corner of 1ha plot).
- rename files to be consistent with all data in the LTERN Data Portal.  Our standardised names have been created using controlled vocabularies.  Packages and files are designated tracking numbers – this is denoted by a plot network code and a unique “Package” number ascribed to each data package.  Each data package contains one or more data table which is the smallest trackable unit (denoted by a unique “Table” number).
- while we prefer deposit of plain text CSV files, if we receive Excel spreadsheets we check for hidden rows or columns that might not be intended for publication (and  may have been deposited as an oversight).
- it is always best to open Excel workbooks and use the in-built export function to save as CSV files for further re-use.  While R packages and other tools exist to programmatically extract the data from Excel, few tools that interoperate with Excel actually get the all the bug/feature cases right.  It has been noted that
"because working with data that has passed through Excel is hard to get right, data that has passed through Excel is often wrong." [7]

## References:

1. Lohr, S. [http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0](http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0)
2. White, E., Baldridge, E., Brym, Z., Locey, K., McGlinn, D., & Supp, S. (2013). Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution, 6(2), 1–10. [http://dx.doi.org/10.4033/iee.2013.6b.6.f](http://dx.doi.org/10.4033/iee.2013.6b.6.f)
3. Wickham, H. (Under Review). Tidy data. Journal of Statistical Software, VV(Ii).
4. Leek, J. 2014. [https://github.com/jtleek/datasharing](https://github.com/jtleek/datasharing)
5. Borer, E., Seabloom, E., Jones, M., and Schildhauer, M. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America 90:205–214. [http://dx.doi.org/10.1890/0012-9623-90.2.205](http://dx.doi.org/10.1890/0012-9623-90.2.205)
5. Campbell, J. L., Rustad, L. E., Porter, J. H., Taylor, J. R., Dereszynski, E. W., Shanley, J. B., Gries, C., Henshaw, D. L., Martin, M. E., Sheldon, W. M., and Boose, E. R. 2013. Quantity is Nothing
without Quality: Automated QA/QC for Streaming Environmental Sensor Data. BioScience, 63,
574-585. [http://dx.doi.org/10.1525/bio.2013.63.7.10](http://dx.doi.org/10.1525/bio.2013.63.7.10)
6. Mount, J. 2014. Excel spreadsheets are hard to get right. [http://www.win-vector.com/blog/2014/11/excel-spreadsheets-are-hard-to-get-right/](http://www.win-vector.com/blog/2014/11/excel-spreadsheets-are-hard-to-get-right/)
    
#+end_src
** TODO visualising survey data,  communicating uncertainty and change in Surveys
http://rud.is/b/2015/11/08/visualizing_survey_data/
https://gist.github.com/hrbrmstr/f7ab55e7ae1fc8569df3
see visu surv data zip and unpacked
* Data Operation
** R-spatial
*** TODO xy2shp
#+name:xy2shp
#+begin_src R :session *R* :tangle no :exports none :eval no
  # func
  
  if(!require(ggmap)) install.packages('ggmap'); require(ggmap)
  if (!require(rgdal)) install.packages('rgdal'); require(rgdal)
  epsg <- make_EPSG()
  # load
  latlong <- read.table(tc <- textConnection(
  "ID  POINT_Y   POINT_X
  1  150.5556 -35.09305
  2  150.6851 -35.01535
  3  150.6710 -35.06412
  4  150.6534 -35.08666
  "), header = TRUE); close(tc)
  # do
  for(i in 1:nrow(latlong)){
    coords <- as.numeric(latlong[i,c('POINT_Y', 'POINT_X')])
    e <- as.data.frame(cbind(i, t(coords), revgeocode(coords)))
    write.table(e, "test.csv", sep = ',', append = i > 1, col.names = i == 1, row.names = F)
  }
  d <- read.csv('test.csv')
  head(d)
  ## Treat data frame as spatial points
  pts <- SpatialPointsDataFrame(cbind(d$V2,d$V3),d,
    proj4string=CRS(epsg$prj4[epsg$code %in% '4283']))
  writeOGR(pts, 'test.shp', 'test', driver='ESRI Shapefile')
  
#+end_src

*** COMMENT sptransform-code
#+name:sptransform
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:sptransform####
  require(disentangle)
  require(rgdal)
  epsg <- make_EPSG()
  # load
  d <- read.table(tc <- textConnection(
  "ID  POINT_Y   POINT_X
  1  150.5556 -35.09305
  2  150.6851 -35.01535
  3  150.6710 -35.06412
  4  150.6534 -35.08666
  "), header = TRUE); close(tc)
  # do
  str(epsg)
  epsg[grep("UTM", epsg$note),"note"]
  ## Treat data frame as spatial points
  pts <- SpatialPointsDataFrame(cbind(d$V2,d$V3),d,
  proj4string=CRS(epsg$prj4[epsg$code %in% '4283']))
    # this alternative was from a website  #CRS("+init=epsg:4283"))
  # defin inputs
  datadir <- "data_provided/pr_bg_data_20140612/Data_tables/site_data/"
  flist<-dir(datadir)
  #flist
  #change this
  fname <- "Site_data.txt"
  
  # define outputs
  outdir <- "ljbb_plot_details"
  outfile <- "ljbb_plot_details.shp"
  
  # load
  dat <- read.csv(
    file.path(datadir, fname)
    , header=TRUE)
  
  # clean
  dat$Date.of.last.fire <-  as.Date(dat$Date.of.last.fire, "%d/%m/%Y")
  #head(dat)
  names(dat) <- tolower(gsub("\\.", "_", names(dat)))
  # to match up with the site_number from the spotlighting data
  dat$site_number <- paste("site_", dat$site_number, sep = "")
  
  # TODO check coordinate system code at spatialreference.org
  # EPSG:20256: AGD66 / AMG zone 56
  srid <- CRS(epsg$prj4[epsg$code %in% '20256'])
  #srid
  pts <- SpatialPointsDataFrame(cbind(dat$easting,dat$northing),dat,
    proj4string=srid)
  setwd(outdir)
  writeOGR(pts, dsn = outfile, layer = gsub(".shp","", outfile), driver = "ESRI Shapefile")
  setwd("..")
  
  #### and the csv
  write.csv(pts@data, file.path(outdir, gsub(".shp", ".csv", outfile)), row.names = F)
  
  #### Bounding Box in lat/long ####
  # work out the lat longs
  latlong  <-  spTransform(pts, CRS("+init=epsg:4283"))
  #summary(latlong@data[,c("easting", "northing")])
  #str(latlong)
  
  bb <- morpho_bounding_box(latlong)
  print(xtable(bb), type = "html")
    
#+end_src
*** 2015-10-16-gis-issues-when-r-used-transforming-coordinate-systems
#+name:gis-issues-when-r-used-transforming-coordinate-systems-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-16-gis-issues-when-r-used-transforming-coordinate-systems.md :exports none :eval no :padline no
---
name: gis-issues-when-r-used-transforming-coordinate-systems
layout: post
title: GIS Issues when R is Used for Transforming Coordinate Systems
date: 2015-10-16
categories:
- Data Operation
- spatial
---

- I have been using RGDAL to transform and write out spatial data in GDA94
- It is a problem to know what I need to do to create the right prj file for ArcGIS to read without complaining     
- I have an example of code below, that I used on a dataset I knew was in GDA94 when I read it in.  I want to do this mostly for when I have to convert from one to another, but I have done the manual hack a couple of times now and thought I better just check with an expert.

#### R Code:
    infile <- "ap_map"
    outfile <- gsub("map", "map_GDA94.shp", infile)
    outfile
    setwd(indir)
    shp <- readOGR(".", infile)
    setwd(projdir)
    plot(shp, add = T)
    #str(shp)
    shp@proj4string@projargs
    #[1] "+proj=longlat +ellps=GRS80 +no_defs"
    # confirm this is GDA94
    epsg <- make_EPSG()
    str(epsg)
    epsg[grep(4283, epsg$code),]
    #     code    note                                                       prj4
    # 212 4283 # GDA94   +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs
    # Arcmap sees this as GRS 1980(IUGG, 1980) which I think is the same thing
    # to be on the safe side I will force it to refer to GDA94
    shp2 <- spTransform(shp, CRS("+init=epsg:4283"))
    shp2@proj4string@projargs
    #now write
    setwd(outdir)
    dir()
    writeOGR(shp2,  outfile, gsub(".shp", "", outfile), driver = "ESRI Shapefile")
    setwd(projdir)
    # Checking this shows it did not write the correct prj file, but I believe that this is because the GDA94 definition is not different to the WGS80 params.
    # one other option is to manually replace that prj file with the correct text found at http://spatialreference.org/ref/epsg/4283/
    # SO I did this to avoid any future confusions
    # OLD GEOGCS["GRS 1980(IUGG, 1980)",DATUM["D_unknown",SPHEROID["GRS80",6378137,298.257222101]],PRIMEM["Greenwich",0],UNIT["Degree",0.017453292519943295]]
    # NEW GEOGCS["GDA94",DATUM["D_GDA_1994",SPHEROID["GRS_1980",6378137,298.257222101]],PRIMEM["Greenwich",0],UNIT["Degree",0.017453292519943295]]
    # checked with ArcMap and they align ok

<p></p>

## I asked on of my colleagues and here is his reply

- I usually use the sp and maptools library, rather than the readOGR and writeOGR functions.  Generally my workflow, as an example of projecting a GDA94 zone 55 file to WGS84, would be something like:

#### R Code:
    shp <- readShapePoly("myfile.shp")
    proj4string(shp) <- "+proj=utm +zone=55 +south +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
    shp2 <- spTransform(shp, CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs "))
    writePolyShape(shp2,"myfile_p.shp",proj4string=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs "))
<p></p>

- I've never *noticed* any problems doing this, the projection file is generated and I've never noticed any alignment problems in ArcMap.  It's true that I recall ArcMap sometimes displays an incorrect plaintext projection description, but things seem to be in the right place.

- Also, I don't really ever rely on EPSG numbers in R - I just grab the proj4 strings and try to use them directly. 

## TODO, see if PostGIS handles this OK

-  I wonder if my FOSS GIS stack should still pass things through PostGIS as I suspect it does these things better.  
- I might check the output if I run it through the DB use st_transform and then extract to shapefile.

    
#+end_src


*** TODO gIntersection
#+name:gIntersection
#+begin_src R :session *shell* :tangle no :exports none :eval no
  #### name:gIntersection ####
  #?gIntersection
  subset(ma_grid10@data, Id == 125)
  str(mash_ch@data)
  mash_ch2 <- gUnaryUnion(mash_ch)
  str(mash_ch2@data)
  summary(mash_ch2)
  int <- gIntersection(ma_grid10, mash_ch2, byid = T, id = as.character(ma_grid10@data$Id))
  plot(int)
  summary(int)
  str(int)
  n<-names(int)
  ## n  <- as.character(n)
  ## n
  ## n<-data.frame(t(data.frame(
  ##   strsplit(n," ",fixed=TRUE)
  ##   )))
  n <- as.data.frame(n)
  head(n)
  tail(n)
  row.names(n) <- NULL
  colnames(n) <- "Id"
  #[1:2]<-c("Id","chull")
  
  
  n$area<-sapply(int@polygons, function(x) x@area)
  summary(n)
  head(n)
  str(n)
  subset(n, Id == 125)
  #a<- sapply(slot(ma_grid10, "polygons"), function(x) sapply(slot(x, "Polygons"), slot, "area"))
  #str(a)
  
  #df<-merge(n,a,all.x=TRUE)
  #head(df)
  #df$share.area<-df$area/df$total.area*100
  #subset(df, grid == 100)
  #table(df$grid)
  ## str(ma_grid10@data)
  ## summary(ma_grid10@data)
  ## df <- n
  ## df$Id <- as.numeric(as.character(df$Id))
  ## summary(df)
  #ma_grid10 <- ma_grid10
  #by <- "Id"
  # http://stackoverflow.com/questions/3650636/how-to-attach-a-simple-data-frame-to-a-spatialpolygondataframe-in-r
  ma_grid10@data = data.frame(ma_grid10@data, n[match(ma_grid10@data[,"Id"], n[,"Id"]),])
  head(ma_grid10@data)
  tail(ma_grid10@data)
  summary(ma_grid10)
  plot(ma_grid10, col = ma_grid10$area)
  #plot(chull, add = T)
  #setwd("data")
  writeOGR(ma_grid10, "temp20.shp", "temp20", driver = "ESRI Shapefile")
  crs <- ma_grid10@proj4string
  spp <- SpatialPolygonsDataFrame(int,data=as.data.frame(n[,1]),match.ID=F)
  writeOGR(spp, "temp3.shp", "temp3", driver = "ESRI Shapefile")
  
  setwd("..")
  
#+end_src

*** COMMENT shapefile_attributes_join-code
#+name:shapefile_attributes_join
#+begin_src R :session *shell* :tangle no :exports none :eval no
#### name:shapefile_attributes_join ####
see ramnath

sp@data = data.frame(sp@data, df[match(sp@data[,by], df[,by]),])

#+end_src
*** COMMENT shapefile_colour_ramp-code
#+name:shapefile_colour_ramp
#+begin_src R :session *shell* :tangle no :exports none :eval no
#### name:shapefile_colour_ramp ####
  # If we wanted to do a colour ramp 
  ## cls <- -.9 +  (occupancy_grids@data$area.1/(10000*10000))
  ## cls <- abs(ifelse(is.na(cls), -1, cls))
  ## cls <- gray(cls)
  # but we don't, this time just in or out
  cls<-occupancy_grids@data$area/(10000*10000)
  cls
  cls <- ifelse(cls >= 0.01, "lightgrey", "darkgrey")
  cls <- ifelse(is.na(cls), 'white', cls)
  cls
  png("results/B2_occupancy.png")
  plot(occupancy_grids, col = cls)
  plot(chull, add = T)
  plot(ecosystem_current, add = T, border = T, col = 'darkgrey')
  dev.off()

OR
http://www.geog.uoregon.edu/GeogR/examples/maps_examples01.htm
library(RColorBrewer) # creates nice color schemes
library(classInt)     # finds class intervals for continuous variables

 # equal-frequency class intervals
plotvar <- orcounty.shp@data$POP1990
nclr <- 8
plotclr <- brewer.pal(nclr,"BuPu")
class <- classIntervals(plotvar, nclr, style="quantile")
colcode <- findColours(class, plotclr)

plot(orcounty.shp, xlim=c(-124.5, -115), ylim=c(42,47))
plot(orcounty.shp, col=colcode, add=T)
title(main="Population 1990",
    sub="Quantile (Equal-Frequency) Class Intervals")
legend(-117, 44, legend=names(attr(colcode, "table")),
    fill=attr(colcode, "palette"), cex=0.6, bty="n")

#+end_src

*** COMMENT convert_dms-code
#+name:convert_dms
#+begin_src R :session *shell* :tangle R/convert_dms.R :exports none :eval no
  #### name:convert_dms ####
  ## First need to convert character degree min sec data to decimal degree
  ##### I initially investigated functions in other packages and found
  # DMS2DD for PostGIS but this seems defunct
  # http://gis.stackexchange.com/a/84626
  #### Then I found in R
  # library(sp)
  # ?char2dms
  # but this would require extra processing of the input data
  
  #### In the end the format of the supplied file suits this function
  # http://stackoverflow.com/a/18999925
  convert_dms <- function(coord){
    tmp1=strsplit(coord,"°")
    tmp2=strsplit(tmp1[[1]][2],"'")
    tmp3=strsplit(tmp2[[1]][2],"\"")
    dec=c(as.numeric(tmp1[[1]][1]),as.numeric(tmp2[[1]][1]),as.numeric(tmp3[[1]]))
    c<-abs(dec[1])+dec[2]/60+dec[3]/3600
    c<-ifelse(dec[1]<0,-c,c)
    return(c)
  }

#+end_src

*** big_pt
**** R-big_pt_intersect
#+name:big_pt_intersect
#+begin_src R :session *R* :tangle R/big_pt_intersect.r :exports none :eval no
  #' @title big points intersect
  #' @name big_pt_intersect
  #' @param pts a points spatial
  #' @param ply a polygons spatial
  #' @param chunks how many to split into
  #' @return a combined data frame that avoids run out of memory with big points file 
  big_pt_intersect <- function(pts, ply, chunks = 100){
    idx <- split(pts@data, 1:chunks)
    #str(idx)
    for(i in 1:length(idx)){
    #i = 1
    print(i)
      ids <- idx[[i]][,1]
    #str(pts@data)
    qc <- pts[pts@data[,1] %in% ids,]
    #str(qc)
    tryCatch(chunk <-  raster::intersect(qc, ply), error = function(err){print(err)})
    if(!exists('chunk_out')){
    
      chunk_out <- chunk@data
    } else {
      chunk_out <- rbind(chunk_out, chunk@data)
    }
    rm(chunk)
    
    }
    #str(chunk_out)
    return(chunk_out)
  }
  # NB warning about split length multiple is not fatal, just due to nonequal chunks (ie the geocodes are 2009/100)
  
#+end_src
**** test-big_pt_intersect
#+name:big_pt_intersect
#+begin_src R :session *R* :tangle no :exports none :eval no
################################################################
# name:big_pt_intersect

#+end_src
*** 2015-09-17-if-you-dont-find-a-solution-in-r-keep-googling
#+name:if-you-dont-find-a-solution-in-r-keep-googling-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-09-17-if-you-dont-find-a-solution-in-r-keep-googling.md :exports none :eval no :padline no
  ---
  name: if-you-dont-find-a-solution-in-r-keep-googling
  layout: post
  title: If You Don't Find A Solution In R, Keep Googling!
  date: 2015-09-17
  categories:
  - disentangle
  tags:
  - spatial
  ---
  
  I've learnt this lesson multiple times. It happens like this.  A solution is not immediately obvious in R so you might think of writing your own function.  Generally there is a solution you just did not google enough.
  This time I was tricked a little because the GIS functions have been bad for a long time but getting better very rapidly recently.  A little while ago I had a very successful
  outcome from using the `raster::extract` function on a large raster file
  to get the attributes for a set of points.  I needed to do the same
  thing but this time for a shapefile and points.  I looked at the
  raster package and saw you can use the `raster::intersect` function
  here, and it worked on the small sample data I tested with but failed
  with the big dataset as it ran out of memory.  I assumed that R had not caught up with the GIS world yet and so I came up with this workaround below by splitting the points data layer into chunks.
  
  I then got access to ArcMap and was wondering whether it could do it, and it DID!
  So then I googled a bit and found the solution was simple:
  
  #### Code:
      sp::over()
  
  <p></p>
  
  Here is my hack in case I ever need to pull out the bit that does the splitting up of the points file, or the tryCatch():
  
  #### Code:
      big_pt_intersect <- function(pts, ply, chunks = 100){
        idx <- split(pts@data, 1:chunks)
        #str(idx)
        for(i in 1:length(idx)){
        #i = 1
        print(i)
          ids <- idx[ [i] ][,1]
        #str(pts@data)
        qc <- pts[pts@data[,1] %in% ids,]
        #str(qc)
        tryCatch(
          chunk <-  raster::intersect(qc, ply), 
          error = function(err){print(err)})
        if(!exists('chunk_out')){
        
          chunk_out <- chunk@data
        } else {
          chunk_out <- rbind(chunk_out, chunk@data)
        }
        rm(chunk)
        
        }
        #str(chunk_out)
        return(chunk_out)
      }
      # NB warning about split length multiple is not fatal, just due to nonequal chunks 
      # (ie the geocodes are 2009/100)

  
  
  <p></p>
  
  
#+end_src

*** COMMENT 2015-11-05-kriging
#+name:2015-11-05-kriging
#+begin_src R :session *R* :tangle 2015-11-05-kriging.R :exports none :eval no
#### name:2015-11-05-kriging ####
http://www4.stat.ncsu.edu/~reich/CUSP/Ordinary_Kriging_in_R.pdf
http://r-video-tutorial.blogspot.com.au/2015/08/spatio-temporal-kriging-in-r.html
uses gstat

#+end_src

** 2015-10-28-use-google-geocoding-api-from-r
~/projects/ivanhanigan.github.com.raw/_posts/2015-10-28-use-google-geocoding-api-from-r.md
#+name:use-google-geocoding-api-from-r-header
#+begin_src markdown :tangle no :exports none :eval no :padline no
  ---
name: use-google-geocoding-api-from-r
  layout: post
title: Use google geocoding API from R
  date: 2015-10-28
categories:
  - disentangle
- Data Operation
  - R-spatial
---
  
- Google have provided a geocoding API
  - Using it from R is a cinch
- Use of this means you agree to their terms which for my purposes pretty much means I
  

      
#+end_src

** COMMENT aggregate-list-of-dfs
#+name:aggregate
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:aggregate####
  # stoopid hack for lu
  # desc
  "I have a do.call question that perhaps you can answer in 5 seconds... [my brain is leaking... I can't see the solution].
  
  I have a list of data.frames imagine I wanted to sum them up, like d1+d2+... N. If I do that manually the output is a data.frame, but when I use do.call I get one number, and I need a data.frame instead.
  
  Another issues is that some columns have a factor, so I'm planning to wrap my operation into a function with a condition to do the sum if the cell contains a number. Other to skip it.
  so I though of:
  
  foo = function (x) { ifelse(is.numeric(x), sum(x), NA)} #but it doesn't work
  
  This example addressed the issues.
  
  d1 = mtcars
  d2 = d1*2
  
  str(mtcars)
  l1 = list(d1,d2)
  str(l1)
  
  d3 = do.call('sum', l1)
  #> d3
  #[1] 41826.61 # I don't want one number, but a data.frame with the sums.
  
  foo = function (x) { ifelse(is.numeric(x), sum(x), NA)}
  d4 = do.call('foo', l1)
  
  Any help or tip will be welcome!"
  
  # I feel like it should be simple because R can add dataframes
  # but maybe these need to be matrices (because a data.frame is really
  # a list)
  d1 = as.matrix(mtcars)
  d1
  d2 = as.matrix(d1*2)
  d2
  d1 + d2
  # lets make more dfs
  d3 = as.matrix(d2*3)
  d4 = as.matrix(d3*2)
  d1+d2+d3+d4
  # I note that do.call can add two df fine
  l1  <- list(d1, d2)
  out  <- do.call("+", l1)
  str(out)
  out
  # but with multiple no good
  l1 = list(d1,d2, d3, d4)
  str(l1)
  out  <- do.call("+", l1)
  # when these are dataframes the error is
  ## Error in `+`(list(mpg = c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4,  :
  ##   unused arguments (list(mpg = c(... blah blah
  # when matrices the error is
  ## Error in `+`(c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2,  :
  ##   operator needs one or two arguments
  
  
  # But the lists content are able to be combined if done individually
  out1 <- l1[[1]] + l1[[2]] + l1[[3]] + l1[[4]]
  str(out1)
  out1
  
  # make things interesting
  l1 <- list()
  for(i in 1:1000){
    #i = 1
    di  <- iris
    di[sapply(iris, is.numeric)]  <- iris[sapply(iris, is.numeric)] * sample(rnorm(100,1,2), 1)
    l1[[i]]  <- di
  }
  str(l1)
  # so we could figure out a way to iterate over the dataframes with a
  # loop like
  ## for(i in 1:length(l1)){
  ##   if(i == 1){
  ##     out2 <- l1[[i]]
  ##     next
  ##   } else {
  ##     out2  <- out2 + l1[[i]]
  ##   }
  ## }
  # doesn't like factor
  #out2
  # or 
  ## d1 <- iris
  ## str(d1) # Note pesky factor column
  ## d2 <- d1
  ## # Only multiply cells that are numeric
  ## str(d2[sapply(d2, is.numeric)])
  ## d2[sapply(d2, is.numeric)] <- d2[sapply(d2, is.numeric)] * 2
  
  ## # Add another data.frame to make it interesting
  ## d3 <- d1
  ## d3[sapply(d3, is.numeric)] <- d3[sapply(d3, is.numeric)] * 3
  
  
  ## l1 = list(d1,d2, d3)
  str(l1)
  
  # Inititalise output dataframe to zero
  
  # Sum data frames
  summarise_list_dfs3 <- function(listed){
    d.results <- d1
    d.results[sapply(d.results, is.numeric)] <- 0 
    for (i in seq_along(l1)){
      d.results[sapply(d.results, is.numeric)] <-
        d.results[sapply(d.results, is.numeric)] +
        l1[[i]][sapply(l1[[i]], is.numeric)]
    }
    return(d.results)
  }
  system.time(d.results  <- summarise_list_dfs3(l1))
  str(d.results)
  
  # but we want an elegant solution that will be
  # able to give any number of dataframes, and also have
  # the issue of some variables being factor so not to be used
  
  # let's write some func
  # I like SQL for it's clarity
  library(sqldf)
  # we are basically grouping the values of each row in each df, so add
  # an id
  
  nam <- function(x){
    x$row_names <- 1:nrow(x)
    return(x)
  }
  l_df2 <- lapply(l1, nam)
  str(l_df2)
  # now construct some sql and run it.  let's make it flexible for
  # different summarising functions like sum, mean, median, stdev etc
  summarise_list_of_dfs <- function(
    list_of_dfs = l1
    ,
    summarise_fun = 'sum'
    ,
    id = 'row_names'
    ){
    if(!is.data.frame(list_of_dfs[[1]])) list_of_dfs <- lapply(list_of_dfs, as.data.frame)
    l_df2 <- lapply(list_of_dfs, nam)
    x = do.call('rbind.data.frame', l_df2)
  names(x)<-gsub("\\.", "_", names(x))
    todo <- sapply(x, 'is.numeric')
    todo <- names(x)[todo]
    todo <- todo[-which(todo == "row_names")]
    oper  <- sprintf('), %s(',summarise_fun)
    sql<-paste("sum(",paste(todo, sep = '', collapse = oper),")", sep = "")
  #sql
    out<-sqldf(
    #cat(
    sprintf("select row_names, %s
    from x
    group by row_names
    order by row_names
    ", sql)
    )
    names(x)<-gsub("_", "\\.", names(x))
    return(out)
  }
  
  # do
  #l1 <- lapply(l1, as.data.frame)
  
  system.time(qc  <- summarise_list_of_dfs(l1))
  
  str(qc)
  str(out1)
  out1  <- as.data.frame(out1)
  names(qc)<-names(out1)
  # same?
  identical(qc, out1)
  # not identical
  all(qc == out1)
  # all values are equal tho
  
  # compare to base r
  summarise_list_dfs2 <- function(listed){
    listed  <- l1
    d5=do.call('rbind',listed)
  #  str(d5)
    d5.rnames =rep(rownames(listed[[1]]),times=length(listed))
    d5$reg=d5.rnames
    d6 = aggregate(d5[sapply(d5, is.numeric)], by=list(d5$reg), FUN=sum) # I should use an
                                            # ifelse, to do this only to
                                            # numeric columns.
    return(d6)
  }
  system.time(d6 <- summarise_list_dfs2(l1))
  
  str(d6)
  row.names(d6) <- d6$Group.1
  # same?
  identical(d6, out1[sort(row.names(out1)),])
  # not identical
  all(
    d6[,-c(1)] == out1[sort(row.names(out1)),]
    )
  # looks like the integers are equal but not the doubles.
  # but a diff of the two looks same
  
  # now does it work for  mean?
  qc  <- summarise_list_of_dfs(l1, 'mean')
  # whoops, sql uses 'avg'
  qc  <- summarise_list_of_dfs(l1, 'avg')
  str(qc)
  # and stdev
  qc  <- summarise_list_of_dfs(l1, 'stdev')
  str(qc)
  # this did multiple dataframes, now try with factor variables
  str(l1)
  d1[,3] <- as.factor(d1[,3])
  d2[,3] <- as.factor(d2[,3])
  d3[,3] <- as.factor(d3[,3])
  d4[,3] <- as.factor(d4[,3])
  
  l1 = list(d1,d2, d3, d4)
  str(l1)
  qc  <- summarise_list_of_dfs(l1)
  str(qc)
  # it has just skipped that variable so this should be good to go
  # NB if the combination of all the dataframes is too big for RAM then
  # R will fail. in this case I would
  # put into a PostgreSQL database as this will use the disk rather than
  # RAM
  # this can also be used to add indexes and clustering functions to
  # speed up the calculations.
  # HTH, let me know if there is a more efficient R solution?
  
  install.packages("data.table")
  library(data.table)
  sum_ldf4 <- function(
    listed=l1
    ,
    summarise_fun  = 'sum'
                       ){
    d5=do.call('rbind',listed)
    d5.rnames =rep(rownames(listed[[1]]),times=length(listed))
    d5 <- d5[sapply(d5, is.numeric)]
    d5$reg=d5.rnames
    oper  <- sprintf('), %s(',summarise_fun)
    todo <- sapply(d5, 'is.numeric')
    todo <- names(d5)[todo] 
    sql<-paste("sum(",paste(todo, sep = '', collapse = oper),")", sep = "")
    #sql
      
    DT = data.table(d5)
    
    out <-  eval(parse(
        text = sprintf("DT[,list(%s),by=list(d5$reg)]", sql)
        ))
      
  
    return(out)
  }
  
  
  # sqldf
  o1 <- system.time(qc  <- summarise_list_of_dfs(l1))
  str(qc)
  # aggregate
  o2 <- system.time(d6 <- summarise_list_dfs2(l1))
  str(d6)
  # loop
  o3 <- system.time(d.results  <- summarise_list_dfs3(l1))
  str(d.results)
  # data.table
  o4 <- system.time(datab  <- sum_ldf4(l1))
  str(datab)
  
  # http://zvfak.blogspot.com.au/2011/03/applying-functions-on-groups-sqldf-plyr.html
  library(gplots)
  x<-c(o1[3],o2[3],o3[3],o4[3])
  balloonplot( rep("time.elapsed",5),c("sqldf","aggregate","loop", "data.table"),round(x,1), ylab ="Method", xlab="",sorted=F,dotcolor=rev(heat.colors(5)),main="time.elapsed for different methods") 
#+end_src

** COMMENT aggregate-list-of-dfs-asRNW-code
#+name:aggregate-list-of-dfs-asRNW
#+begin_src R :session *R* :tangle no :exports none :eval no
#### name:aggregate-list-of-dfs-asRNW####
---
title: "aggregate list of dfs"
author: "Ivan C Hanigan"
date: "7/2/2015"
output: html_document
---

```{r}
#### name:aggregate####
# stoopid hack for lu
# desc
"I have a do.call question that perhaps you can answer in 5 seconds... [my brain is leaking... I can't see the solution].

I have a list of data.frames imagine I wanted to sum them up, like d1+d2+... N. If I do that manually the output is a data.frame, but when I use do.call I get one number, and I need a data.frame instead.

Another issues is that some columns have a factor, so I'm planning to wrap my operation into a function with a condition to do the sum if the cell contains a number. Other to skip it.
so I though of:

foo = function (x) { ifelse(is.numeric(x), sum(x), NA)} #but it doesn't work

This example addressed the issues.

d1 = mtcars
d2 = d1*2

str(mtcars)
l1 = list(d1,d2)
str(l1)

d3 = do.call('sum', l1)
#> d3
#[1] 41826.61 # I don't want one number, but a data.frame with the sums.

foo = function (x) { ifelse(is.numeric(x), sum(x), NA)}
d4 = do.call('foo', l1)

Any help or tip will be welcome!"

# I feel like it should be simple because R can add dataframes
# but maybe these need to be matrices (because a data.frame is really
# a list)
d1 = as.matrix(mtcars)
d1
d2 = as.matrix(d1*2)
d2
d1 + d2
# lets make more dfs
d3 = as.matrix(d2*3)
d4 = as.matrix(d3*2)
d1+d2+d3+d4
# I note that do.call can add two df fine
l1  <- list(d1, d2)
out  <- do.call("+", l1)
str(out)
out
# but with multiple no good
l1 = list(d1,d2, d3, d4)
str(l1)
#out  <- do.call("+", l1)
# when these are dataframes the error is
## Error in `+`(list(mpg = c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4,  :
##   unused arguments (list(mpg = c(... blah blah
# when matrices the error is
## Error in `+`(c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2,  :
##   operator needs one or two arguments


# But the lists content are able to be combined if done individually
out1 <- l1[[1]] + l1[[2]] + l1[[3]] + l1[[4]]
str(out1)
out1

# make things interesting
l1 <- list()
for(i in 1:1000){
  #i = 1
  di  <- iris
  di[sapply(iris, is.numeric)]  <- iris[sapply(iris, is.numeric)] * sample(rnorm(100,1,2), 1)
  l1[[i]]  <- di
}
str(l1)
# so we could figure out a way to iterate over the dataframes with a
# loop like
## for(i in 1:length(l1)){
##   if(i == 1){
##     out2 <- l1[[i]]
##     next
##   } else {
##     out2  <- out2 + l1[[i]]
##   }
## }
# doesn't like factor
#out2
# or 
## d1 <- iris
## str(d1) # Note pesky factor column
## d2 <- d1
## # Only multiply cells that are numeric
## str(d2[sapply(d2, is.numeric)])
## d2[sapply(d2, is.numeric)] <- d2[sapply(d2, is.numeric)] * 2

## # Add another data.frame to make it interesting
## d3 <- d1
## d3[sapply(d3, is.numeric)] <- d3[sapply(d3, is.numeric)] * 3


## l1 = list(d1,d2, d3)
str(l1)

# Inititalise output dataframe to zero

# Sum data frames
summarise_list_dfs3 <- function(
  listed=l1
  ){
  d.results <- listed[[1]]
  d.results[sapply(d.results, is.numeric)] <- 0 
  for (i in seq_along(listed)){
    d.results[sapply(d.results, is.numeric)] <-
      d.results[sapply(d.results, is.numeric)] +
      listed[[i]][sapply(listed[[i]], is.numeric)]
  }
  return(d.results)
}
system.time(d.results  <- summarise_list_dfs3(l1))
str(d.results)

# but we want an elegant solution that will be
# able to give any number of dataframes, and also have
# the issue of some variables being factor so not to be used

# let's write some func
# I like SQL for it's clarity
library(sqldf)
# we are basically grouping the values of each row in each df, so add
# an id

nam <- function(x){
  x$row_names <- 1:nrow(x)
  return(x)
}
l_df2 <- lapply(l1, nam)
str(l_df2)
# now construct some sql and run it.  let's make it flexible for
# different summarising functions like sum, mean, median, stdev etc
summarise_list_of_dfs <- function(
  list_of_dfs = l1
  ,
  summarise_fun = 'sum'
  ,
  id = 'row_names'
  ){
  if(!is.data.frame(list_of_dfs[[1]])) list_of_dfs <- lapply(list_of_dfs, as.data.frame)
  l_df2 <- lapply(list_of_dfs, nam)
  x = do.call('rbind.data.frame', l_df2)
names(x)<-gsub("\\.", "_", names(x))
  todo <- sapply(x, 'is.numeric')
  todo <- names(x)[todo]
  todo <- todo[-which(todo == "row_names")]
  oper  <- sprintf('), %s(',summarise_fun)
  sql<-paste("sum(",paste(todo, sep = '', collapse = oper),")", sep = "")
#sql
  out<-sqldf(
  #cat(
  sprintf("select row_names, %s
  from x
  group by row_names
  order by row_names
  ", sql)
  )
  names(x)<-gsub("_", "\\.", names(x))
  return(out)
}

# do
#l1 <- lapply(l1, as.data.frame)

system.time(qc  <- summarise_list_of_dfs(l1))

str(qc)
str(out1)
out1  <- as.data.frame(out1)
#names(qc)<-names(out1)
# same?
#identical(qc, out1)
# not identical
#all(qc == out1)
# all values are equal tho

# compare to base r
summarise_list_dfs2 <- function(listed){
  #listed  <- l1
  d5=do.call('rbind',listed)
#  str(d5)
  d5.rnames =rep(rownames(listed[[1]]),times=length(listed))
  d5$reg=d5.rnames
  d6 = aggregate(d5[sapply(d5, is.numeric)], by=list(d5$reg), FUN=sum) # I should use an
                                          # ifelse, to do this only to
                                          # numeric columns.
  return(d6)
}
system.time(d6 <- summarise_list_dfs2(l1))

str(d6)
row.names(d6) <- d6$Group.1
# same?
#identical(d6, out1[sort(row.names(out1)),])
# not identical
#all(
#  d6[,-c(1)] == out1[sort(row.names(out1)),]
#  )
# looks like the integers are equal but not the doubles.
# but a diff of the two looks same

# now does it work for  mean?
#qc  <- summarise_list_of_dfs(l1, 'mean')
# whoops, sql uses 'avg'
qc  <- summarise_list_of_dfs(l1, 'avg')
str(qc)
# and stdev
qc  <- summarise_list_of_dfs(l1, 'stdev')
str(qc)
# this did multiple dataframes, now try with factor variables
str(l1)
d1[,3] <- as.factor(d1[,3])
d2[,3] <- as.factor(d2[,3])
d3[,3] <- as.factor(d3[,3])
d4[,3] <- as.factor(d4[,3])

l1 = list(d1,d2, d3, d4)
str(l1)
qc  <- summarise_list_of_dfs(l1)
str(qc)
# it has just skipped that variable so this should be good to go
# NB if the combination of all the dataframes is too big for RAM then
# R will fail. in this case I would
# put into a PostgreSQL database as this will use the disk rather than
# RAM
# this can also be used to add indexes and clustering functions to
# speed up the calculations.
# HTH, let me know if there is a more efficient R solution?

#install.packages("data.table")
library(data.table)
sum_ldf4 <- function(
  listed=l1
  ,
  summarise_fun  = 'sum'
                     ){
  d5=do.call('rbind',listed)
  d5.rnames =rep(rownames(listed[[1]]),times=length(listed))
  d5 <- d5[sapply(d5, is.numeric)]
  d5$reg=d5.rnames
  oper  <- sprintf('), %s(',summarise_fun)
  todo <- sapply(d5, 'is.numeric')
  todo <- names(d5)[todo] 
  sql<-paste("sum(",paste(todo, sep = '', collapse = oper),")", sep = "")
  #sql
    
  DT = data.table(d5)
  
  out <-  eval(parse(
      text = sprintf("DT[,list(%s),by=list(d5$reg)]", sql)
      ))
    

  return(out)
}


# sqldf
o1 <- system.time(qc  <- summarise_list_of_dfs(l1))
str(qc)
# aggregate
o2 <- system.time(d6 <- summarise_list_dfs2(l1))
str(d6)
# loop
o3 <- system.time(d.results  <- summarise_list_dfs3(l1))
str(d.results)
# data.table
o4 <- system.time(datab  <- sum_ldf4(l1))
str(datab)

# http://zvfak.blogspot.com.au/2011/03/applying-functions-on-groups-sqldf-plyr.html
library(gplots)
x<-c(o1[3],o2[3],o3[3],o4[3])
balloonplot( rep("time.elapsed",5),c("sqldf","aggregate","loop", "data.table"),round(x,1), ylab ="Method", xlab="",sorted=F,dotcolor=rev(heat.colors(5)),main="time.elapsed for different methods") 

sessionInfo()
```



#+end_src

** COMMENT text2columns-code
#+name:text2columns
#+begin_src R :session *shell* :tangle no :exports none :eval no
#### name:text2columns ####
setwd("/home/ivan_hanigan/Dropbox/projects/air_pollution_ucrh")
dat <- read.csv("air_pollution_ucrh_data_inventory_files_20150724.csv", stringsAsFactor = F)
str(dat)
qc <- dat[dat$v5 == "arcgis mxd", ]
qclist <- names(table(qc$entity_name))
qc2  <- strsplit(qclist, "_")
str(qc2)
qc2[[3]]
qc3 <- t(sapply(qc2, '[', 1:max(sapply(qc2, length)))) 
qc3 <- as.data.frame(qc3)
names(qc3) <-  paste("V", 1:13, sep = "")

#+end_src

** COMMENT climate-grids-thredds-code
#+name:climate-grids-thredds

#+begin_src R :session *R* :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-07-27-climate-grids-and-thredds-server-experimenting-update.md  :exports none :eval no :padline no
  ---
  name: climate-grids-and-thredds-server-experimenting-update
  layout: post
  title: Web Data, Climate Grids and THREDDS UPDATE
  date: 2015-07-27
  categories:
  - extreme weather events
  tags:
  - software
  ---
  
  # Update  
  - This is an update to my previous post:  [http://ivanhanigan.github.io/2014/12/climate-grids-and-thredds-server-experimenting/](http://ivanhanigan.github.io/2014/12/climate-grids-and-thredds-server-experimenting/)
  - I was worried about the available metadata with those netcdf data I was working with so contacted Dr Brad Evans at EMAST who set me straight:
  
  #### QUOTE:
      "you need to use ncdf4  - not ncdf  ...
      because they are netcdf4 files and not netcdf3 files. 
      This is poorly explained in the R community.
      All of the netcdf files from Australian providers (AusCover, BoM, etc...) 
      have been in netcdf4 for a couple of years now."
  
  # So here is the revised code using ncdf4 library:
  
  #### CODE
      # sudo apt-get install r-cran-ncdf4 
      library(ncdf4)
      library(raster)
      strt <-'2012-01-01'
      end <- '2012-01-04'
      dates <- seq(as.Date(strt),as.Date(end),1)          
      dates
      par(mfrow = c(2,2))
      for(i in 1:length(dates)){
       # i=1
        date_i <- dates[i]
        infile <- sprintf("http://dapds00.nci.org.au/thredds/dodsC/rr9/Climate/eMAST/ANUClimate/0_01deg/v1m0_aus/day/land/tmin/e_01/2012/eMAST_ANUClimate_day_tmin_v1m0_%s.nc", gsub("-", "", date_i))
       
        nc <- nc_open(infile)
        str(nc)
        print(nc)
        vals <- ncvar_get(nc, varid="air_temperature")
        str(vals)
        nc.att <-    nc$var$air_temperature
        xmin <- min(nc.att$dim[[1]]$vals)
        xmax <- max(nc.att$dim[[1]]$vals)
        ymin <- min(nc.att$dim[[2]]$vals)
        ymax <- max(nc.att$dim[[2]]$vals)
       
        print(c(xmin,xmax))
        print(c(ymin,ymax))
       
        r <- raster(t(vals),
                    xmn=xmin, xmx=xmax,
                    ymn=ymin, ymx=ymax)
        #str(r)
        plot(r)
        nc_close(nc)
      }
  
  
  <p></p>
  
  # RESULTS 1: METADATA
  
  - the result is I now have a lot more metdata returned to my R workspace
    
  #### EXERPT
      [...]
      [1] "        licence_copyright: Copyright 2009-2013 ANU. Rights owned by The Australian National University (ANU). Rights licensed subject to TERN  Attribution (TERN-BY)."
      [1] "        short_desc: Australian coverage, ANUClimate 1.0, 0.01 degree grid, 1970-2012"
      [1] "        summary: Minimum daily temperature, for the Australian continent between 1970-2012. Daily temperature regulates rates of plant growth and determines critical conditions such as frost on flowering and fruiting. Modelled by expressing each daily value as a difference anomaly with respect to the gridded 1976-2005 mean daily minimum temperature for each month as provided by eMAST_ANUClimate_mmn_tmin_v1m0_1976_2005. The daily anomalies were interpolated by trivariate thin plate smoothing spline functions of longitude, latitude and vertically exaggerated elevation using ANUSPLIN Version 4.5. There was an average of 671 Bureau of Meteorology data points available for each day between 1970 and 2012. Automated quality assessment rejected on average 3 data values per day with extreme studentised residuals. These were commonly associated with days following missing observations. The root mean square of all individual cross validation residuals provided by the spline analysis is 1.5 degrees Celsius. A comprehensive assessment of the analysis and the factors contributing to the quality of the final interpolated daily minimum temperature grids is in preparation."
      [1] "        long_name: Daily minimum temperature"
      [1] "        contact: Michael Hutchinson, Professor of spatial and temporal analysis, 3.23A, Fenner School of Environment & Society, College of Medicine, Biology & Environment, Frank Fenner Building 141, Australian National University, Canberra, Australian Capital Territory, 200, Australia, (+61) 2 6125 4783, Michael.Hutchinson@anu.edu.au, http://orcid.org/0000-0001-8205-6689"
      [1] "        references: 1. Hutchinson, M.F., Mckenney, D.W., Lawrence, K., Pedlar, J., Hopkinson, R., Milewska, E. and Papadopol, P. 2009. Development and testing of Canada-wide interpolated spatial models of daily minimum/maximum temperature and precipitation for 1961-2003. Journal of Applied Meteorology and Climatology 48: 725�741. http://dx.doi.org/10.1175/2008JAMC1979.1 2. Hutchinson, M.F. and Xu, T. 2013. ANUSPLIN version 4.4 User Guide. Fenner School of Environment and Society, Australian National University, Canberra http://fennerschool.anu.edu.au/files/anusplin44.pdf"
      [1] "        source: ANUClimate 1.0"
      [1] "        keywords: EARTH SCIENCE > ATMOSPHERE > ATMOSPHERIC TEMPERATURE > MAXIMUM/MINIMUM TEMPERATURE"
      [1] "        Conventions: CF-1.6"
      [1] "        institution: Australian National University"
      [1] "        geospatial_lat_min: -43.74"
      [1] "        geospatial_lat_max: -9"
      [1] "        geospatial_lat_units: degrees_north"
      [1] "        geospatial_lat_resolution: -0.01"
      [1] "        geospatial_lon_min: 112.9"
      [1] "        geospatial_lon_max: 154"
      [1] "        geospatial_lon_units: degrees_east"
      [1] "        geospatial_lon_resolution: 0.01"
      [1] "        keywords_vocabulary: Global Change Master Directory (http://gcmd.nasa.gov)"
      [1] "        metadata_link: http://datamgt.nci.org.au:8080/geonetwork"
      [1] "        standard_name_vocabulary: Climate and Forecast(CF) convention standard names (http://cf-pcmdi.llnl.gov/documents/cf-standard-names)"
      [1] "        id: eMAST_ANUClimate_day_tmin_v1m0_1970_2012"
      [1] "        DOI: To be added"
      [1] "        cdm_data_type: grid"
      [1] "        contributor_name: Michael Hutchinson, Jennnifer Kesteven, Tingbao Xu"
      [1] "        contributor_role: principalInvestigator, author, author"
      [1] "        creator_email: eMAST.data@mq.edu.au"
      [1] "        creator_name: eMAST data manager"
      [1] "        creator_url: http://www.emast.org.au/"
      [1] "        Metadata_Conventions: Unidata Dataset Discovery v1.0"
      [1] "        publisher_name: Ecosystem Modelling and Scaling Infrastructure (eMAST) Facility: Macquarie University"
      [1] "        publisher_email: eMAST.data@mq.edu.au"
      [1] "        publisher_url: http://www.emast.org.au/"
  
    
  # RESULTS 2: Grid data  
  
  - I still get lots of good data
  
  ![/images/thredds2.png](/images/thredds2.png)
  
  <p></p>
  
  # NOTE I still need that weird transpose
  
  - I note that the weird hacky transpose is still required 
  
  #### CODE
      # NB weird hacky transpose still required or else you get this
      r <- raster(vals,
                  xmn=xmin, xmx=xmax,
                  ymn=ymin, ymx=ymax)
       
      #str(r)
      plot(r)
  
  <p></p>
  
  ![/images/thredds2raw.png](/images/thredds2raw.png)
  
  
  
#+end_src

** projecting rasters
CSIRO blend 2005
===

```{r, eval = F}
library(rgdal)

indir <- "Q:/Research/Environment_General/CTM_CSIRO/CSIRO_blend_2005/data_provided"
infile <- "blended_no2_24h.txt"

#had to be copied to 
indir <- "Q:/Research/Environment_General/CTM_CSIRO/CSIRO_blend_2005/data_derived"
infile <- "blended_no2_24h.grid"
'
with the header changed to 
ncols 61 
nrows 61
xllcenter 264623.000 
yllcenter 6204784.00 
cellsize 1500.00000
(delete second 1500)
and then 
'
outdir <- "Q:/Research/Environment_General/CTM_CSIRO/CSIRO_blend_2005/data_derived"
outfile <- "blended_no2_24h.tif"

ctm <- rgdal::readGDAL(file.path(indir, infile))
str(ctm)
epsg <- make_EPSG()
names(epsg)
epsg[grep("AMG zone 56", epsg$note),]
crs <- epsg[grep("AGD84 / AMG zone 56", epsg$note),"prj4"]
crs
ctm2 <- raster(ctm)
proj4string(ctm2) <- CRS(crs)

str(ctm2)
crs2 <- epsg[grep(4283, epsg$code),"prj4"]
crs2
# thought this was logical not character? use directly
ctm3 <- projectRaster(ctm2, crs = "+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs")
str(ctm3)
writeRaster(ctm3, file.path(outdir, outfile), drivername="GTiff")
```

* Statistical Modelling
** 2015-09-12-effect-modification-and-mediation
*** intro
~/projects/ivanhanigan.github.com.raw/_posts/2015-09-12-effect-modification-and-mediation.md 
#+name:effect-modification-and-mediation-header
#+begin_src markdown :tangle no :exports none :eval no :padline no
---
name: effect-modification-and-mediation
layout: post
title: effect-modification-and-mediation
date: 2015-09-12
categories:
- disentangle
tags:
- statistical modelling
---

http://www.talkstats.com/showthread.php/27943-moderator-vs.-mediator-variable

trinker
Here's what I've come up with:
  
moderator can affect direction and size of an effect between 2 other variables (x1 and y) but it you remove the moderator(x2) the effect between x1 and y still remains.
  
A mediator explains the relationship between 2 variables (x1 and y). If you remove the effects of the mediator (x2) there no longer exists a relationship between x1 and y.
  
Please critique 
  
#### Code:effect-modification-and-mediation
  
Lazar
  Re: moderator vs. mediator variable

      If a moderator is present then the relationship between x and y is different for different levels of a moderator M. For example the relationship between reading ability (x) and school engagement (y) may be different for boys and girls (where gender is the moderator m). I like to think of moderation as like a multigroup analysis. For example, if I thought gender was a moderator of some regression effect I was interested in I could run the regression in boys and girls seperately and see if they were different.

      Mediation is about mechanism (m) that explain why x effects y. For example, the reason girls choose to study physical sciences at a lower rate than boys may be due to gendered parental expectations. In other words the relationship x(gender) -> Y(physical science college majors) is the result of a mechanism (i.e. it is not gender itself that leads to lower science uptake among women). Rather the mechanism is X(gender) -> M(gendered parental expectations) -> Y(physical science majors).

      EDIT: While I am at it the distinction between moderation and mediation gets a bit grey in causal mediation or conditional indirect effects as these increasingly common models involve both moderation and mediation processes. 

      For mediation you essentially need the following:

  Y = \alpha + \beta_{x1} (1)

  M = \alpha + \beta_{x2} (2)

  Y = \alpha + \beta_m + \beta_{x3} (3)

  Mediation Occurs if \beta_{x1} is significant in formula 1 but no longer significant (OR in many case we just want it to be smaller) in formula 3 (\beta_{x3}). If this is the case we can then decompose the effect of x on y into total, indirect (the effect of x on y that is mediated by m), and direct effects (the remaining effect of x on y after the indirect effect is accounted for).

  Total effects = \beta_{x1}
  Direct effects = \beta_{x3}
  Indirect effects = \beta_{x2} * \beta_{m}
  in all 3 of the equations, \beta_{xi} is the same X,
  
  EDIT: Note how this differs from moderation. Moderation is estimated in a single equation
  Y = \beta_{01} + \beta_{11}x + \beta_{12}z + \beta_{13}xz

  Plugging in the applicable numbers here will give you the effect of x on y for different values of z. Here is some R code of mine which illustrates the relationship between experience and self-concept for different levels of workplace seniority (i.e. moderation).
Code: 
  
#General Workplace self-concept
#+end_src
*** COMMENT eg-code
#+name:eg
#+begin_src R :session *R* :tangle eg.R :exports none :eval no
  #### name:eg ####
    
    
    
  plot(1, type="n", xlim=c(-3,5), ylim=c(-2,2), xlab="Experience", ylab="Self-concept", main="General Workplace")
  curve(exp=-.008*x + .137*-2 + .100*-2*x, from=-3, to=3, add=TRUE)
  curve(exp=-.008*x + .137*2  + .100*2*x,  from=-3, to=3, lty=3, add=TRUE)
  #locator()
  #used locator to get position of text
  text(4, .85, "High Seniority (+2 SD)")
  text(4, -.90, "Low Seniority (-2 SD)")
    
  Bryan said Isn't that must multicollinearity between two of the predictors? 
    
  Lazar replied you are correct (in that the varibles must obviously be colinear). The distinction to be made is that 
  a theoretical interpretation is applied to this (and all of the associated causal ordering baggage that this brings with it). 
  This theoretical formation is conceptualised as a path model and the relationship between y and x is decomposed as above. 
    
  
#+end_src
*** COMMENT after_brambor
#+name:after_brambor
#+begin_src R :session *R* :tangle after_brambor.R :exports none :eval yes
  #### name:after_brambor ####
  
  x  <- seq(-3,3, by = 0.05)
  #y  <- (-.008*x + .137*-2 + .100*-2*x) + sample(rnorm(length(x),0,0.05))
  y <- -0.5 + sample(rnorm(length(x),0,0.05))
  y2 <- (-.008*x + .137* 2  + .100* 2*x) + sample(rnorm(length(x),0,0.05))
  
  
  pdf("effect_modification.pdf")#,  width = 1000, height = 550)
  par(mar = c(3,3,2,1))
  plot(1, type="n", xlim=c(-3.3,3), ylim=c(-1,1), xlab="", ylab="", axes = F)
  axis(1, labels = F); axis(2, labels = F)
  mtext("Y", 2, 1 , at = 0, las = 2)
  mtext("X", 1, 1 , at = 0)
  title(expression(paste("Regression model Y = ", beta[0] + beta[1],"X + ", beta[2], "Z + ", beta[3], "XZ + ", epsilon )))
  #, 3, 0, at = 1, cex = .85)
  segments(-3, -0.5, 3, -0.5, lwd=2)    
  points(x,y, pch = 1, cex = .5)
  
  #curve(exp=-.008*x + .137*-2 + .100*-2*x, from=-3, to=3, lwd=2,
  #add=TRUE)
  #mtext(expression(paste("Y = ", beta[0] + beta[1] ,"X when Z = 0")),
  #  4, 0, las = 2, at = -0.5, cex = .85)
   
  #mtext(expression(paste("Y = (", beta[0] + beta[2],") + (", beta[1] + beta [3],")X")),
  #  4, 0, las = 2, at = 0.85, cex = .85)
  #mtext("when Z = 1",
  #  4, 0, las = 2, at = 0.775, cex = .85)
  
  
  
  curve(exp=-.008*x + .137*2  + .100*2*x,  from=-3, to=3, lwd=2, add=TRUE)
  points(x, y2, pch = 3, cex = .5)
  
  text(x = -1.2, y = 0.6, expression(paste("Y = (", beta[0] + beta[2],") + (", beta[1] + beta [3],")X when Z = 1")))
  text(x = -2.1, y = 0.5, expression(paste("Slope = ", beta[1] + beta[3])))

  #text(x = -3.4, y = -0.4, '{', srt = 0, cex = 3)
  text(x = -3.3, y = -0.4, expression(beta[2]))
  text(x = 0.57, y = -0.21,  expression(paste("Y = ", beta[0] + beta[1] ,"X when Z = 0")))
  text(x = 0, y = -0.3, expression(paste("Slope = ", beta[1])))
  segments(-3, -0.3, -3, -0.5, lty = 1, col = 'grey', lwd = 6)
  segments(-3.9, -0.5, -3, -0.5, lty = 3)
  segments(-3.9, -0.3, -3, -0.3, lty = 3)
  #mtext(expression(beta[2]),2,2, at = -0.4, las = 2)
  #text(x = -3.4, y = -.75, '{', srt = 0, cex = 3)
  text(x = -3.3, y = -.75, expression(beta[0]))
  segments(-3, -0.5, -3, -1, lty = 3)
  #mtext(expression(beta[0]),2,2, at = -0.75, las = 2)
  dev.off()
  #browseURL("effect_modification.pdf")
#+end_src

#+RESULTS: after_brambor
: 2


** p-values and garden of forking paths
The p-value is the probability of seeing something at least as extreme as the data, if the null hypothesis were true.
http://andrewgelman.com/2014/10/14/didnt-say-part-2/

R doesn’t offer a pseudo-R^2 with ordinary poisson GLM)
 
From: Keith Dear
Sent: Thursday, 11 November 2010 6:16 PM
To: Ivan Hanigan
Subject: RE: Psuedo-R-Squared for poisson GLM in R
 
I have run Dobson’s small dataset through Stata, as below (since I don’t have your data) to see what R2 values we get:
 
. clear
. input counts
18
17
15
20
10
20
25
13
12
. end
. egen byte outcome=seq(), to(3)
. egen byte treatment=seq(), to(3) block(3)
. poisson counts outcome treatment
 
Poisson regression                                Number of obs   =          9
 
                                                  LR chi2(2)      =       2.57
 
                                                  Prob > chi2     =     0.2773
 
Log likelihood = -24.824068                       Pseudo R2       =     0.0491
 
------------------------------------------------------------------------------
 
      counts |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
 
-------------+----------------------------------------------------------------
 
     outcome |  -.1606877   .1006457    -1.60   0.110    -.3579497    .0365744
 
   treatment |  -4.77e-17         .1    -0.00   1.000    -.1959964    .1959964
 
       _cons |   3.126198   .2880594    10.85   0.000     2.561612    3.690784
 
------------------------------------------------------------------------------
 
. ereturn list
 
/*
 
partial list of saved quantities:
 
                e(ll) =  -24.82406763934515
 
               e(ll_0) =  -26.10681159435371
 
               e(df_m) =  2
 
               e(chi2) =  2.565487910017119
 
                  e(p) =  .2772754243649355
 
               e(r2_p) =  .0491344548288688
 
So the “pseudo R2” reported by Stata is just the proportional reduction
 
in deviance (or log-likelihood): only 4.9%
 
*/
 
. di (e(ll_0) - e(ll)) / e(ll_0)
 
.04913445
 
 Now calculate the max possible LL: sum of log Poisson probability for each observation x,
 
 if each x came from a Poisson distribution with mean mu=x (the "saturated model").
 
 note Poisson probabilities are (mu^x)exp(-mu)/x!, and lngamma(x+1) = ln(x!).
 
. gen lmin = counts*ln(counts) - counts - lngamma(counts+1)
 
. qui sum lmin
 
. di r(sum)
 
-20.816089
 
 How far has the model moved us towards that upper limit?
 
. di (e(ll_0) - e(ll)) / (e(ll_0) - (-20.816089))
 
.24245156
 
 24.2% is a bit different!!
 
 Now the formula you found:
 
. scalar dev=-2*e(ll)
 
. scalar dev0=-2*e(ll_0)
 
. scalar n=9
 
. di (1-exp((dev-dev0)/n))/(1-exp(-dev0/n))
 
.24877856
 
 Very similar, though I don’t understand why (guess I should read Maddalla)
 
Keith
 
_____________________________________________
From: Ivan Hanigan
Sent: Wednesday, 10 November 2010 11:36 AM
To: Keith Dear
Subject: Psuedo-R-Squared for poisson GLM in R
 
Thanks for this tip!
 
There seem to be a few formulae available but this one seemed easiest.
 
http://www.math.yorku.ca/Who/Faculty/Monette/S-news/0422.html
 
Rsquared.glm <- function(o) {
 
n <- length(o$residuals) # number of observations
 
R2 <- (1 - exp((o$deviance - o$null.deviance)/n))/(1 - exp( - o$null.deviance/n))
 
names(R2) <- "pseudo.Rsquared"
 
R2
 
}
 
## poisson GLM from Dobson (1990) Page 93: Randomized Controlled Trial :
 
counts <- c(18,17,15,20,10,20,25,13,12)
 
outcome <- gl(3,1,9)
 
treatment <- gl(3,3)
 
print(d.AD <- data.frame(treatment, outcome, counts))
 
glm.D93 <- glm(counts ~ outcome + treatment, family=poisson())
 
Rsquared.glm(glm.D93)
** 2015-10-31-decisions-to-make-when-modelling-interactions
#+name:decisions-to-make-when-modelling-interactions-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-31-decisions-to-make-when-modelling-interactions.md :exports none :eval no :padline no
  ---
  name: decisions-to-make-when-modelling-interactions
  layout: post
  title: Decisions to make when modelling interactions
  date: 2015-10-31
  categories:
  - disentangle
  - statistical modelling
  ---

  This post focuses on decision making during statistical
  modelling.  The case of investigating effect modification is used as
  an example.  The analyst has several options available to them when
  constructing the model parametrisation for adjustment to explain
  modification effectively.  Unlike confounding (in which the criterion
  of substantial magnitude change of the effect estimate when
  controlling for a third variable can be easily assessed), models
  including effect modification can be hard to interpret.  Taking
  account of effect modification becomes increasingly important when
  modelling complex interactions.
  
  If an effect moderator is present then the relationship between
  exposure and response varies between different levels of the
  moderator. An example is provided by a paper we published, in which the
  relationship between proximity to wetlands and Ross River virus
  infection was found to be different for towns and rural areas, where the
  'urban' variable is the 'effect moderator'.
  
  There are three common ways for data analysts to address this question
  in statistical models: 
  
  1. multiple models for each group, 
  1. interaction terms or 
  1. re-parametrisation to explicitly depict interactions.  


  The first way is
  to split apart the dataset and conduct separate analyses of multiple
  groups. For example, one could run the regression in the urban zone
  and the rural zone seperately and see if there were different exposure
  response functions in the two models.  This was the approach of our paper.
  This approach has the strength that it is simple to do and yeilds
  results that are easy to interpret.  A limitation of this method is
  that by splitting the dataset one loses degrees of freedom and
  therefore statistical power.
  
  The second approach (which removes that limitation of the first option) is to
  fit interaction terms.  The example shown in Figure 1 is a multiplicative interaction model (Brambor 2006). 
  This  approach was not taken in the models reported in our paper, but can easily be implemented and shows that the function of distance was
  estimated to have different 'slopes' in each of the dichotomous urban
  groups.

  The statistical method can be easily implemented in 
  software by including a multiplicative term between two variables,
  however in practice the resulting post estimation regression outputs
  can be difficult to interpret.  For example, say one wants to calculate
  the effect and standard error for exposure X on health outcome Y with
  the interaction of the effect modifier Z.  The form of this model can
  be written as:
  
  $$ Y ~ \beta_{1}X + \beta_{2}Z + \beta_{3}XZ $$


  where B1, B2 and B3 are the regression coefficients estimated and  the term XZ is the interaction between exposure and effect modifier.
  
  The difficulty for interpretation comes when using this method for calculating the marginal effect of X on Y and the conditional standard error.  The specific method described in Brambor et al. is:
    
  1. Calculate the coefficients and the variance-covariance matrix from the regression model
  2. The marginal effect is $\beta_{1} + \beta_{3}XZ$ where Z is the level of the 
  modifying factor (0 or 1 in the dichotomous effect modifier case)
  3. The conditional standard error is:
  
  $$\sqrt{var(\beta_{1}) + Z^2 var(\beta_{3}) + 2Zcov(\beta_{1}\beta_{3})}$$
  
  Therefore the strengths of this approach is that it does not reduce
  degrees of freedom and is straightforward to specify the model in
  standard statistical software packages. The limitations are related to
  interpretation of the resulting coefficients for both the main effects and the marginal effects, and standard errors for these.
  
  The third approach available to analysts makes it easier to access the
  resulting regression output.  This method was employed in Paper 1 in
  the final modelling phase in which estimates were calculated for the
  different drought exposure-response funcitons in each of the
  sub-groups. In the terms of Figure \ref{fig:effectmod.png} it is simple to:
    
  1. Calculate X1 = X * Z (i.e = exposure for condition is met, zero otherwise)
  1. Calculate X0 = X * (1-Z) (i.e. = exposure for NON-condition, zero for condition is TRUE)
  1. Instead of X, Z and XZ, fit X1, X0 and Z.  This model also contains three parameters and captures the same interactions as it is the same model with a different parametrisation.  The standard errors for the X1 and X0 are calculated directly from the regression.
  
  This method is much easier to implement and interpret.  This is also
  considerably more flexible than the other two approaches.  A
  limitation remains for this method in that the pre-processing steps
  required are more complicated, and there are inherently more
  possibilities for the data analyst to make errors in writing their
  code as they make these changes to the analytical data.  

  ```
  # Demonstrating this with the data from the paper (available in table 2)
  ## the following code shows the different parametrisations for the effect modification by urban
  ## we show that the coeffs and se are equivalent but that the psuedo-R
  ## squared will be better when including all our data in stratified analysis
  
  # model 0 effect in eastern
  d_eastern
  fit <- glm(cases~ buffer + offset(log(1+pops)),family='poisson', data=d_eastern )
  summa <- summary(fit)
  summa
  ## Coefficients:
  ##             Estimate Std. Error z value Pr(>|z|)
  ## (Intercept) -4.82425    0.14451 -33.382  < 2e-16 ***
  ## buffer      -0.24702    0.07921  -3.119  0.00182 **
  
  # model 1 effect in urban
  fit1 <- glm(cases~ buffer + offset(log(1+pops)),family='poisson', data=d_urban )
  summa <- summary(fit1)
  summa
  ## Coefficients:
  ##             Estimate Std. Error z value Pr(>|z|)
  ## (Intercept) -5.52363    0.33354 -16.561   <2e-16 ***
  ## buffer       0.03853    0.06352   0.607    0.544
  
  # step 1, combine the urban and rural data
  d_eastern$urban <- 0
  d_urban$urban <- 1
  dat2 <- rbind(d_eastern, d_urban)
  str(dat2)
  dat2
  
  # model 2 is a multiplicative term
  fit2 <- glm(cases ~ buffer * urban + offset(log(1+pops)), family = 'poisson', data = dat2)
  summa <- summary(fit2)
  summa
  ## Coefficients:
  ##              Estimate Std. Error z value Pr(>|z|)
  ## (Intercept)  -4.82425    0.14451 -33.382  < 2e-16 ***
  ## buffer       -0.24702    0.07921  -3.119  0.00182 **
  ## urban        -0.69938    0.36350  -1.924  0.05435 .
  ## buffer:urban  0.28555    0.10153   2.812  0.00492 **
  
  # the coeff on buffer is for urban = 0 is main effect
  # the coeff on buffer:urban is for urban = 1 is the marginal effect
  b1 <- summa$coeff[2,1]
  b3 <- summa$coeff[4,1]
  b1 + b3
  # 0.0385268
  # but what about that p-value?  and the se?
  str(fit2)
  fit2_vcov <- vcov(fit2)
  fit2_vcov
  # now calculate the conditional standard error for the marginal effect of buffer for the value of the modifying variable (Z, urban =1)
  varb1<-fit2_vcov[2,2]
  varb3<-fit2_vcov[4,4]
  covarb1b3<-fit2_vcov[2,4]
  Z<-1
  conditional_se <- sqrt(varb1+varb3*(Z^2)+2*Z*covarb1b3)
  conditional_se
  
  
  
  # model 3 is the re-parametrisation
  dat2$buffer_urban <- dat2$buffer * dat2$urban
  dat2$buffer_eastern <- dat2$buffer * (1-dat2$urban)
  
  fit3 <- glm(cases ~ buffer_urban + buffer_eastern + urban + offset(log(1+pops)), family = 'poisson', data = dat2)
  summa <- summary(fit3)
  summa
  
  ## Coefficients:
  ##                Estimate Std. Error z value Pr(>|z|)
  ## (Intercept)    -4.82425    0.14451 -33.382  < 2e-16 ***
  ## buffer_urban    0.03853    0.06352   0.607  0.54416
  ## buffer_eastern -0.24702    0.07921  -3.119  0.00182 **
  ## urban          -0.69938    0.36350  -1.924  0.05435 .
  ```  
#+end_src


** TODO driver analysis versus partial correlation maps
Instead of a driver analysis that narrows our thinking to a single dependent variable and its highest regression weights, the partial correlation map opens us to the possibilities

What can we learn from the data? The answer is a good deal more than can be revealed by the largest coefficient in a single regression equation
http://joelcadwell.blogspot.com.au/2015/11/statistical-models-that-support-design.html
* Workflow Tools

** TODO-list
- make a newnode_{family} of functions to do certain pre-processing for dagrammer package to work with
- eg newnode_and_edges takes dataframe of node_name, inputs_csv, outputs_csv and splits to nodes and edges
- newnode_csv takes that compact view and expands out the csv_strings to individual rows of a table
- newnode_rmd takes compact df  and creates RMD like IUCN did
** COMMENT DEPRECATED R-newnode_df
*** COMMENT R- OLD newnode
#+name:newnode
#+begin_src R :session *R* :tangle no :exports none :eval yes
  ################################################################
  # name:newnode
  newnode<-function(
    name = "name_of_step"
    ,
    inputs= c("input_to_step", "input2", "in3", "in4")
    ,
    outputs= c("output_from_step", "out2", "out3") # character(0)
    ,
    desc = "some (potentially) long descriptive text saying what this step is about and why and how"
    ,
    graph = 'nodes'
    , newgraph=F, notes=F, code=NA, ttype=NA, plot = T,
    rgraphviz = F,
    nchar_to_snip = 40
    ){
     if(rgraphviz == F){
  
    if(nchar(name) > 140) print("that's a long name. consider shortening this")
    if(nchar(desc) > nchar_to_snip) desc <- paste(substr(desc, 1, nchar_to_snip), "[...]")
    name2paste <- paste('"', name, '"', sep = "")
    inputs <- paste('"', inputs, '"', sep = "")
    inputs_listed <- paste(inputs, name2paste, sep = ' -> ', collapse = "\n")
    #cat(inputs_listed)
    outputs <- paste('"', outputs, '"', sep = "")  
    outputs_listed <- paste(name2paste, outputs, sep = ' -> ', collapse = "\n")
    #cat(outputs_listed)
    strng <- sprintf('%s
  %s  [ shape=record, label="{{ { Name | Description } | { %s | %s } }}"] 
  %s\n\n', inputs_listed, name2paste, name, desc, outputs_listed
    )
    if(newgraph == F) eval(parse(text =
                                   sprintf('strng <- paste(%s, strng, "\n")', graph, graph)
                             ))
    # cat(strng)
    return(strng)
  
    } else {
    # USAGE
    # nodes <- newnode(  # adds to a graph called nodes
    # name = 'aquire the raw data'  # the name of the node being added 
    # inputs = REQUIRED c('external sources','collected by researcher') # single or multiple inputs to it
    # outputs = OPTIONAL c('file server','metadata','cleaning') # single or multiple outputs from it
    # append=F # append to existing graph?  if False remove old graph of that name and start new
    # TODO 
    # nodes <- addEdge(from='analyse using stats package',
    # to='new data in database server',graph=nodes,weights=1)
    # INIT
    # source('http://bioconductor.org/biocLite.R')
    # biocLite("Rgraphviz")
    # or may be needed for eg under ubuntu
    # biocLite("Rgraphviz", configure.args=c("--with-graphviz=/usr"))
    require(Rgraphviz)
    # FURTHER INFO
    # see the Rgraphviz examples
    # example(layoutGraph)
    # require(biocGraph) # for imageMap
    # TODO change names in following
    dsc <- name
    i <- inputs
    o <- outputs
    #   if(!exists('nodes')) {
    if(newgraph==T) {    
      nodes <- new("graphNEL", nodes=c(dsc),
                 edgemode="directed")
      # nodes <- addEdge(from=i, to=dsc, graph=nodes, 1)    
    } else {
      if(length(grep(dsc,nodes@nodes)) == 0) nodes <- addNode(node=dsc,object=nodes)
    }  
    if(sum(i %in% nodes@nodes) != length(i)) {
      inew <- i[!i %in% nodes@nodes]
      nodes <- addNode(node=inew,object=nodes)   
    }
    nodes <- addEdge(i, dsc, nodes, 1)
    #}
    if(length(o) > 0){
    if(sum(o %in% nodes@nodes) != length(o)) {
      onew <- o[!o %in% nodes@nodes]
      nodes <- addNode(node=onew,object=nodes)   
    }
    nodes <- addEdge(from=dsc, to=o, graph=nodes, 1)  
    }
    if(plot == T){
      try(silent=T,dev.off())
      plot(nodes,attrs=list(node=list(label="foo", fillcolor="grey",shape="ellipse", fixedsize=FALSE), edge=list(color="black")))
    }
    return(nodes)
    }
  }
  
#+end_src

#+RESULTS: newnode

*** test-newnode
#+name:newnode
#+begin_src R :session *R* :tangle no :exports reports :eval no
  ################################################################
  # name:newnode
  # REQUIRES GRAPHVIZ, AND TO INSTALL RGRAPHVIZ
  # source('http://bioconductor.org/biocLite.R')
  # biocLite("Rgraphviz")
  # or may be needed for eg under ubuntu
  # biocLite("Rgraphviz", configure.args=c("--with-graphviz=/usr"))
  # FURTHER INFO
  # see the Rgraphviz examples
  # example(layoutGraph)
  # require(biocGraph) # for imageMap
  
  # source("R/newnode.r")
  require(devtools)
  #install_github("disentangle", "ivanhanigan")
  load_all()
  #require(disentangle)
  newnode(
    name = "NAME"
    ,
    inputs="INPUT"
    ,
    outputs = "OUTPUT"
    ,
    graph = 'nodes'
    ,
    newgraph=T
    ,
    notes=F
    ,
    code=NA
    ,
    ttype=NA
    ,
    plot = T, rgraphviz = F
    )
  
  nodes <- newnode("merge", c("d1", "d2", "d3"), c("EDA"),
                   newgraph =T)
  nodes <- newnode("qc", c("data1", "data2", "data3"), c("d1", "d2", "d3"))
  nodes <- newnode("modelling", "EDA")
  nodes <- newnode("model checking", "modelling", c("data checking", "reporting"))
#+end_src
*** COMMENT test-df-input-code
#+name:test-df-input
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:test-df-input####
  library(devtools)
  load_all()
  #require(disentangle)
  # either edit a spreadsheet with filenames, inputs and outputs 
  # filesList <- read.csv("exampleFilesList.csv", stringsAsFactors = F)
  # or 
  filesList <- read.csv(textConnection('
  CLUSTER ,  FILE         , INPUTS                    , OUTPUTS                                , DESCRIPTION                      
  A       ,  siteIDs      , GPS                       ,                                        , latitude and longitude of sites  
  A       ,  weather      , BoM                       ,                                        , weather data from BoM            
  B       ,  trapped      , siteIDs                   ,                                        , counts of species caught in trap 
  B       ,  biomass      , siteIDs                   ,                                        ,                                  
  B       ,  correlations , "weather,trapped,biomass" , report1                                , A study we published             
  C       ,  paper1       , report1                   , "open access repository, data package" ,                                  
  '), stringsAsFactors = F, strip.white = T)
  str(filesList)
  filesList
  ## newnode _ df <- function(indat, names_col, in_col, out_col, desc_col, clusters_col){
  ## # start the graph
  i <- 1
  nodes <- newnode(name = indat[i,names_col],
                   inputs = strsplit(indat[,in_col], ",")[[i]],
                   outputs =
                   strsplit(indat[,out_col], ",")[[i]]
                   ,
                   newgraph=T)
   
  for(i in 2:nrow(indat))
  {
    # i <- 2
    if(length(strsplit(indat[,out_col], ",")[[i]]) == 0)
    {
      nodes <- newnode(name = indat[i,names_col],
                       inputs = strsplit(indat[,in_col], ",")[[i]]
      )    
    } else {
      nodes <- newnode(name = indat[i,names_col],
                       inputs = strsplit(indat[,in_col], ",")[[i]],
                       outputs = strsplit(indat[,out_col], ",")[[i]]
      )
    }
  }
  
  help_txt <- c('
  sink("fileTransformations.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  ')
  cat(help_txt)
  ## return(nodes)
  ## }
  
  
  ## newnode _ df <- function(indat, names_col, in_col, out_col, desc_col, clusters_col, graph = 'nodes_list'){
  ## # start the graph
  ## i <- 1
  ## nodes_list <- eval(parse(text=sprintf('newnode(name = indat[i,names_col],
  ##                  inputs = strsplit(indat[,in_col], ",")[[i]],
  ##                  outputs =
  ##                  strsplit(indat[,out_col], ",")[[i]]
  ##                  ,
  ##                  newgraph=T, graph = %s)', graph)))
   
  ## for(i in 2:nrow(indat))
  ## {
  ##   # i <- 2
  ##   if(length(strsplit(indat[,out_col], ",")[[i]]) == 0)
  ##   {
  ##     nodes_list <- eval(parse(text=sprintf('newnode(name = indat[i,names_col],
  ##                      inputs = strsplit(indat[,in_col], ",")[[i]]
  ##     , graph = %s)',graph)))    
  ##   } else {
  ##     nodes_list <- eval(parse(text=sprintf('newnode(name = indat[i,names_col],
  ##                      inputs = strsplit(indat[,in_col], ",")[[i]],
  ##                      outputs = strsplit(indat[,out_col], ",")[[i]]
  ##     , graph = %s)', graph)))
  ##   }
  ## }
  ## cat(nodes_list)
  ## help_txt <- c('
  ## sink("fileTransformations.dot")
  ## cat("digraph G {")
  ## cat(nodes)
  ## cat("}")
  ## sink()
  ## system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  ## ')
  ## cat(help_txt)
  ## return(nodes_list)
  ## }
  
  
  
  
  str(filesList)
  ## undebug(newnode _ df)
  ## nodes <- newnode _ df(
  ## indat = filesList
  ## ,
  ## names_col = "FILE"
  ## ,
  ## in_col = "INPUTS"
  ## ,
  ## out_col = "OUTPUTS"
  ## ,
  ## desc_col = "DESCRIPTION"
  ## ,
  ## clusters_col = "CLUSTER"
  ## )
  cat(nodes)
  
  sink("fileTransformations.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  
#+end_src
*** COMMENT test-df-input-code2
#+name:test-df-input
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test-df-input####
  library(devtools)
  load_all()
  filesList <- read.csv(textConnection('
  CLUSTER ,  FILE    , INPUTS                    , OUTPUTS                                , DESCRIPTION                      
  A  ,  siteIDs      , GPS                       , spatial                                , latitude and longitude of sites  
  A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
  B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
  B  ,  biomass      , spatial                   , biomass_g                              ,                                  
  B  ,  correlations , "exposures,trapped_no,biomass_g" , report1                         , A study we published             
  C  ,  paper1       , report1                   , "open access repository, data package" ,                                  
  D  ,  biomass revision, new estimates          , biomass_g                              , this came late
  '), stringsAsFactors = F, strip.white = T)
  str(filesList)
  
  indat = filesList
  #,
  names_col = "FILE"
  #,
  in_col = "INPUTS"
  #,
  out_col = "OUTPUTS"
  #,
  desc_col = "DESCRIPTION"
  #,
  clusters_col = "CLUSTER"
  
  ## # start the graph
  rm(nodes)
  
  cluster_ids <- names(table(indat[,clusters_col]))
  cluster_ids
  for(cluster_i in cluster_ids){
    #cluster_i <- cluster_ids[1]
    if(cluster_i == cluster_ids[1]){
    nodes <- sprintf('subgraph cluster_%s {
    label = "%s"
    ', cluster_i, cluster_i)
    
    i <- 1
    nodes <- newnode(name = indat[i,names_col],
                     inputs = strsplit(indat[,in_col], ",")[[i]],
                     outputs =
                     strsplit(indat[,out_col], ",")[[i]]
                     ,
                     newgraph=F)
    } else {
    nodes <- paste(nodes, sprintf('subgraph cluster_%s {
    label = "%s"
    ', cluster_i, cluster_i))
  
    #i <- 1
    if(length(strsplit(indat2[,out_col], ",")[[i]]) == 0){
      nodes <- newnode(name = indat2[i,names_col],
                       inputs = strsplit(indat2[,in_col], ",")[[i]]
      )
    } else {
      nodes <- newnode(name = indat2[i,names_col],
                       inputs = strsplit(indat2[,in_col], ",")[[i]],
                       outputs = strsplit(indat2[,out_col], ",")[[i]]
      )
    }
    }
  #cat(nodes)


  indat2 <- indat[indat[,clusters_col] == cluster_i,]
  indat2
  
  for(i in 2:nrow(indat2)){
    # i <- 2
    if(length(strsplit(indat2[,out_col], ",")[[i]]) == 0){
      nodes <- newnode(name = indat2[i,names_col],
                       inputs = strsplit(indat2[,in_col], ",")[[i]]
      )    
    } else {
      nodes <- newnode(name = indat2[i,names_col],
                       inputs = strsplit(indat2[,in_col], ",")[[i]],
                       outputs = strsplit(indat2[,out_col], ",")[[i]]
      )
    }
  }
  
  nodes <- paste(nodes,"}\n\n")
  }
  cat(nodes)
  help_txt <- c('
  sink("fileTransformations.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  ')
  cat(help_txt)
  cat(nodes)
  
  sink("fileTransformations.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  
#+end_src

#+RESULTS: test-df-input
=0
==0
==0
=*** COMMENT test2-code
#+name:test2
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:test2####
  getwd()
  source("R/newnode.r")
  newnode
  rm(nodes)
  nodes <- newnode("Setup_Data.R", inputs = "data",
                   outputs = c("stag.data.all.years.df",
                     "ash.fire.matrix.df",
                     "stag.data.yyyy.xx.df"), # "Subsets of the data by year and fire type"),
                   newgraph = T)
  
  nodes <- newnode("average number of stags standing per site",
                   inputs = "stag.data.yyyy.xx.df",
                   outputs = "ash.no.stags.yyyy.xx"
                   )
  
  nodes <- newnode("variance in the average number of stags per site",
                   inputs = "stag.data.yyyy.xx.df", 
                   outputs = "ash.var.no.stags.yyyy.xx"
                   )
  nodes <- newnode("compute the number of transitions from form to form",
                   inputs = "stag.data.yyyy.xx.df",
                   outputs = "tpm.98.11.xx.coll.ash"
                   )
  nodes <- newnode("convert to a probability transition matrix",
                   inputs = "tpm.98.11.xx.coll.ash",
                   outputs = "tpm.98.11.xx.coll.ash.per"                 
                   )
  nodes <- newnode("frequencies) by form  in 2011 by fire category ",
                   inputs = "stag.data.2011.xx.df",
                   outputs = "freq.11.xx.coll.ash"
                   )
  sink("test.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf test.dot -o test.pdf")
#+end_src

*** COMMENT TODO man-newnode
#+name:newnode
#+begin_src R :session *R* :tangle no :exports none :eval no
################################################################
# name:newnode

#+end_src

** R-newnode
*** COMMENT newnode-code
#+name:newnode
#+begin_src R :session *R* :tangle R/newnode.r :exports none :eval yes
  #' @title newnode
  #' @name newnode
  #' @param indat the input data.frame
  #' @param names_col the name of each edge (the boxes)
  #' @param in_col the name of the nodes that are inputs to each edge with comma seperated vals.  whitespace will be stripped
  #' @param out_col the nodes that are outputs of each edge.
  #' @param desc_col description
  #' @param clusters_col optional column identifying clusters
  #' @param todo_col optional column with TODO status (DONE and WONTDO will be white, others are red)
  #' @return character string object that has the DOT language representatio of the input
  newnode <- function(indat, names_col = NULL, in_col = NULL, out_col = NULL, desc_col = NULL, clusters_col = NULL, todo_col = NULL, nchar_to_snip = 40, colour_col = NULL){
  if (is.null(names_col)) stop("Names of the steps are needed")
  if (is.null(in_col)) stop("Inputs are needed")
  if (is.null(out_col)) stop("Outputs are needed")
  if (is.null(desc_col)){
    print("Descriptions are strongly recommended, we\'re creating empty records for you")
    indat$descriptions <- ""
    desc_col <- "descriptions"
  }
  
  # sanitize any single quotes
  for(i in 1:ncol(indat)){
   indat[,i] <- gsub("'","", indat[,i]) 
  }
  
  if(!is.null(clusters_col)){
  cluster_ids <- names(table(indat[,clusters_col]))
  #cluster_ids
  
  for(cluster_i in cluster_ids){
    # cluster_i <- cluster_ids[1]
  
    if(cluster_i == cluster_ids[1]){
      nodes_graph <- sprintf('subgraph cluster_%s {
      label = "%s"
      ', cluster_i, cluster_i)
    } else {
      nodes_graph <- paste(nodes_graph, sprintf('subgraph cluster_%s {
      label = "%s"
      ', cluster_i, cluster_i))  
    }
  #  cat(nodes_graph)    
    indat2 <- indat[indat[,clusters_col] == cluster_i,]
  #  indat2
      for(i in 1:nrow(indat2)){
        # i <- 1
        #i
        indat2[i,]
        name <- indat2[i,names_col]
        inputs <- unlist(lapply(strsplit(indat2[i,in_col], ","), str_trim))
        outputs <- unlist(lapply(strsplit(indat2[i,out_col], ","), str_trim))
        desc <- indat2[i,desc_col]
  
  
        if(nchar(name) > 140) print("that's a long name. consider shortening this")
        if(nchar(desc) > nchar_to_snip) desc <- paste(substr(desc, 1, nchar_to_snip), "[...]")
        name2paste <- paste('"', name, '"', sep = "")
        inputs <- paste('"', inputs, '"', sep = "")
        #inputs
        inputs_listed <- paste(inputs, name2paste, sep = ' -> ', collapse = "\n")
        #cat(inputs_listed)
        outputs <- paste('"', outputs, '"', sep = "")  
        outputs_listed <- paste(name2paste, outputs, sep = ' -> ', collapse = "\n")
        #cat(outputs_listed)
  if(!is.null(todo_col)){
  
        status <- indat2[i,todo_col]
  
  strng <- sprintf('%s\n%s  [ shape=record, style = \"filled\", fillcolor=\"white\", label="{{ { Name | Description | Status } | { %s | %s | %s } }}"]\n%s\n\n',
                   inputs_listed, name2paste, name, desc, status, outputs_listed
                   )
        # cat(strng)
        if(!status %in% c("DONE", "WONTDO", "", NA)){
          strng <- gsub("shape=record,", "shape=record, style = \"filled\", fillcolor=\"indianred\",", strng)
        }
      } else {
  
  strng <- sprintf('%s\n%s  [ shape=record, style = \"filled\", fillcolor=\"white\", label="{{ { Name | Description } | { %s | %s } }}"]\n%s\n\n',
                   inputs_listed, name2paste, name, desc, outputs_listed
                   )
  }
        nodes_graph <- paste(nodes_graph, strng, "\n")
        if(nrow(indat2) == 1) break
  
  
        # cat(nodes_graph)
        # set col
        if(!is.na(indat2[i,colour_col])){
         nodes_graph <- gsub("white", indat2[i,colour_col], nodes_graph)
        }
  }
  #cat(nodes_graph)
  ###########
    nodes_graph <- paste(nodes_graph, "}\n\n")
  }
  } else {
    indat2 <- indat
    nodes_graph  <- ""
  #  indat2
      for(i in 1:nrow(indat2)){
        # i <- 1
        #i
        indat2[i,]
        name <- indat2[i,names_col]
        inputs <- unlist(lapply(strsplit(indat2[i,in_col], ","), str_trim))
        outputs <- unlist(lapply(strsplit(indat2[i,out_col], ","), str_trim))
        desc <- indat2[i,desc_col]
  
        if(nchar(name) > 140) print("that's a long name. consider shortening this")
        if(nchar(desc) > nchar_to_snip) desc <- paste(substr(desc, 1, nchar_to_snip), "[...]")
        name2paste <- paste('"', name, '"', sep = "")
        inputs <- paste('"', inputs, '"', sep = "")
        #inputs
        inputs_listed <- paste(inputs, name2paste, sep = ' -> ', collapse = "\n")
        #cat(inputs_listed)
        outputs <- paste('"', outputs, '"', sep = "")  
        outputs_listed <- paste(name2paste, outputs, sep = ' -> ', collapse = "\n")
        #cat(outputs_listed)
  if(!is.null(todo_col)){
    status <- indat2[i,todo_col]
         
  strng <- sprintf('%s\n%s  [ shape=record, style = \"filled\", fillcolor=\"white\", label="{{ { Name | Description | Status } | { %s | %s | %s } }}"]\n%s\n\n',
                   inputs_listed, name2paste, name, desc, status, outputs_listed
                   )
        # cat(strng)
        if(!status %in% c("DONE", "WONTDO", "", NA)){ 
          strng <- gsub("shape=record,", "shape=record, style = \"filled\", color=\"indianred\",", strng)
        }
  
        # set col
        if(!is.na(indat2[i,colour_col])){
         strng <- gsub("white", indat2[i,colour_col], strng)
        }
  
  } else {
  
  strng <- sprintf('%s\n%s  [ shape=record, style = \"filled\", fillcolor=\"white\", label="{{ { Name | Description } | { %s | %s } }}"]\n%s\n\n',
                   inputs_listed, name2paste, name, desc, outputs_listed
                   )
        # set col
        if(!is.na(indat2[i,colour_col])){
         strng <- gsub("white", indat2[i,colour_col], strng)
        }
  
  }
  
  
        nodes_graph <- paste(nodes_graph, strng, "\n")
  }
  #  cat(nodes_graph)
  }
  nodes_graph <- paste("digraph transformations {\n\n",
                       nodes_graph,
                       "}\n")
  
  cat('# to run this graph
  sink("file_name.dot")
  cat(nodes_object)
  sink()')
  cat('# If graphviz is installed and on linux
  system("dot -Tpdf file_name.dot -o file_name.pdf")
  system("dot -Tpng file_name.dot -o file_name.png")
  ')
  cat('# if not
  DiagrammeR::grViz("file_name.dot")
  ')
  
  return(nodes_graph)
  }
  
#+end_src

*** COMMENT test-newnode

#+begin_src R :session *R* :tangle tests/test-newnode.r :exports none :eval yes
  library(devtools)
  #document()
  #load_all()
  install_github("ivanhanigan/disentangle", ref = "gh-pages")
  library(disentangle)
  #source("R/newnode.r")
  library(stringr)
  ## filesList <- read.csv(textConnection('
  ## CLUSTER ,  STEP    , INPUTS                    , OUTPUTS                                , DESCRIPTION                      
  ## A  ,  Ivan\'s siteIDs      , "Ivan\'s GPS, helicopter"          , "Ivan\'s spatial, site doco"                 , "Ivan\'s latitude and longitude of sites"
  ## A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
  ## B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
  ## B  ,  biomass      , spatial                   , biomass_g                              ,                                  
  ## B  ,  correlations , "exposures,trapped_no,biomass_g" , report1                         , A study we published             
  ## C  ,  paper1       , report1                   , "open access repository, data package" ,                                  
  ## D  ,  biomass revision, new estimates          , biomass_g                              , this came late
  ## '), stringsAsFactors = F, strip.white = T)
  #write.csv(filesList, "fileTransformations.csv", row.names = F)
  filesList <- read.csv("fileTransformations.csv", stringsAsFactors = F, strip.white = T)
  
  str(filesList)
  nrow(filesList)
  filesList$COL <- c("red", "yellow", "blue", "green", "orange", "blue", "red", "red", "blue")
  filesList[,c("STEP", "COL")]
  
  nodes <- newnode(
    indat = filesList
    ,
    names_col = "STEP"
    ,
    in_col = "INPUTS"
    ,
    out_col = "OUTPUTS"
    ,
    desc_col = "DESCRIPTION"
    ,
    clusters_col = "CLUSTER"
    ,
    todo_col = "STATUS"
    ,
    nchar_to_snip = 40
    ,
    colour_col = "COL"
    )
  DiagrammeR::grViz(nodes)
  
  sink("fileTransformations.dot")
  cat(nodes)
  sink()
  #DiagrammeR::grViz("fileTransformations.dot")
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  browseURL("fileTransformations.pdf")
  
  
#+end_src

** 2015-11-17-a-picture-of-the-newnode-function-for-workflow-visualisation-in-r
#+name:a-picture-of-the-newnode-function-for-workflow-visualisation-in-r-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-11-17-a-picture-of-the-newnode-function-for-workflow-visualisation-in-r.md :exports none :eval no :padline no
  ---
  name: a-picture-of-the-newnode-function-for-workflow-visualisation-in-r
  layout: post
  title: A picture of the newnode function for workflow visualisation in R
  date: 2015-11-17
  categories:
  - disentangle 
  tags:
  - workflow tools
  ---

  ## Newnode: An R function for visualising workflows
  
  The scientific workflow concept is essentially a pipeline.  The method step is the key atomic unit of a scientific pipeline.  It consists of inputs, outputs and a rationale for why the step is taken.
  
  A simple way to keep track of the steps, inputs and outputs is shown in Table below:
  
  ```
  CLUSTER ,  STEP       , INPUTS                   , OUTPUTS                   
  A  ,       Step1      , "Input 1, Input 2"       , Output 1                 
  A  ,       Step2      ,  Input 3                 , Output 2                   
  B  ,       Step3      , "Output 1, Output 2"     , Output 3                  
  ```  
  
  The steps and data listed in this way can be visualised.
  To achieve this an R function was written as part of my PhD project and is distributed in the R package available on Github [https://github.com/ivanhanigan/disentangle](https://github.com/ivanhanigan/disentangle).
  
  This is the `newnode` function.  The function returns a string of text
  written in the `dot` language which can be rendered in R using the
  `DiagrammeR` package, or the standalone `graphviz` package.   This creates the graph of this pipeline shown in Figure below.  Note that a new field was added for Descriptions as these are highly recommended.
    
    
  ![/images/steps_basic_rstudio-test.png](/images/steps_basic_rstudio-test.png)    
#+end_src

** R-newnode_rmd
*** COMMENT newnode-code
#+name:newnode_rmd
#+begin_src R :session *R* :tangle R/newnode_rmd.R :exports none :eval no
  #' @title newnode_rmd
  #' @name newnode_rmd
  #' @param indat the input data.frame
  #' @param names_col the name of each edge (the boxes)
  #' @param in_col the name of the nodes that are inputs to each edge with comma seperated vals.  whitespace will be stripped
  #' @param out_col the nodes that are outputs of each edge.
  #' @param desc_col description
  #' @param clusters_col optional column identifying clusters
  #' @param todo_col optional column with TODO status (DONE and WONTDO will be white, others are red)
  #' @return character string object that has the DOT language representatio of the input
  newnode_rmd <- function(indat = NULL,names_col = NULL,in_col = NULL,out_col = NULL,desc_col = NULL,clusters_col = NULL, todo_col = NULL, nchar_to_snip = 40){
  if (is.null(names_col)) stop("Names of the steps are needed")
  if (is.null(in_col)) stop("Inputs are needed")
  if (is.null(out_col)) stop("Outputs are needed")
  if (is.null(desc_col)){
    print("Descriptions are strongly recommended, we\'re creating empty records for you")
    indat$descriptions <- ""
    desc_col <- "descriptions"
  }
  
    indat2 <- indat
    nodes_graph  <- ""
  #  indat2
      for(i in 1:nrow(indat2)){
        # i <- 1
        #i
        indat2[i,]
        name <- indat2[i,names_col]
        desc <- indat2[i,desc_col]
  
        #if(nchar(name) > 140) print("that's a long name. consider shortening this")
        #if(nchar(desc) > nchar_to_snip) desc <- paste(substr(desc, 1, nchar_to_snip), "[...]")
        name2paste <- paste('', name, ':', sep = "")
        inputs <- unlist(lapply(strsplit(indat2[i,in_col], ","), str_trim))      
        inputs <- paste('## inputs\n',
                        paste("- ", inputs, '\n',
                                              sep = "", collapse = "")
                        , collapse = "\n", sep = "")
  #      cat(inputs)
  #      inputs_listed <- paste(inputs, name2paste, sep = ' -> ', collapse = "\n")
        #cat(inputs_listed)
        outputs <- unlist(lapply(strsplit(indat2[i,out_col], ","), str_trim))
        outputs <- paste('## outputs\n',
                        paste("- ", outputs, '\n',
                                              sep = "", collapse = "")
                        , collapse = "\n", sep = "")      
  #      outputs <- paste('"', outputs, '"', sep = "")  
  #      outputs_listed <- paste(name2paste, outputs, sep = ' -> ', collapse = "\n")
  #      cat(outputs)
  ## if(!is.na(todo_col)){
  ##   status <- indat2[i,todo_col]
         
  ## strng <- sprintf('%s\n%s  [ shape=record, label="{{ { Name | Description | Status } | { %s | %s | %s } }}"]\n%s\n\n',
  ##                  inputs_listed, name2paste, name, desc, status, outputs_listed
  ##                  )
  ##       # cat(strng)
  ##       if(!status %in% c("DONE", "WONTDO")){ 
  ##         strng <- gsub("shape=record,", "shape=record, style = \"filled\", color=\"indianred\",", strng)
  ##       }
  ## } else {
    
  strng <- sprintf('# %s\n## Description\n\n%s\n\n%s\n\n%s\n\n',
                   name2paste,  desc, inputs, outputs
                   )
  #cat(strng)
  #nodes_graph  <- ""      
        nodes_graph <- paste(nodes_graph, strng, "\n", sep = '')
  #}
  }
  nodes_graph <- paste("---\ntitle: Test report of Workflow
  author: Ivan Hanigan
  output:
    html_document:
      toc: true
      theme: united
      number_sections: yes
  documentclass: article
  classoption: a4paper\n---\n",
                       nodes_graph,
                       "\n", sep = '')
  
  #help_text <- c('# to run this graph
  #sink("fileTransformations.dot")
  #cat(nodes_graph)
  #sink()
  #system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  #')
  #cat(help_text)
  return(nodes_graph)
  }
  
#+end_src

*** COMMENT test-df-input-code3
#+name:test-df-input
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test-df-input####
  library(devtools)
  load_all()
  library(stringr)
  ## filesList <- read.csv(textConnection('
  ## CLUSTER ,  FILE    , INPUTS                    , OUTPUTS                                , DESCRIPTION                      
  ## A  ,  siteIDs      , "GPS, helicopter"          , "spatial, site doco"                 , latitude and longitude of sites  
  ## A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
  ## B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
  ## B  ,  biomass      , spatial                   , biomass_g                              ,                                  
  ## B  ,  correlations , "exposures,trapped_no,biomass_g" , report1                         , A study we published             
  ## C  ,  paper1       , report1                   , "open access repository, data package" ,                                  
  ## D  ,  biomass revision, new estimates          , biomass_g                              , this came late
  ## '), stringsAsFactors = F, strip.white = T)
  #write.csv(filesList, "fileTransformations.csv", row.names = F)
  filesList <- read.csv("fileTransformations.csv", stringsAsFactors = F, strip.white = T)
  
  str(filesList)
  # filesList
  
  nodes_graphy <- newnode_rmd(
    indat = filesList
    ,
    names_col = "STEP"
    ,
    in_col = "INPUTS"
    ,
    out_col = "OUTPUTS"
    ,
    desc_col = "DESCRIPTION"
    ,
    clusters_col = "CLUSTER"
    ,
    todo_col = NA #"STATUS"
    ,
    nchar_to_snip = 40
    )
  
  sink("fileTransformations.Rmd")
  cat(nodes_graphy)
  sink()
  #system("dot -Tpng fileTransformations.dot -o fileTransformations.png")
  
#+end_src

** R-newnode_csv
*** COMMENT newnode_csv-code
#+name:newnode_csv
#+begin_src R :session *R* :tangle R/newnode_csv.R :exports none :eval yes
  #' @title newnode_csv
  #' @name newnode_csv
  #' @param indat the input data.frame
  #' @param names_col the name of each edge (the boxes)
  #' @param in_col the name of the nodes that are inputs to each edge with comma seperated vals.  whitespace will be stripped
  #' @param out_col the nodes that are outputs of each edge.
  #' @param clusters_col optional
  #' @return dataframe representation of the inputs and outputs
  newnode_csv <- function(indat = NULL,names_col = NULL,in_col = NULL,out_col = NULL, clusters_col = NULL){
  if (is.null(names_col)) stop("Names of the steps are needed")
  if (is.null(in_col)) stop("Inputs are needed")
  if (is.null(out_col)) stop("Outputs are needed")
  
  
  if(!is.null(clusters_col)){
    cluster_ids <- names(table(indat[,clusters_col]))
  #cluster_ids
  
    nodes_graph  <- as.data.frame(matrix(NA, ncol = 4, nrow = 0))
    names(nodes_graph) <- c("cluster", "name", "in_or_out", "value")
  
  for(cluster_i in cluster_ids){
    # cluster_i <- cluster_ids[1]
    indat2 <- indat[indat[,clusters_col] == cluster_i,]  
      for(i in 1:nrow(indat2)){
        indat2[i,]
        name <- indat2[i,names_col]
        name2paste <- paste('', name, '', sep = "")
        inputs <- data.frame(in_or_out = "input", value = unlist(lapply(strsplit(indat2[i,in_col], ","), str_trim)))
        outputs <- data.frame(in_or_out = "output", value = unlist(lapply(strsplit(indat2[i,out_col], ","), str_trim)))
        inout <- rbind(inputs, outputs)      
        strng <- data.frame(cluster= cluster_i,
                      name = name2paste,                    
                      inout = inout)
        names(strng) <- c("cluster", "name", "in_or_out", "value")
        nodes_graph <- rbind(nodes_graph, strng)
      }
    } 
  ######################################
  # option 2 no cluster
  } else { 
    indat2 <- indat
    nodes_graph  <- as.data.frame(matrix(NA, ncol = 3, nrow = 0))
    names(nodes_graph) <- c("name", "in_or_out", "value")
  #  indat2
      for(i in 1:nrow(indat2)){
        # i <- 1
        #i
        indat2[i,]
        name <- indat2[i,names_col]
        #desc <- indat2[i,desc_col]
  
        #if(nchar(name) > 140) print("that's a long name. consider shortening this")
        #if(nchar(desc) > nchar_to_snip) desc <- paste(substr(desc, 1, nchar_to_snip), "[...]")
        name2paste <- paste('', name, '', sep = "")
        inputs <- data.frame(in_or_out = "input", value = unlist(lapply(strsplit(indat2[i,in_col], ","), str_trim)))
        
  #      inputs
        
        outputs <- data.frame(in_or_out = "output", value = unlist(lapply(strsplit(indat2[i,out_col], ","), str_trim)))
  #      outputs
        inout <- rbind(inputs, outputs)      
  
  strng <- data.frame(name = name2paste,                    
                      inout = inout
                   )
  #strng
        names(strng) <- c("name", "in_or_out", "value")
  #nodes_graph  <- ""      
        nodes_graph <- rbind(nodes_graph, strng)
  #
        }
  #nodes_graph
  }
  return(nodes_graph)
  }
  
#+end_src

#+RESULTS: newnode_csv

*** COMMENT test-df-input-code3
#+name:test-df-input
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test-df-input####
  library(devtools)
  load_all()
  library(stringr)
  #dat <- read.csv("~/Dropbox/projects/air_pollution_ucrh/Neighbourhood_staging/BME_steps.csv", stringsAsFactors = F, strip.white = T)
  
  dat <- read.csv(textConnection('
  cluster ,  step    , inputs                    , outputs                                , description                      
  A  ,  siteIDs      , "GPS, helicopter"          , "spatial, site doco"                 , latitude and longitude of sites  
  A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
  B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
  B  ,  biomass      , spatial                   , biomass_g                              ,                                  
  B  ,  correlations , "exposures,trapped_no,biomass_g" , report1                         , A study we published             
  C  ,  paper1       , report1                   , "open access repository, data package" ,                                  
  D  ,  biomass revision, new estimates          , biomass_g                              , this came late
  '), stringsAsFactors = F, strip.white = T)
  
  str(dat)
  # dat
  
  nodes_graphy <- newnode_csv(
    indat = dat
    ,
    names_col = "step"
    ,
    in_col = "inputs"
    ,
    out_col = "outputs"
    ,
    clusters_col = 'cluster'
    )
  
  
  str(nodes_graphy)
  nodes_graphy
  
  write.csv(nodes_graphy, "tests/test-newnode_csv_output.csv", row.names = F, quote=F)
  
#+end_src



** R-newnode_org_beamer
*** COMMENT newnode_org_beamer
#+name:newnode_rmd
#+begin_src R :session *R* :tangle R/newnode_org_beamer.R :exports none :eval no
  #' @title newnode_rmd
  #' @name newnode_rmd
  #' @param indat the input data.frame
  #' @param names_col the name of each edge (the boxes)
  #' @param in_col the name of the nodes that are inputs to each edge with comma seperated vals.  whitespace will be stripped
  #' @param out_col the nodes that are outputs of each edge.
  #' @param desc_col description
  #' @param clusters_col optional column identifying clusters
  #' @param todo_col optional column with TODO status (DONE and WONTDO will be white, others are red)
  #' @return character string object that has the DOT language representatio of the input
  #' @export
  newnode_org_beamer <- function(indat = NULL,names_col = NULL,in_col = NULL,out_col = NULL,desc_col = NULL,clusters_col = NULL, todo_col = NULL, nchar_to_snip = 40){
  if (is.null(names_col)) stop("Names of the steps are needed")
  if (is.null(in_col)) stop("Inputs are needed")
  if (is.null(out_col)) stop("Outputs are needed")
  if (is.null(desc_col)){
    print("Descriptions are strongly recommended, we\'re creating empty records for you")
    indat$descriptions <- ""
    desc_col <- "descriptions"
  }
  
    indat2 <- indat
    nodes_graph  <- ""
  #  indat2
      for(i in 1:nrow(indat2)){
        #i <- 2
        #i
        indat2[i,]
        name <- indat2[i,names_col]
        desc <- indat2[i,desc_col]
  
        #if(nchar(name) > 140) print("that's a long name. consider shortening this")
        #if(nchar(desc) > nchar_to_snip) desc <- paste(substr(desc, 1, nchar_to_snip), "[...]")
        name2paste <- paste('', name, ':', sep = "")
        inputs <- unlist(lapply(strsplit(indat2[i,in_col], ","), str_trim))      
        inputs <- paste('## inputs\n',
                        paste("- ", inputs, '\n',
                                              sep = "", collapse = "")
                        , collapse = "\n", sep = "")
  #      cat(inputs)
  #      inputs_listed <- paste(inputs, name2paste, sep = ' -> ', collapse = "\n")
        #cat(inputs_listed)
        outputs <- unlist(lapply(strsplit(indat2[i,out_col], ","), str_trim))
        outputs <- paste('## outputs\n',
                        paste("- ", outputs, '\n',
                                              sep = "", collapse = "")
                        , collapse = "\n", sep = "")      
  #      outputs <- paste('"', outputs, '"', sep = "")  
  #      outputs_listed <- paste(name2paste, outputs, sep = ' -> ', collapse = "\n")
  #      cat(outputs)
  ## if(!is.na(todo_col)){
  ##   status <- indat2[i,todo_col]
         
  ## strng <- sprintf('%s\n%s  [ shape=record, label="{{ { Name | Description | Status } | { %s | %s | %s } }}"]\n%s\n\n',
  ##                  inputs_listed, name2paste, name, desc, status, outputs_listed
  ##                  )
  ##       # cat(strng)
  ##       if(!status %in% c("DONE", "WONTDO")){ 
  ##         strng <- gsub("shape=record,", "shape=record, style = \"filled\", color=\"indianred\",", strng)
  ##       }
  ## } else {
  strng <- sprintf('** %s\n# %s\n## Description\n\n%s\n\n%s\n\n%s\n\n',
                   name2paste, name2paste,  desc, inputs, outputs
                   )
  # I assume graph and graphsiz, but not sure about case
  indat3 <- indat2      
  names(indat3)  <- toupper(names(indat2))      
  if(indat3[i,"GRAPH"] != ""){
  strng <- sprintf('%s\n** %s graph\n# %s graph\n\n\\begin{figure}[!h]\n\\centering\n\\includegraphics[width=%s\\textwidth]{%s}\n\\end{figure}\n',
                   strng,
                   name2paste, name2paste,
                   indat3[i,"GRAPH_SIZE"], indat3[i,"GRAPH"]
                   )
  
  }
  #cat(strng)
  #nodes_graph  <- ""      
        nodes_graph <- paste(nodes_graph, strng, "\n", sep = '')
        cat(nodes_graph)
  #}
  }
  nodes_graph <- paste("---\ntitle: Test report of Workflow
  author: Ivan Hanigan
  output:
    html_document:
      toc: true
      theme: united
      number_sections: yes
  documentclass: article
  classoption: a4paper\n---\n",
                       nodes_graph,
                       "\n", sep = '')
  
  #help_text <- c('# to run this graph
  #sink("fileTransformations.dot")
  #cat(nodes_graph)
  #sink()
  #system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  #')
  #cat(help_text)
  return(nodes_graph)
  }
  
#+end_src

*** COMMENT test-df-input-code3
#+name:test-df-input
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test-df-input####
  library(devtools)
  load_all()
  library(stringr)
  ## filesList <- read.csv(textConnection('
  ## CLUSTER ,  FILE    , INPUTS                    , OUTPUTS                                , DESCRIPTION                      
  ## A  ,  siteIDs      , "GPS, helicopter"          , "spatial, site doco"                 , latitude and longitude of sites  
  ## A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
  ## B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
  ## B  ,  biomass      , spatial                   , biomass_g                              ,                                  
  ## B  ,  correlations , "exposures,trapped_no,biomass_g" , report1                         , A study we published             
  ## C  ,  paper1       , report1                   , "open access repository, data package" ,                                  
  ## D  ,  biomass revision, new estimates          , biomass_g                              , this came late
  ## '), stringsAsFactors = F, strip.white = T)
  #write.csv(filesList, "fileTransformations.csv", row.names = F)
  filesList <- read.csv("fileTransformations.csv", stringsAsFactors = F, strip.white = T)
  dir()
  str(filesList)
  # filesList
  
  txt2 <- newnode_org_beamer(
    indat = filesList
    ,
    names_col = "STEP"
    ,
    in_col = "INPUTS"
    ,
    out_col = "OUTPUTS"
    ,
    desc_col = "DESCRIPTION"
    ,
    clusters_col = "CLUSTER"
    ,
    todo_col = NA #"STATUS"
    ,
    nchar_to_snip = 40
    )
  
  txt2 <-  gsub("## Description\n", " ", txt2)
  txt2 <-  gsub("## outputs", "\n\n## Outputs:", txt2)
  txt2 <-  gsub("## inputs", "\n\n## Inputs:", txt2)
  txt2 <-  gsub("classoption: a4paper", "* Workflow steps:", txt2)
  txt2 <-  gsub("---", "## COMMENT snip", txt2)
  
  txt2 <-  gsub("##", "****", txt2)
  txt2 <-  gsub("#", "***", txt2)
  
  cat(txt2)
  #   '+latex_header: \\AtBeginSection[]{\\begin{frame}<beamer>\\frametitle{Topic}\\tableofcontents[currentsection]\\end{frame}}  
  header <- paste("'+TITLE:     A test presentation
  '+AUTHOR:    Ivan Hanigan
  '+EMAIL:     ivan.hanigan@gmail.com
  '+DATE:      ",Sys.Date(),"
  '+DESCRIPTION:
  '+KEYWORDS:
  '+LANGUAGE:  en
  '+OPTIONS:   H:3 num:t toc:t \\n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
  '+OPTIONS:   TeX:t LaTeX:t skip:ninl d:nil todo:t pri:nil tags:not-in-toc
  '+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
  '+EXPORT_SELECT_TAGS: export
  '+EXPORT_EXCLUDE_TAGS: noexport
  '+LINK_UP:   
  '+LINK_HOME: 
  '+XSLT:
  '+LaTeX_CLASS: beamer
  '+BEAMER_HEADER_EXTRA: \\institute[UCRH]{University of Sydney}
  '+LaTeX_CLASS_OPTIONS: [bigger]
  '+latex_header: \\mode<beamer>{\\usetheme{default}}
  '+latex_header: \\usecolortheme{beaver}
  '+latex_header: \\usepackage{verbatim}
  '+latex_header: \\setbeamertemplate{headline}{}
  '+latex_header: \\setbeamertemplate{navigation symbols}{}
  '+BEAMER_FRAME_LEVEL: 3
  '+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
  ,* Introduction
  ,** Introduction
  ,*** Introduction
  - The idea is that a table of steps, inputs and outputs can produce a graph, script, presentation and report
  ,** Aims
  ,*** Aims
  This is a test
  ", sep = "")
  dir()
  
  sink("fileTransformations_pres.org")
  cat(gsub("'", "#", header))
  cat(txt2)
  sink()
  
#+end_src

    
** 2013-11-25-setting-up-a-workflow-script
#+name:project-template-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-11-25-setting-up-a-workflow-with-code-chunks.md :exports none :eval no :padline no
  ---
  name: 2013-11-25-setting-up-a-workflow-with-code-chunks
  layout: post
  title: Setting Up A Workflow Script With Code Chunks
  date: 2013-11-25
  categories:
  - research methods
  ---
  
  This post describes some ideas and techniques I use to set up a "workflow script".  I use this term to refer to the structured combination of code, data and narrative that make an executable Reproducible Research Report (RRR).
  
  A lot of these ideas are inpsired by  a great paper by Kieran Healy called  "Choosing Your Workflow Applications" available at [https://github.com/kjhealy/workflow-paper](https://github.com/kjhealy/workflow-paper) to accompany [his Emacs Starter Kit](http://kieranhealyo.org/resources/emacs-starter-kit.html). My shortened version of his main points are:
  
  - 1 use a good code editor
  - 2 analyse data with scripts
  - 3 store your work simply and document it properly
  - 4 use a version control system
  - 5 Automate back ups 
  - 6 Avoid distracting gadgets
  
  #### Here's my current approach in each of these categories
  - 1 use Emacs with Orgmode (and kjhealy's drop-in set of useful defaults)
  - 2 Scripts that utilise the literate programming technique of mixing Code Chunks in with descriptive prose
  - 3 John Myles White's ProjectTemplate R Package and Josh Riech's LCFD paradigm 
  - 4 git and GitHub for version control
  
  5 Automated Backups and 6 Avoiding Gadgets are still somethings I find challenging
  
  #### 1 Use a good code editor
  I like using Emacs with Orgmode.
  
  - I have previously tried a variety of code editors from Tinn-r, NppToR, Rstudio and Eclipse.  
  - Emacs with Orgmode suits me the most because it has a great number of features especially the linkage with LaTeX or HTML export
  - A key reference to look at for reasons why Emacs is so good for scientific work is Eric Schulte et al ["A Multi-Language Computing Environment for Literate Programming"](www.jstatsoft.org/v46/i03‎) 
  - Here is a [link to a great orgmode description](http://doc.norang.ca/org-mode.html)
  - (this guy spends a lot of time on tweaking his set up)
  
  #### 2 Analyse data with Scripts (stitch together code chunks)
  I use Scripts but prefer to think of them as stitched together Code Chunks with prose into Compendia.
  
  - Compendia are documents that weave together Code and Prose into an executable report
  - The underlying philosophy is called Reproducible Research Reports
  - A very useful tool is a keyboard shortcut to quickly create a chunk for code
  - so you can be writing parts of the report like this: "Blah Blah Blah as shown in Figure X and Table Y"
  - then just hit the correct keys and WHAMM-O there is a new chunk ready for the code that creates Figure X and Table Y to be written.
  - Here is how I use Emacs to achieve this (the other editors I mentioned above also have the abiltiy to do this too).  The IPython Notebook does this stuff too but calls chunks "cells" for some reason.
  
  #### Emacs Code: Put this into the ~/.emacs.d/init.el file
      (define-skeleton chunk-skeleton
        "Info for a code chunk."
        "Title: "
        "*** " str "-code\n"
        "#+name:" str "\n"
        "#+begin_src R :session *R* :tangle src/" str ".r :exports reports :eval no\n"
        "#### name:" str " ####\n"
        "\n"
        "#+end_src\n"
      )
      (global-set-key [?\C-x ?\C-\\] 'chunk-skeleton)
  
  #### Using the Emacs Shortcut
  - now whenever you type Control-x control-\ a new code chunk will appear
  - you'll be typing "blah blah blah" and think I need a figure or table, just hit it.
  - move into the empty section and add some code
  - you can hit C-c ' to enter a org-babel code execution session that will be able to send these line by line to an R session
  - or within the main org buffer if your eval flag is set to yes then you can run the entire chunk (and return tabular output to the doc) using C-c C-c
  - To export the code chunks and create the modular code scripts without the narrative prose use C-c C-v t
  - this is called "tangling" and the chunks will be written out to the file specified in the chunk header ":tangle" flag
  
  #### Compiling the resulting Compendium
  - Emacs uses LaTeX or HTML to produce the Report
  - I find both of these outputs very pleasing
  - to compile to TEX use C-c C-e d
  - for HTML use C-c C-e h (FOR CODE HIGHLIGHTING INSTALL htmlize.el)
  - these commands will also evaluate all the chunks where ":eval" = yes to load the data and calculate the results fresh. 
  - AWESOME!
      
  #### 3 Store your work simply and document it properly
  - I use the [ProjectTemplate R package](http://www.johnmyleswhite.com/notebook/2010/08/26/projecttemplate/) to organise my code sections into modules
  - These modules are organised into the Reichian LCFD paradigm described first [on StackOverflow here](http://stackoverflow.com/a/1434424), and encoded into [the makeProject R package](http://cran.r-project.org/web/packages/makeProject/makeProject.pdf)
  - documentation is within the main orgmode script
  - data documentation is a whole other universe that I will deal with in a separate post
  
  #### 4 use a version control system using git and github
      # once you have the project via R
      R
      require(ProjectTemplate)
      create.project("AwesomeProject", minimal = T)
      q()
      # use the shell to start a git repo
      cd AwesomeProject
      git init
      # and commit the TODO
      git add TODO
      git commit -m "first commit"
      # tada!
  
  - Emacs can now be used to manage the git repo using the C-x g command
  - Rstudio has a really nice GUI for doing this inside it;s Project management interface too.
  
  #### Using Github or another Git Server
  - You can easily set up a Github repo for this now but it will be public
  - Alternatative is to set up your own private Git server.  I followed [these instructions to Running a Simple Git Server Using SSH](http://blog.goosoftware.co.uk/2012/02/07/quick-git-server/)
  - Either way once you have set up your remote git repo you need to set the remote tracking
  
  #### Git Code:
      cd /path/to/local/git/repo
      git remote add origin git@github-or-other-server:myname/myproject.git
      git push origin master
  
  #### 5 Automate back ups AND 6 Avoid distracting gadgets
  - OMG backups stress me out
  - ideally I would follow [this advice because "when it comes to losing your data the universe tends toward maximum irony. Don't push it."](http://www.jwz.org/blog/2007/09/psa-backups/)
  - But I don;t fully comply
  - Instead I generally use Dropbox for  basic project management admin stuff
  - I use github for code projects I am happy to share, I also pay for 10 private repos 
  - I Set up a git server at my workplace for extra projects but this is on a test server that is not backed up, and I am not really happy about this
  - In terms of Distracting Gadgets, I think that with the current tempo of new innovations related to new software tools for this type of work I should keep trying new things but I have pretty much settled into a comfortable zone with the gadgets I described here. 
  
  #### Conclusions
  - This is how I've worked for a couple of years
  - I find it very enjoyable, mostly productive but prone to the distractions of "distractions by gadgets"
  - The main thing I want to point out is the usage of Code Chunks in RRR scripts.
  - These things are awesome.
      
#+end_src

** 2013-12-01-graphviz-automagic-flowcharts
*** workflow_steps-code
#+begin_src R :session *R* :tangle no :exports reports :eval no :padline no
# ~/tools/transformations/workflow_steps.txt
Transformation
	description
		the thing 
	inputs
		the thing before
		another thing
	output
		the next thing
	notes
		the thing is this other thing	this is a really long description 
		blah blah asdfasdfasdfasdfasdfa 

Transformation
	description
		yet another thing
	inputs
		the next thing
	output
		a final thing
	notes
		this is a note

#+end_src

*** post
#+name:graphviz-automagic-flowcharts-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-12-01-graphviz-automagic-flowcharts.md :exports none :eval no :padline no
  ---
  name: 2013-12-01-graphviz-automagic-flowcharts
  layout: post
  title: graphviz-automagic-flowcharts
  date: 2013-12-01
  categories:
  - research methods 
  ---
  
  - Back in 2009 Joseph Guillaume worked with me on a complicated workflow
  - He came up with this python script to help keep track of the steps
  - The simple text file is a list of transformations, inputs and output
  - It is converted to the right format and graphviz creates a html page with pop-outs
  
  #### Simple text list
      Transformation
              description
                      the thing 
              inputs
                      the thing before
                      another thing
              output
                      the next thing
              notes
                      the thing is this other thing   this is a really long description 
                      blah blah asdfasdfasdfasdfasdfa 
       
      Transformation
              description
                      yet another thing
              inputs
                      the next thing
              output
                      a final thing
              notes
                      this is a note
  
  <p></p>
  
  - keep this in your work directory and update it whenever you add a step to the workflow
  - the list can be as big as you like (hundreds of steps), and entered in any order, the inputs/output relationships determine how the graph looks at the end
  - to run the script just do the one line

  #### Python code: run
      python transformations.py workflow_steps.txt index
  
  <p></p>
  - open the html page and click on a square box to bring up the pop-out
  - short text is shown, long text is replaced by an ellipse and only shown in pop-out
  
  #### Conclusions  
  - I've popped the script up as a [Github repo](https://github.com/ivanhanigan/transformations)
  - The example is in the [gh-pages branch](http://ivanhanigan.github.io/transformations/)
  
#+end_src

** 2013-12-24-a-few-best-practices-for-statistical-programming
#+name: a-few-best-practices-for-statistical-programming-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-12-24-a-few-best-practices-for-statistical-programming.md :exports none :eval no :padline no
---
name: 2013-12-24-a-few-best-practices-for-statistical-programming
layout: post
title: a-few-best-practices-for-statistical-programming
date: 2013-12-24
categories:
- research methods
---

- John Myles White invented the ProjectTemplate R Package
- This is a great application that helps streamline the process of creating a data analysis project
- Recently John posted about some tips for [best practices for statistical programming](http://www.johnmyleswhite.com/notebook/2013/01/24/writing-better-statistical-programs-in-r/)


#### Best Practices for Statistical Programming

- Write Out a Directed Acyclic Graph (DAG)
- Vectorize Your Operations
- Profile your code and understand where it spends its time
- Generate Data and Fit Models
- Correctness: always ensure that code infers  parameters of models given simulated data with known parameters.


#### Additional suggestions

- Unit Testing (use testthat)
- Create modular code with discrete chunks
- Write functions as much as possible, put these into a personal 'misc' package
   
#+end_src

** 2015-10-17-how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology
#+name:how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-17-how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology.md :exports none :eval no :padline no
---
name: how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology
layout: post
title: How To Effectively Implement Electronic Lab Notebooks In Epidemiology
date: 2015-10-17
categories:
- disentangle
- Workflow tools
---

- It is often stated in the literature that an electronic lab notebook is a core component of reproducible research
- For example the following is from Buck, S. (2015). Solving reproducibility. Science, 348(6242), 1403–1403. [http://dx.doi.org/10.1126/science.aac8041](http://dx.doi.org/10.1126/science.aac8041)

```
one of the most effective ways to promote high-quality science 
is to create free open-source tools that give scientists
easier and cheaper ways to incorporate transparency
into their daily workflow: from open lab notebooks, to
software that tracks every version of a data set, to dynamic 
document generation.
```
  
<p></p>

- But I have struggled to operationalise the lab notebook in the epidemiology projects I work in
- Here are some notes based on my recent readings and attempts with a new team

## Modularised lab notebooks 

There seem to be a small number of components to a lab notebook that can be defined as:

- Data management plan
- Workplan
- Worklog
- Workflow
- Distribution

One thing I think is important is to have levels of organisation in a hierarchy:

- Macro level: The 'Research Programme' level is about the entire breadth of the projects in the group.
    + Data Management Plan: including managing the computers and a Data Inventory
    + _Personal Workplan and Worklog_: this is an overview of things I do, plan to do or learn along the way (this is for the high level things like planning professional development, or a holiday)
    + This is operationalised in the blog you are reading right now.
- Meso level: the 'Research Project' level is about a single study, or a small group of studies based around a core dataset or Concept
    + This is the level that you might write up a manuscript for a journal, or report to a client
    + _Project workplan_: at this level there may be high level information about the study design, hypotheses, resources and admin for managing relationships with a variety of collaborators 
    + _Worklog_: WS Noble [http://dx.doi.org/10.1371/journal.pcbi.1000424](http://dx.doi.org/10.1371/journal.pcbi.1000424) recommends that this be the main lab notebook for the analysts
    + He says 'This is a document that resides in the root of the results directory and that records your progress in detail. Entries in the notebook should be dated, and they should be relatively verbose, with links or embedded images or tables displaying the results of the experiments that you performed. In addition to de- scribing precisely what you did, the notebook should record your observations, conclusions, and ideas for future work'
- Micro level: the 'Experiment Results' level is about work you might do on a single day, or over a week
    + _Workflow_ scripts: At this level each 'experiment' is written up in chronological order, as entries to the Worklog at the meso level
    + Noble recommends 'create either a README file, in which I store every command line that I used while performing the experi- ment, or a driver script (I usually call this runall) that carries out the entire experiment automatically'... 
    + and 'you should end up with a file that is parallel to the lab notebook entry. The lab notebook contains a prose description of the exper- iment, whereas the driver script contains all the gory details.'
    + this is the level I usually think of managing the distribution side of things.  I will want to pack up the results and email to my collaborators, or decide on the one set of tables and figures to write into the manuscript for submission to a journal.  If this is accepted for publication, this is the one combined package of 'analytical data and code' that I would consider putting up online (to github) as supporting information for the paper.


  
#+end_src

** 2015-10-18-reproducible-research-pipelines-improve-description-of-method-steps
#+name:reproducible-research-pipelines-improve-description-of-method-steps-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-18-reproducible-research-pipelines-improve-description-of-method-steps.md :exports none :eval no :padline no
---
name: reproducible-research-pipelines-improve-description-of-method-steps
layout: post
title: Reproducible research pipelines improve description of method steps
date: 2015-10-18
categories:
- disentangle
- Workflow tools
---

Adequately documenting the methods and results of data analysis helps
safeguard against errors of execution and
interpretation. It is proposed that
reproducible research pipelines address the problem of
adequate documentation of data analysis. 

This is because they make it easy to
check the methods. Assumptions are easy to challenge and results
verified in new analyses. Reproducible research pipelines extend
traditional research.  They do this by encoding the steps in a
computer ‘scripting’ language and distributing the data and code with
publications.  Traditional research moves through the steps of
hypothesis and design, measured data, analytic data, computational
results (for figures, tables and numerical results), and reports (text
and formatted manuscript).

## Fundamental components of a reproducible research pipeline

The basic components of a pipeline are:

- Data Management Plan and Data Inventory
- Method steps
- Code
- Data storage
- Reports
- Distribution.

### Method Steps

The method step is the key atomic unit of a scientific pipeline.  It consists of inputs, outputs and a rationale for why the step is taken.

A simple way to keep track of the steps, inputs and outputs is shown in the Table below.

```
library(stringr)
steps <- read.csv(textConnection('
CLUSTER ,  STEP  , INPUTS                   , OUTPUTS                   
A  ,  Step1      , "Input 1, Input 2"       , "Output 1"                 
A  ,  Step2      ,  Input 3                  , Output 2                   
B  ,  Step3      , "Output 1, Output 2"      , Output 3                  
'), stringsAsFactors = F, strip.white = T)

```  
<p></p>

The steps and data listed in the Table above can be visualised.
To achieve this an R function was written as part of this PhD project and is distributed in my own R package available on Github [https://github.com/ivanhanigan/disentangle](https://github.com/ivanhanigan/disentangle).
This is the `newnode` function.  The function returns a string of text
written in the `dot` language which can be rendered in R using the
`DiagrammeR` package, or the standalone `graphviz` package.   This creates the graph of this pipeline shown in Figure below.  Note that a new field was added for Descriptions as these are highly recommended.

```
library(disentangle); library(stringr)
nodes <- newnode(indat = steps,   names_col = "STEP", in_col = "INPUTS",
  out_col = "OUTPUTS", 
  nchar_to_snip = 40)
sink("fig-basic.dot");
cat(nodes);
sink()
# or DiagrammeR::grViz(nodes)
system("dot -Tpdf fig-basic.dot -o fig-basic.pdf")

```
<p></p>

![/images/fig-basic.png](/images/fig-basic.png)
    
#+end_src

** 2015-10-23-keeping-an-electronic-lab-notebook-for-computational-statistics-projects
*** blog
#+name:keeping-an-electronic-lab-notebook-for-computational-statistics-projects-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-23-keeping-an-electronic-lab-notebook-for-computational-statistics-projects.md :exports none :eval no :padline no
---
name: keeping-an-electronic-lab-notebook-for-computational-statistics-projects
layout: post
title: Keeping an electronic lab notebook for computational statistics projects
date: 2015-10-23
categories:
- disentangle
- Workflow tools
---

- In my previous post on this topic [http://ivanhanigan.github.io/2015/10/how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology/](http://ivanhanigan.github.io/2015/10/how-to-effectively-implement-electronic-lab-notebooks-in-epidemiology/) I summarised some recommendations for electronic lab notebooks 
- I've collated from a variety of sources for managing computational statistics projects in a reproducible research Pipelines
- One thing I found while reading the literature around this topic is that the concepts are difficult to really grasp until I see them being used
- The example worklog from Scott Long was a good insight into his method, that I really got more out of the figure below than from descriptions of the concept
- screen shot taken from Long, S. (2012). Principles of Workflow in Data Analysis. Retrieved from [http://www.indiana.edu/~wim/docs/2012-long-slides.pdf](http://www.indiana.edu/~wim/docs/2012-long-slides.pdf) (accessed 2015-10-23).

![/images/worklog-long.png](/images/worklog-long.png)

<p></p>

- I added a red box around an important aspect of this example, the communication of gory details, that are often difficult to track if not using a notebook to log our work
- ARGH! indeed.

## Replication from Noble's description
- That looks good, but I also really liked the description in Noble's paper where scripts that do computations are linked to log entries
- This was something that I felt I wanted to see in action
- Without an example online, I had to have a go at creating one from the instructions
- I also had to make some modifications to the method because I want to have this set up work in a multidisciplinary team with some users on windows and others on linux, sharing the project on a network folder


## My paraphrasing of Noble's description

- Worklog: this is the main file, like a 'lab notebook' for the analyst to track their work.  This  document resides in the root of the project directory and records your progress in detail. Entries in the notebook should be dated, and they should be relatively verbose, with links or embedded images or tables displaying the results of the experiments that you performed. In addition to describing precisely what you did, the notebook should record your observations, conclusions, and ideas for future work
- For group work, this can also contain a 'working' folder for each person to store their messy day-to-day files that we don't want to clutter up the main folder (eg 'working_ivan')

Conventions I used for writing the worklog entries are:

- Names follow this structure [**] [date in ISO 8601 YYYY-MM-DD] [meeting/notes/results] [with/from UserName] [Re: topic shortname]
- 'meetings' are for both agenda preparation and also notes of discussion
- 'notes' are such things as emailed information or ad hoc Discovery
- 'results' are entries related to a section of the 'results' folder. That is, this kind of entry is in parallel to the results entry (see below), however the log contains a prose description of the experiment, whereas the results folder contains scripts etc of all the gory details.

## Tests
- I use Emacs on linux for most of my work but I need to share with windows users so tested out keeping the log in a MS word doc.  This got corrupted quickly I think because I edited in Libre office.
- I decided to try and just use a plain text format 
- text files created in Ubuntu are so difficult to understand (read) when opened in Windows' Notepad. No matter how many lines have been used, all the lines appear in the same one line.
To set the buffer coding to DOS style issue:

`M-x set-buffer-file-coding-system utf-8-dos`

### a couple of examples

- As this is a plain text document opening it in emacs will not automatically render it in the Orgmode fashion
- To achieve this the command is `M-x org-mode` and the file looks like below

![/images/worklog-ivan1.png](/images/worklog-ivan1.png)
<p></p>

- From here I can keep adding new entries at the bottom, and have a section for URGENT ACTION a the top
- Orgmode can expand the entries by moving to that line and hitting TAB, or use the command `C-u C-u C-u TAB` to expand all branches


![/images/worklog-ivan2.png](/images/worklog-ivan2.png)
<p></p>


## the 'Experiment Results' level is about work you might do on a single day, or over a week or two

- Each results subfolder would have workflow scripts that does the work
- At this level each 'experiment' is written up in chronological order
- It is recommended to store every command used while performing the experiment preferably as an executable script that carries out the entire experiment automatically
- you should end up with a file that is parallel to the worklog entry
- The worklog contains a prose description of the experiment, whereas the driver script contains all the gory details
- this is the level I usually think of the distribution side of things
-  You may want to pack up the results from one of these folders and email it to the collaborators, or decide on the one set of tables and figures to write into the manuscript for submission to a journal
- If this is accepted for publication, this is the one combined package of 'analytical data and code' that I would consider putting up online as supporting information for the paper.

### Example

- I followed Noble's advice to create a driver script to set up the folder structure:
- it is in my Github R package `disentangle`

```
> dir.create("exposures_blending")
> setwd("exposures_blending")
> disentangle::AdminTemplate()
[1] TRUE
> dir.create("results")
> setwd("results")
> makeProject::makeProject("2015-10-23-preliminary-modelling")
Creating Directories ...
Creating Code Files ...
Complete ...
> 
```

This populates the folders as shown below

![/images/worklog-ivan3.png](/images/worklog-ivan3.png)
<p></p>


## Conclusions

- I feel pretty happy with this as a replication of Noble's method
- My colleagues can look at my work and see a high level log that links to the gory details of day to day life in the trenches
- The only downside I can see at the moment is that my colleagues on windows will see a text file that is pretty dense, and will not be as easy to navigate as a word document (or emacs org file)
- Perhaps notepad++ can be used instead.  On my windows machine I did a quick experiment with NPP and found that under the Language menu > Define your language there is a method to define code folding with ** as the opening.  Just need to define a closing tag.  I experimented with '---' which might be good, but ultimately I don't think my colleagues are going to want to do this on their machines.
    
#+end_src
*** eg
**** 2015-10-21 meeting with PI and statistician Re: study design
- Meeting to identify the key deliverable will be a journal paper
- Study type will be time-series
- Data sets collected/used will be the survey and the weather data
- Study population inclusion and exclusion criteria will be discussed later
- Exposure variables will focus on extreme weather
- Outcome measure will be self reported health
- Covariates: need to consider potential confounders or effect modifiers from readings
- Action 1: statistician will email an expert they know from related project

**** 2015-10-22 notes from expert statistician Re: methods review for model
On Thu, Oct 22, 2015 at 6:01 PM, Expert <a.expert@somewhere.edu.au> wrote:
> Sounds good.
>
> Sent from my iPhone
>
> On 22 Oct 2015, at 14:00, "statistician" <statistician@somewhere.edu.au> wrote:
>
> Hi Expert,
> Just introducing you to Ivan Hannigan who works with me.

> We are working on a seed project looking at comparing/blending exposure models, 
> and Ivan will be driving our model in the project.

> Ivan may have a couple of stats questions re: the model build that are best
> addressed to you, hence this email.

> Any help you could offer would be much appreciated!
> Cheers,

**** 2015-10-23 results from Ivan Re: preliminary modelling
Project directory is:
~/projects/exposure_blending/results/2015-10-23-preliminary-modelling/

Aims:
- summarise understanding of model
- add in exploratory results of the test data 
- TODO write a short synopsis for discussion at next meeting

Email to statistician:

From: Ivan Hanigan <ivan.hanigan@gmail.com>
Date: Fri, Oct 23, 2015 at 12:10 PM
Subject: Blending exposures project, preliminary modelling
To: statistician@somewhere.edu.au


Dear Statistician,
Here are some these preliminary results.
I attach a Rmarkdown report of my explorations of the data and
my conceptual understanding about how the modelling workflow will proceed.  

I hope it makes sense.



** TODO tikz with hyperlinks
http://tex.stackexchange.com/a/36185
is in  tikz_hlink.tex 

# failed with hidelink undefined
http://tex.stackexchange.com/a/36111


* Reproducible Research Reports

** 2015-09-16-reproducible-research-reports-with-rmarkdown-and-latex-for-stylish-reports
#+name:reproducible-research-reports-with-rmarkdown-and-latex-for-stylish-reports-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-09-16-templates-needed-for-reproducible-research-reports-that-look-good.md :exports none :eval no :padline no
---
name: templates-needed-for-reproducible-research-reports-that-look-good
layout: post
title: Templates are Needed for Reproducible Research Reports (that Look Good)
date: 2015-09-16
categories:
- disentangle
- reproducible research reports
---

I read with interest the the Transparency and Openness Promotion (TOP) Committee templates for guidelines to enhance transparency in the science that journals publish.

#### Citation

    Supplementary Materials for Nosek, B. A., Alter, G., Banks, G. C.,
    Borsboom, D., Bowman, S. D., Breckler, S. J., … Yarkoni,
    T. (2015). Promoting an open research culture. Science, 348(6242),
    1422–1425. doi:10.1126/science.aab2374
      
<p></p>
  
I think though that guidelines like the suggestion to copy-paste bits of the manuscript leave a bit to be desired:
  
#### Quote;
  
    Authors document compliance by copy-pasting the relevant passages
    in the paper that address the question into the form. For example,
    when indicating how sample size was determined, authors copy paste
    into the form the text in the paper that describes how sample size
    was determined.
  
<p></p>
  
Reproducible Research Reports solve this problem by ensuring that the data preparation and analysis are executed in the same script that produces the manuscript, therefore a one-stop-shop for documentation of the entire study.
  
## There is a need for Templates of Reproducible Research Reports (that look good!)
  
Rstudio provides very easy support for these documents if you use R.  In particular the option of a menu button to create a new report populates that report with the required header information and some example script to work off.  But the easiest option does not look so good.  This is the Rmarkdown option and it is very user friendly in terms of the markup language needed to write the descriptive language around your analysis (mostly plain text with a few simple options for heading styles etc) rather than the Sweave option which leads to the full blown LaTeX markup language that is a lot more complicated.
  
#### Boilerplate Rmarkdown header from Rstudio:
    ---
    title: "Untitled"
    author: "Ivan C. Hanigan"
    date: "16 September 2015"
    output: html_document
    ---
  
<p></p>
  
This is great for quick reporting of work as you go, but I  primarily write for output that will be printed (e.g. pdf docs). More specifically, I need the concept of a page, and to have full control over the placement of table and figure ‘environments’, stuff that is easy in LaTeX (once you figure out some of the esoteric parts of that language).
  
To achieve a simple writing environment in Markdown but with the powerful layout options of LaTeX I reviewed this guys work but I think it takes it to an uneccessary level of complicated-ness
[https://github.com/jhollist/manuscriptPackage](https://github.com/jhollist/manuscriptPackage).

So I went back to some of the old Sweave/Latex templates I had put together and ported it into a markdown header. 

#### Boilerplate Rmarkdown header for pretty report
    ---
    title: "Untitled"
    author: "Ivan C. Hanigan"
    date: "16 September 2015"
    header-includes:
      - \usepackage{graphicx}
      - \usepackage{fancyhdr} 
      - \pagestyle{fancy} 
      - \usepackage{lastpage}
      - \usepackage{float} 
      - \floatstyle{boxed} 
      - \restylefloat{figure} 
      - \usepackage{url} 
      - \usepackage{color}
      - \lhead{Left Header}
      - \chead{Rmarkdown Rocks}
      - \rhead{\today}
      - \lfoot{Left Footer}
      - \cfoot{Centre Footer}
      - \rfoot{\thepage\ of \pageref{LastPage}}  
    output: 
      pdf_document:
        toc: false
    documentclass: article
    classoption: a4paper
    bibliography: references.bib
    ---
  
<p></p>
  
Now the layout of tables and figures is done with latex
  
#### Code
  
    Using the xtable package allows results to be displyed in tables
    and has built in support for some R objects, so summrising the
    linear fit above in ~\ref{ATable}
      
    ```{r, results='asis', type = 'tex'}
    library(xtable)    
    print(xtable(fit, caption="Example Table",
      digits=4,table.placement="ht",label="ATable"), comment = F)    
    ```
       
    ## A Plot
       
    Plots intergrate most easily if made seperately as can be seen in figure ~\ref{test}
    ```{r}
    png("Rmarkdownfig.png")
    plot(x,y,main="Example Plot",xlab="X Variable",ylab="Y Variable")
    abline(fit,col="Red")
    dev.off()
    ```
    \begin{figure}[H]
    \begin{center}
    \includegraphics[width=.5\textwidth]{Rmarkdownfig.png}
    \end{center}
    \caption{Some Plot}
    \label{test}
    \end{figure}
    \clearpage
  
<p></p>
  

I also realised that if this was to be a full report of a scientific study it would need to include some of the machinery needed for bibliographies.

#### Stuff for bibliographies

    ```{r, echo=F, results = 'hide', message = F, warning=F}
    library("knitcitations")
    library("bibtex")
    cleanbib()
    cite_options(citation_format = "pandoc", check.entries = FALSE)
     
    bib <- read.bibtex("C:/Users/Ivan/Dropbox/references/library.bib")
     
    ```

    <!--Put data analysis and reporting here, then at the end of the doc-->

    ```{r, echo=F, message=F, eval=T}
    write.bibtex(file="references.bib")
    ```
          
    # References

    <!--The bib will then be written following the final subheading-->

<p></p>

## Conclusion

I hope this might help others develop their own templates for RRR that look great.

#+end_src


** 2015-09-28-the-best-thing-about-reproducibility-is-not-reproducibility-but-transparency-and-rigour
#+name:the-best-thing-about-reproducibility-is-not-reproducibility-but-transparency-and-rigour-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-09-28-the-best-thing-about-reproducibility-is-not-reproducibility-but-transparency-and-rigour.md :exports none :eval no :padline no
---
name: the-best-thing-about-reproducibility-is-not-reproducibility-but-transparency-and-rigour
layout: post
title: The Best Thing About Reproducibility Is Not Reproducibility, It Is Transparency And Rigour
date: 2015-09-28
categories:
- disentangle
- Reproducible Research Reports
---

Adequately documenting the methods and results of data analysis helps
safeguard against errors of execution and
interpretation. My PhD thesis proposes that
reproducible research pipelines address the problem of
adequate documentation of data analysis. 

A graphical view of the reproducible research pipeline concept is shown below.  The ideas  were introduced into epidemiology by Peng et al in 2006, although Peng has more recently been using the terms 'evidence based data analysis pipeline' (Peng 2013) and 'Data Science Pipeline' (Peng 2015). Both terms are useful, but I chose to follow the original phrase.  The graphical version shown below was introduced by Solymos and Feher (2008).
  

![img](/images/reproduciblepipeline2.png)


The best thing about reproducible work is not merely the ability to repeatedly arrive at the same result, but that having the organisational structures in place that are required for reproducibility also implicitly will improve the transparency and rigour of the work.  This is because they make
it easy to check the methods. Assumptions are easy
to challenge and results verified in new analyses. 

Reproducible
research pipelines extend traditional research.  They do this by
encoding the steps in a computer ‘scripting’ language and distributing
the data and code with publications.  Traditional research moves
through the steps of hypothesis and design, measured data, analytic
data, computational results (for figures, tables and numerical
results), and reports (text and formatted manuscript).

This model of the research pipeline sees a new relationship possible between the author and the reader.  They approach the results and understandings of the research from opposite directions. Readers can dig deeper into the research to verify results or conduct similar studies. Reproducibility exists along a spectrum from minimum reproducibility that can be achieved by providing measured or analytic data and the analytic code. More reproducibility is gained by providing processing code necessary to transform original measured data into tidy data for analysis. Full reproducibility would include all stages of the pipeline. 

## References

- Peng, R.D., Dominici, F. & Zeger, S.L. (2006). Reproducible epidemiologic research. American Journal of Epidemiology, 163, 783–789.  Retrieved from [http://dx.doi.org/10.1093/aje/kwj093](http://dx.doi.org/10.1093/aje/kwj093)
    
- Peng, R.D. (2013). Implementing Evidence-based Data Analysis: Treading a New Path for Reproducible Research. Simply statistics. Retrieved from [http://simplystatistics.org/2013/09/05/implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/](http://simplystatistics.org/2013/09/05/implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/)

- Peng, R.D. (2015). Report Writing for Data Science in R. leanpub. Retrieved from [https://leanpub.com/reportwriting](https://leanpub.com/reportwriting)
    
- Sólymos, P. & Fehér, Z. (2008). The mefa package: a tool for reproducible data processing in biogeography. International Biogeography Society Newsletter.  Retrieved from [http://biogeography.blogspot.com.au/2008/04/mefa-package-tool-for-reproducible-data.html](http://biogeography.blogspot.com.au/2008/04/mefa-package-tool-for-reproducible-data.html)


    
#+end_src

** 2015-10-01-reproducible-research-pipelines-in-epidemiology
#+name:reproducible-research-pipelines-in-epidemiology-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-01-reproducible-research-pipelines-in-epidemiology.md :exports none :eval no :padline no
  ---
  name: reproducible-research-pipelines-in-epidemiology
  layout: post
  title: Reproducible Research Pipelines In Epidemiology
  date: 2015-10-01
  categories:
  - disentangle
  tags:
  - Reproducible Research Reports
  ---

   


  The scientific questions motivating my work explore the health
  effects of environmental changes.  These include droughts, bushfires,
  woodsmoke, dust storms, heat waves and local environmental
  conditions. The research needed to disentangle health effects of
  environmental changes from social factors. Some of the findings were
  novel and unexpected. Adequate documentation of the methods was
  problematic because of the many steps of data processing and
  analysis. Reproducible research pipelines address the problem of
  documenting data analyses by distributing data and code with
  publications.
  
  Reproducibility is needed to improve credibility.  It is often
  asserted in the literature that much research is not easy to
  reproduce. It is not clear what an effective way to implement these
  techniques is. The thesis asks how pipelines can be effectively
  implemented in epidemiology. It describes methods for reproducible
  research pipelines. It also demonstrates several applications of these
  methods in environmental epidemiology.
  
  Environmental epidemiology requires us to study multifactorial
  pathogenesis.  All diseases have multiple causal factors. To
  understand the many factors affecting health, epidemiologists must
  disentangle strands of a web of causal influences. Isolating factors
  is difficult and risks being overly reductionist. These determinants
  can interact in complex ways. Environmental epidemiologists often
  narrow the focus to a single environmental cause and health effect. A
  simple example is bushfire smoke and direct effects on
  cardio-respiratory disease.  A more complex example is drought and
  suicide where the effects are indirect. The focus is on a chain
  of intermediary causal factors. These questions are usually explored
  in the context of many other factors that describe human biological
  variables and the socio-economic milieu.
  
  While there is greater weight given to evidence from experimental than
  observational studies, experiments are difficult in environmental
  health.  Analysis of observational data is often used instead. There
  are problems inherent in observational studies that pertain to
  variables that are confounders and effect modifiers.  Observational
  studies face the principal problem of a large number of
  inter-relationships between variables. These can confound or modify
  effects.  It is vital to a valid analysis and meaningful
  interpretation that we include these. It is problematic that
  scientists select variables from a multitude of possibilities found in
  the literature. Scientists also gather variables from a plethora of
  possible data sources. There is a long process of hypothesising, study
  design, data collection, cleaning, exploration, decision making,
  preparation, data analysis, model building and model checking.  This
  process has been described as a vast ‘garden of forking paths’ which
  connect steps and decisions the analyst must make, but they could have
  made others. These issues might result in mere
  correlation interpreted as causation.
  
  Adequately documenting the methods and results of data analysis helps
  safeguard against such mistakes. This thesis proposes that
  reproducible research pipelines address the problem of adequate
  documentation of data analysis.  This is because they make it easy to
  check the methods. Assumptions are easy to challenge and results
  verified in new analyses. Reproducible research pipelines extend
  traditional research.  They do this by encoding the steps in a
  computer ‘scripting’ language and distributing the data and code with
  publications.  Traditional research moves through the steps of
  hypothesis and design, measured data, analytic data, computational
  results (for figures, tables and numerical results), and reports (text
  and formatted manuscript).

#+end_src  
** 2015-10-09-coming-to-grips-with-citations-in-reproducible-research-reports
*** COMMENT Rmd-code
#+name:Rmd
#+begin_src R :session *R* :tangle ~/projects/ivanhanigan.github.com.raw/rmarkdown_utils/CitationsInRmarkdown.Rmd :exports none :eval no
  ---
  title: "Coming to grips with citations in Rmarkdown"
  author: Ivan C. Hanigan
  output:
    html_document:
      toc: true
      theme: united
      number_sections: yes    
      toc_depth: 2
    pdf_document:
      toc: true
      toc_depth: 2
      highlight: zenburn
      keep_tex: true
      number_sections: no        
  documentclass: article
  classoption: a4paper
  csl: meemodified.csl
  bibliography: refs.bib
  ---
  
  ```{r, echo=F, eval = F}
#+end_src
*** go
#+name:Rmd
#+begin_src R :session *R* :tangle ~/projects/ivanhanigan.github.com.raw/rmarkdown_utils/CitationsInRmarkdown.Rmd :exports none :eval yes
  # func
  setwd("~/tools/disentangle")
  library(rmarkdown)
  library(knitr)
  library(knitcitations)
  library(bibtex)
  cleanbib()
  #rm("bib")
  #options("cite_format"="pandoc")
  cite_options(citation_format = "pandoc", check.entries=FALSE)
  rmarkdown::render("~/projects/ivanhanigan.github.com.raw/rmarkdown_utils/CitationsInRmarkdown.Rmd", "html_document")
  #browseURL("CitationsInRmarkdown.html")  
  #rmarkdown::render("~/projects/ivanhanigan.github.com.raw/rmarkdown_utils/CitationsInRmarkdown.Rmd", "pdf_document")
  #browseURL("CitationsInRmarkdown.pdf")
#+end_src

#+RESULTS: Rmd
: /home/ivan_hanigan/projects/ivanhanigan.github.com.raw/rmarkdown_utils/CitationsInRmarkdown_sanitize_mendeley.html

*** rmd
#+name:Rmd
#+begin_src R :session *R* :tangle ~/projects/ivanhanigan.github.com.raw/rmarkdown_utils/CitationsInRmarkdown.Rmd :exports none :eval no
  ```
  
  ```{r, echo = F, results = 'hide'}
  # load my main bib file, generated from Mendeley
  if(!exists("bib")){
  bib <- read.bibtex("~/references/library.bib")
  }
  ```
  
  # Introduction
  Earlier this year I was pleased to stumble on to Petr Keil\'s [Simple
  template for scientific manuscripts in
  Rmarkdown](http://www.petrkeil.com/?p=2401) and the Github Repo
  [https://github.com/petrkeil/Blog/tree/master/2015_03_12_R_ms_template](https://github.com/petrkeil/Blog/tree/master/2015_03_12_R_ms_template).
    
  
  I was already using Rmarkdown effectively for everything I wanted
  except my bibliography, and this helped a lot.  But I eventually
  found I needed to tweak the format of the citation style.  I tried
  out a bunch of other CSL files but none felt just right.  I tried out
  these after downloading from [https://github.com/citation-style-language/styles](https://github.com/citation-style-language/styles):
    
  ```
  american-physiological-society.csl
  annals-of-the-association-of-american-geographers.csl
  biomed-central.csl
  ecology.csl
  pnas.csl
  ```
  
  NB also that the csl file in petrkeil's repo is an older version from 2012 of the version in the citation-style repo called `methods-in-ecology-and-evolution`.  The differences are not large though.
    
  ## The problems
    
  - Every journal paper has a URL 
  - I wanted to cite the electronic version of an article before it later appears in print, called epub ahead of print, like in PubMed.
  - I wanted to cite blog posts (very modern hey? next it will be tweets)
    
  ## Here are some example citations
    
  ### Example 1:
  
  The journal article by `r citet(bib[["Michener1997"]])` and another one `r citep(bib[["Bodnar2004"]])` appear with their full URL even though I just want their DOI.
  
  ### Example 2:
  
  Some recent papers `r citep(c(bib[["OpenScienceCollaboration2015"]],bib[["Davey2015"]], bib[["Aiken2015"]]))` don't have Volume info and I want to say [epub ahead of print].
  
  ### Example 3:
  
  This blog post on 'evidence based data analysis pipeline' by `r citet(bib[["Peng2013"]])` is one that definitely needs the URL and date accessed.      
  
  
  ```{r, echo=FALSE, message=FALSE, eval = T}
  write.bibtex(file="refs.bib")
  ```
  # References  
#+end_src

*** rmd2
#+name:Rmd
#+begin_src R :session *R* :tangle no :exports none :eval no
  ```
  
  ```{r, echo = F, results = 'hide'}
  # load my main bib file, generated from Mendeley
  if(!exists("bib")){
  bib <- read.bibtex("~/references/library.bib")
  }
  ```
  
  # SO I hacked the CSL file

  - I used the `american-physiological-society.csl` to get stuff I wanted and pasted into the `mee.csl`
  - get the files from this link [/rmarkdown_utils/~/projects/ivanhanigan.github.com.raw/rmarkdown_utils/CitationsInRmarkdown.Rmd](/rmarkdown_utils/~/projects/ivanhanigan.github.com.raw/rmarkdown_utils/CitationsInRmarkdown.Rmd)   
 
  ```
  # first replace the <macro name="access">
  # with 
  <macro name="access">
    <choose>
      <if variable="DOI"/>
      <!--don't use if there is a DOI-->
      <else>
        <choose>
          <if variable="URL">
            <group delimiter=" " prefix=" ">
              <group>
                <text variable="URL"/>
              </group>
              <group prefix="[" suffix="]" delimiter=" ">
                <date variable="accessed">
                  <date-part name="day"/>
                  <date-part name="month" prefix=" " suffix=" " form="short"/>
                  <date-part name="year"/>
                </date>
              </group>
            </group>
          </if>
        </choose>
      </else>
    </choose>
  </macro>
  #### Then add this ####
  <macro name="date">
    <choose>
      <if variable="issued">
        <choose>
          <if type="article-journal">
            <date variable="issued">
              <date-part name="year"/>
            </date>
          </if>
          <else>
            <date variable="issued">
              <date-part name="year"/>
            </date>
          </else>
        </choose>
      </if>
      <else>
        <text term="no date" prefix="[" suffix="]"/>
      </else>
    </choose>
  </macro>
  #### And add this ####
        <else-if type="article-journal">
          <choose>
            <if variable="issue volume" match="any">
              <text macro="title" suffix=" "/>
              <text variable="container-title" suffix=" " form="short" font-style="italic" strip-periods="true"/>
              <text variable="volume"/>
              <text variable="page" prefix=": "/>
              <text macro="date" prefix=", " suffix="."/>
            </if>
            <else>
              <choose>
                <if variable="DOI">
                  <text macro="title" suffix=" "/>
                  <text variable="container-title" suffix=" " form="short" font-style="italic"/>
                  <group prefix="(" suffix=").">
                    <date variable="issued">
                      <date-part name="month" prefix=" " suffix=" "/>
                      <date-part name="day" suffix=", "/>
                      <date-part name="year"/>
                    </date>
                  </group>
                  <text variable="DOI" prefix=" doi: "/>
                </if>
                <else>
                  <text variable="container-title" suffix=". " form="short" font-style="italic"/>
                </else>
              </choose>
            </else>
          </choose>
        </else-if>
  ```

  ## Now use this modified csl in the header of the RMD file instead of mee.csl
  ### Example 1:
  
  The journal article by `r citet(bib[["Michener1997"]])` and another one `r citep(bib[["Bodnar2004"]])` appear with their full URL even though I just want their DOI.
  
  ### Example 2:
  
  Some recent papers `r citep(c(bib[["OpenScienceCollaboration2015"]],bib[["Davey2015"]], bib[["Aiken2015"]]))` don't have Volume info and I want to say [epub ahead of print].
  
  ### Example 3:
  
  This blog post on 'evidence based data analysis pipeline' by `r citet(bib[["Peng2013"]])` is one that definitely needs the URL and date accessed.      
  
  ```{r, echo=FALSE, message=FALSE, eval = T}
  write.bibtex(file="refs.bib")
  ```
  # References  
#+end_src


*** blog

#+name:coming-to-grips-with-citations-in-reproducible-research-reports-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-09-coming-to-grips-with-citations-in-reproducible-research-reports.md :exports none :eval no :padline no
---
name: coming-to-grips-with-citations-in-reproducible-research-reports
layout: post
title: Coming To Grips With Citations In Reproducible Research Reports
date: 2015-10-09
categories:
- disentangle
tags:
- Reproducible Research Reports
---

# Introduction
Earlier this year I was pleased to stumble on to Petr Keil\'s [Simple
template for scientific manuscripts in
Rmarkdown](http://www.petrkeil.com/?p=2401) and the Github Repo
[https://github.com/petrkeil/Blog/tree/master/2015_03_12_R_ms_template](https://github.com/petrkeil/Blog/tree/master/2015_03_12_R_ms_template).
  

I was already using Rmarkdown effectively for everything I wanted
except my bibliography, and this helped a lot.  But I eventually
found I needed to tweak the format of the citation style.  I tried
out a bunch of other CSL files but none felt just right.  I tried out
these after downloading from [https://github.com/citation-style-language/styles](https://github.com/citation-style-language/styles):
  
 
```
american-physiological-society.csl
annals-of-the-association-of-american-geographers.csl
biomed-central.csl
ecology.csl
pnas.csl
```
<p></p>


# SO I hacked the CSL file

- I used the `american-physiological-society.csl` to get stuff I wanted and pasted into the `mee.csl`
- get the files from this link [/rmarkdown_utils/CitationsInRmarkdown.Rmd](/rmarkdown_utils/CitationsInRmarkdown.Rmd)   
- get the files from this link [/rmarkdown_utils/meemodified.csl](/rmarkdown_utils/meemodified.csl)   
- get the files from this link [/rmarkdown_utils/refs.bib](/rmarkdown_utils/refs.bib)   

NB also that the csl file in petrkeil's repo is an older version from 2012 of the version in the citation-style repo called `methods-in-ecology-and-evolution`.  The differences are not large though.

<h3><span class="header-section-number">1.1.1</span> Example 1:</h3>
<p>The journal article by <span class="citation">Michener et al. (1997)</span> and another one <span class="citation">(Bodnar <em>et al.</em> 2004)</span> appear with their full URL even though I just want their DOI.</p>
<h3><span class="header-section-number">1.1.2</span> Example 2:</h3>
<p>Some recent papers <span class="citation">(Open Science Collaboration 2015; Aiken <em>et al.</em> 2015; Davey <em>et al.</em> 2015)</span> don’t have Volume info and I want to say [epub ahead of print].</p>
<h3><span class="header-section-number">1.1.3</span> Example 3:</h3>
<p>This blog post on ‘evidence based data analysis pipeline’ by <span class="citation">Peng (2013)</span> is one that definitely needs the URL and date accessed.</p>

<h1>References</h1>
<p>Aiken, A.M., Davey, C., Hargreaves, J.R. &amp; Hayes, R.J. (2015). Re-analysis of health and educational impacts of a school-based deworming programme in western Kenya: a pure replication. <em>International Journal of Epidemiology</em>, dyv127. Retrieved from <a href="http://ije.oxfordjournals.org/content/early/2015/07/21/ije.dyv127.full http://www.ije.oxfordjournals.org/lookup/doi/10.1093/ije/dyv127" class="uri">http://ije.oxfordjournals.org/content/early/2015/07/21/ije.dyv127.full http://www.ije.oxfordjournals.org/lookup/doi/10.1093/ije/dyv127</a></p>
<p>Bodnar, A., Castorina, R., Desai, M., Duramad, P., Fischer, S., Klepeis, N., Liang, S., Mehta, S., Naumoff, K., Noth, E.M., Schei, M., Tian, L., Vork, K.L. &amp; Smith, K.R. (2004). Lessons learned from ‘the skeptical environmentalist’: an environmental health perspective. <em>International journal of hygiene and environmental health</em>, <strong>207</strong>, 57–67. Retrieved from <a href="http://www.sciencedirect.com/science/article/pii/S1438463904702643" class="uri">http://www.sciencedirect.com/science/article/pii/S1438463904702643</a></p>
<p>Davey, C., Aiken, A.M., Hayes, R.J. &amp; Hargreaves, J.R. (2015). Re-analysis of health and educational impacts of a school-based deworming programme in western Kenya: a statistical replication of a cluster quasi-randomized stepped-wedge trial. <em>International Journal of Epidemiology</em>, dyv128. Retrieved from <a href="http://ije.oxfordjournals.org/content/early/2015/07/21/ije.dyv128.full http://www.ije.oxfordjournals.org/lookup/doi/10.1093/ije/dyv128" class="uri">http://ije.oxfordjournals.org/content/early/2015/07/21/ije.dyv128.full http://www.ije.oxfordjournals.org/lookup/doi/10.1093/ije/dyv128</a></p>
<p>Michener, W.K., Brunt, J.W., Helly, J.J., Kirchner, T.B. &amp; Stafford, S.G. (1997). Nongeospatial metadata for the ecological sciences. <em>Ecological Applications</em>, <strong>7</strong>, 330–342. Retrieved from <a href="http://www.scopus.com/inward/record.url?scp=0030616825\&amp;partnerID=8YFLogxK" class="uri">http://www.scopus.com/inward/record.url?scp=0030616825\&amp;partnerID=8YFLogxK</a></p>
<p>Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. <em>Science</em>, <strong>349</strong>, aac4716–aac4716. Retrieved from <a href="http://www.sciencemag.org/cgi/doi/10.1126/science.aac4716" class="uri">http://www.sciencemag.org/cgi/doi/10.1126/science.aac4716</a></p>
<p>Peng, R.D. (2013). Implementing Evidence-based Data Analysis: Treading a New Path for Reproducible Research. <em>Simply statistics</em>. Retrieved July 26, 2015, from <a href="http://simplystatistics.org/2013/09/05/implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/" class="uri">http://simplystatistics.org/2013/09/05/implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/</a></p>


 
```
# first replace the <macro name="access">
# with 
<macro name="access">
  <choose>
    <if variable="DOI"/>
    <!--don't use if there is a DOI-->
    <else>
      <choose>
        <if variable="URL">
          <group delimiter=" " prefix=" ">
            <group>
              <text variable="URL"/>
            </group>
            <group prefix="[" suffix="]" delimiter=" ">
              <date variable="accessed">
                <date-part name="day"/>
                <date-part name="month" prefix=" " suffix=" " form="short"/>
                <date-part name="year"/>
              </date>
            </group>
          </group>
        </if>
      </choose>
    </else>
  </choose>
</macro>
#### Then add this ####
<macro name="date">
  <choose>
    <if variable="issued">
      <choose>
        <if type="article-journal">
          <date variable="issued">
            <date-part name="year"/>
          </date>
        </if>
        <else>
          <date variable="issued">
            <date-part name="year"/>
          </date>
        </else>
      </choose>
    </if>
    <else>
      <text term="no date" prefix="[" suffix="]"/>
    </else>
  </choose>
</macro>
#### And add this ####
      <else-if type="article-journal">
        <choose>
          <if variable="issue volume" match="any">
            <text macro="title" suffix=" "/>
            <text variable="container-title" suffix=" " form="short" font-style="italic" strip-periods="true"/>
            <text variable="volume"/>
            <text variable="page" prefix=": "/>
            <text macro="date" prefix=", " suffix="."/>
          </if>
          <else>
            <choose>
              <if variable="DOI">
                <text macro="title" suffix=" "/>
                <text variable="container-title" suffix=" " form="short" font-style="italic"/>
                <group prefix="(" suffix=").">
                  <date variable="issued">
                    <date-part name="month" prefix=" " suffix=" "/>
                    <date-part name="day" suffix=", "/>
                    <date-part name="year"/>
                  </date>
                </group>
                <text variable="DOI" prefix=" doi: "/>
              </if>
              <else>
                <text variable="container-title" suffix=". " form="short" font-style="italic"/>
              </else>
            </choose>
          </else>
        </choose>
      </else-if>
```

## Now use this modified csl in the header of the RMD file instead of mee.csl


<h1><span class="header-section-number">1.2</span> NEW References</h1>
<p>Aiken, A.M., Davey, C., Hargreaves, J.R. &amp; Hayes, R.J. (2015).Re-analysis of health and educational impacts of a school-based deworming programme in western Kenya: a pure replication <em>International Journal of Epidemiology</em> (epub ahead of print July 2015). doi: <a href="http://dx.doi.org/10.1093/ije/dyv127">10.1093/ije/dyv127</a></p>
<p>Bodnar, A., Castorina, R., Desai, M., Duramad, P., Fischer, S., Klepeis, N., Liang, S., Mehta, S., Naumoff, K., Noth, E.M., Schei, M., Tian, L., Vork, K.L. &amp; Smith, K.R. (2004).Lessons learned from ‘the skeptical environmentalist’: an environmental health perspective. <em>International journal of hygiene and environmental health</em> 207: 57–67, 2004.</p>
<p>Davey, C., Aiken, A.M., Hayes, R.J. &amp; Hargreaves, J.R. (2015).Re-analysis of health and educational impacts of a school-based deworming programme in western Kenya: a statistical replication of a cluster quasi-randomized stepped-wedge trial <em>International Journal of Epidemiology</em> (epub ahead of print July 2015). doi: <a href="http://dx.doi.org/10.1093/ije/dyv128">10.1093/ije/dyv128</a></p>
<p>Michener, W.K., Brunt, J.W., Helly, J.J., Kirchner, T.B. &amp; Stafford, S.G. (1997).Nongeospatial metadata for the ecological sciences <em>Ecological Applications</em> 7: 330–342, 1997. <a href="http://www.scopus.com/inward/record.url?scp=0030616825\&amp;partnerID=8YFLogxK">http://www.scopus.com/inward/record.url?scp=0030616825\&amp;partnerID=8YFLogxK</a></p>
<p>Open Science Collaboration. (2015).Estimating the reproducibility of psychological science <em>Science</em> 349: aac4716–aac4716, 2015.</p>
<p>Peng, R.D. (2013). Implementing Evidence-based Data Analysis: Treading a New Path for Reproducible Research. <em>Simply statistics</em>. <a href="http://simplystatistics.org/2013/09/05/implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/">http://simplystatistics.org/2013/09/05/implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/</a> [26 Jul. 2015]</p>



#+end_src

** 2015-10-24-sanitize-mendeley-references-in-r-markdown-reporting
#+name:sanitize-mendeley-references-in-r-markdown-reporting-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-24-sanitize-mendeley-references-in-r-markdown-reporting.md :exports none :eval no :padline no
---
name: sanitize-mendeley-references-in-r-markdown-reporting
layout: post
title: Sanitize mendeley references in r markdown reporting
date: 2015-10-24
categories:
- disentangle 
- Reproducible Research Reports
---

A key challenge for Reproducible Research Reports in Rmarkdown remains adequate scholarly citation management; the machinery of scholarly citations and references. 

The `knitcitations` R package does a great job of working with bibtex bibliography files, however the bibtex manager that I use is Mendeley and it has implemented some rules on the way it handles special characters that forces the bibtex references into a state with some 'escaped' elements that breaks their presentation via `knitcitations`.

This is a post from my open notebook that shows the workaround I am using for sanitizing the Mendeley escaped underscores in URLS.

Here is an example:

<p>There is considerable public health impact from the effects on mental of drought <span class="citation">(<span>Sarathi Biswas</span> 2012)</span>. It is proposed that the best method to disentangle the multifactorial nature of this causal mechanism is the ‘five-capitals’ framework, indeed this method may even enable understanding the human carrying capacity of ecosystems <span class="citation">(McMichael &amp; Butler 2002)</span>.</p>

<p>McMichael, A.J. &amp; Butler, C.D. (2002). Global health trends: Evidence for and against sustainable progress. <em>International Union for the Scientific Study of Population Committee on Emerging Health Threats</em>. <a href="http://www.demogr.mpg.de/papers/workshops/020619{\_}paper25.pdf">http://www.demogr.mpg.de/papers/workshops/020619{\_}paper25.pdf</a> [21 Sep. 2003]</p>
<p><span>Sarathi Biswas</span>, P. (2012). Alcohol, drought lead to farmer’s suicide. <em>Daily News and Analysis</em>. <a href="http://www.dnaindia.com/pune/report{\_}alcohol-drought-lead-to-farmers-suicide{\_}1688976">http://www.dnaindia.com/pune/report{\_}alcohol-drought-lead-to-farmers-suicide{\_}1688976</a> [17 May 2012]</p>



<p></p>

## See those pesky curly braces `{` and `}` around the underscores?



## The fix
The fix I am using is to sanitize each record where this is an issue as I build my document, so the mendeley version stays as-is, while the R version has been sanitized by removing the escape characters.

```
# read mendeley bibtex file
bib <- read.bibtex("~/references/library.bib")
# ad hoc fix
for(bibkey in c("SarathiBiswas2012", "Mcmichael2002a")){
  bib[ [ bibkey ] ]$url <- gsub("\\{\\\\_\\}","_", bib[ [ bibkey ] ]$url)
}
```


This is the result:

<p>McMichael, A.J. &amp; Butler, C.D. (2002). Global health trends: Evidence for and against sustainable progress. <em>International Union for the Scientific Study of Population Committee on Emerging Health Threats</em>. <a href="http://www.demogr.mpg.de/papers/workshops/020619_paper25.pdf">http://www.demogr.mpg.de/papers/workshops/020619_paper25.pdf</a> [21 Sep. 2003]</p>
<p><span>Sarathi Biswas</span>, P. (2012). Alcohol, drought lead to farmer’s suicide. <em>Daily News and Analysis</em>. <a href="http://www.dnaindia.com/pune/report_alcohol-drought-lead-to-farmers-suicide_1688976">http://www.dnaindia.com/pune/report_alcohol-drought-lead-to-farmers-suicide_1688976</a> [17 May 2012]</p>

<p></p>

#+end_src

*** COMMENT Rmd-code
#+name:Rmd
#+begin_src R :session *R* :tangle ~/projects/ivanhanigan.github.com.raw/rmarkdown_utils/CitationsInRmarkdown_sanitize_mendeley.Rmd :exports none :eval no
  ---
  title: "Sanitizing Mendeley citations in Rmarkdown"
  author: Ivan C. Hanigan
  output: html_document
  csl: meemodified.csl
  bibliography: refssanitized.bib
  ---
  
  ```{r, echo=F, eval = F}
#+end_src
*** go
#+name:Rmd
#+begin_src R :session *R* :tangle ~/projects/ivanhanigan.github.com.raw/rmarkdown_utils/CitationsInRmarkdown_sanitize_mendeley.Rmd :exports none :eval yes
  # func
  setwd("~/projects/ivanhanigan.github.com.raw/rmarkdown_utils")
  library(rmarkdown)
  library(knitr)
  library(knitcitations)
  library(bibtex)
  cleanbib()
  rm("bib")
  #bib <- read.bibtex("~/references/library.bib")
  
  #options("cite_format"="pandoc")
  cite_options(citation_format = "pandoc", check.entries=FALSE)
  rmarkdown::render("~/projects/ivanhanigan.github.com.raw/rmarkdown_utils/CitationsInRmarkdown_sanitize_mendeley.Rmd", "html_document")
#+end_src

#+RESULTS: Rmd
: /home/ivan_hanigan/tools/disentangle/CitationsInRmarkdown.html

*** rmd
#+name:Rmd
#+begin_src R :session *R* :tangle ~/projects/ivanhanigan.github.com.raw/rmarkdown_utils/CitationsInRmarkdown_sanitize_mendeley.Rmd :exports none :eval no
  ```
  
  ```{r, echo = F, eval = T}
  # load my main bib file, generated from Mendeley
  if(!exists("bib")){
  bib <- read.bibtex("~/references/library.bib")
  # ad hoc fix
  for(bibkey in c("SarathiBiswas2012",
    "Mcmichael2002a"  
  )){
  bib[[bibkey]]$url <- gsub("\\{\\\\_\\}","_", bib[[bibkey]]$url)
  }
  
  }
  ```
  
  There is considerable public health impact from the effects on mental of drought `r citep(bib[["SarathiBiswas2012"]])`.  It is proposed that the best method to disentangle the multifactorial nature of this causal mechanism is the 'five-capitals' framework, indeed this method may even enable understanding the human carrying capacity of ecosystems `r citep(bib[["Mcmichael2002a"]])`. 
  
  
  ```{r, echo=FALSE, message=FALSE, eval = T}
  write.bibtex(file="refssanitized.bib")
  ```
  # References  
#+end_src

** TODO summarise minimum standard Re Dr Climate 


1    A description of the software packages and operating system used
2    A (preferably version controlled and publicly accessible) code repository, and
3    A collection of supplementary log files that capture the data processing steps taken in producing each key result

https://drclimate.wordpress.com/2015/11/05/a-call-for-reproducible-research-volunteers/
http://journals.ametsoc.org/doi/pdf/10.1175/BAMS-D-15-00010.1
https://www.authorea.com/users/5641/articles/12197/_show_article

* Writing
** One-pager
focus
down *(mgmnt)
out (clients)

whoami, establish credentials
what
why
Actions
when
what I need
implications

1 issue (one line)
2 Analysis (history, backghround, one paragraph) refer to other docs or conversations)
3 options or dimensions (list of all possible options. alternateives, one paragraph per option
4 suggrestion/request one sentence or short para
5 timing one sentence

name (date)

dots are great
** SI emails
The SI Rules

1    Try to send no more than one email a day. 
2    Emails should be 3 sentences or less. Better if you can get the whole email in the subject line. 
3    If you need information, ask yes or no questions whenever possible. Never ask a question that requires a full sentence response.
4    When something is time sensitive, state the action you will take if you don’t get a response by a time you specify. 
5    Be as specific as you can while conforming to the length requirements. 
6    Bonus: include obvious keywords people can use to search for your email. 

eg
Example 1

Subject: Is my response to reviewer 2 ok with you?

Body: I’ve attached the paper/responses to referees.

Example 2

Subject: Can you send my letter of recommendation to john.doe@someplace.com?

Body:

Keywords = recommendation, Jeff, John Doe.

Example 3

Subject: I revised the draft to include your suggestions about simulations and language

Revisions attached. Let me know if you have any problems, otherwise I’ll submit Monday at 2pm. 
** 2015-09-03-how-to-say-why-before-what
#+name:how-to-say-why-before-what-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-09-03-how-to-say-why-before-what.md :exports none :eval no :padline no
---
name: how-to-say-why-before-what
layout: post
title: how-to-say-why-before-what
date: 2015-09-03
categories:
- disentangle
tags:
- writing
---

I have discovered a flaw in my writing style.  
I often say what it it before I say why it is important.
  
#### Example 
  
    Disentangling health effects of environmental from social factors
    is difficult for a variety of reasons. The effort to examine and
    to separate environmental and social causes is nevertheless
    valuable. [WHY IS IT VALUABLE?] This is especially important to
    policy makers and to others who seek to maximise the public
    good. A greater understanding of their respective contributions
    will lead to more rational, deep-seated, lasting and effective
    interventions.

<p></p>

The caps question is from someone reading my draft.  I need to start with the why.
Perhaps just turn the paragraph on its head?
  
#### Example 
  
    A greater understanding of the respective contributions from
    environmental and social factors will lead to more rational,
    deep-seated, lasting and effective interventions by policy makers
    and to others who seek to maximise the public good.  Disentangling
    health effects of environmental from social factors is difficult
    for a variety of reasons. The effort to examine and to separate
    environmental and social causes is nevertheless valuable.
      
<p></p>

#+end_src

** disavow an expected confusion
http://andrewgelman.com/2015/02/09/24741/
Discussion with Steven Pinker connecting cognitive psychology research to the difficulties of writing
Posted by Andrew on 9 February 2015, 10:00 am

Following up on my discussion of Steven Pinker’s writing advice, Pinker and I had an email exchange that cleared up some issues and raised some new ones.

In particular, Pinker made a connection between the difficulty of writing and some research findings in cognitive psychology. I think this connection is really cool—I’ve been thinking and writing about writing for awhile, now, but I’e never really seen the connection to psychology research. So I wanted to share this with you.

Pinker’s remarks came at the end of an email exchange. I’ll share the earlier messages to give the background, but by far the most interesting part is what Pinker said, so I’ll give that right away.

Here’s Pinker, discussing the difficulty of communicating complex ideas (in particular, in academic writing we typically aren’t just making the case for position B, we’re also arguing why previous position A, reasonable as it might sound, is not correct):

    I address it in part in chapter 5 of The Sense of Style in discussing our comprehension of negation. The human mind cannot represent a proposition without a truth value – to think “X” is to think “X is true,” at least temporarily. Negation requires an extra mental step—which can easily fail when the person is overloaded or distracted. A number of systematic kinds of error and difficulty follow. My colleague Dan Gilbert has an insightful review of this literature in his 1991 article, “How Mental Systems Believe.”

    On top of that people’s comprehension is often driven more by expectations than the literal content of the text (again, particularly when not paying close attention). When I see certain misinterpretations I often think of the puzzling neurospsychological syndrome called “deep dyslexia.” Surface dyslexia consists of misreadings based on alphabetic confusions – misreading “pear” as “bear,” for example. In deep dyslexia, the patient might misread “pear” as “apple.” The puzzle is that if the patient’s word-recognition system could parse the letters well enough to realize it referred to a fruit, it must have been because he matched the input with a stored template for “pear” – so why didn’t he successfully read it as “pear”? Presumably such patients’ semantic representations (the definitions in their mental dictionary) were so degraded that the word merely activated a coarse semantic ballpark, without enough precision to pinpoint the exact entry. From there sheer base-rate frequency determines the output. It’s a crude analogy to the way we non-brain-damaged people often parse a sentence coarsely enough to remind us of a semantic neighborhood and then we fill in the rest from base rates. Often a writer will have to anticipate this and explicitly disavow an expected confusion: “You probably think I mean this, but I really mean that.”

I’ve been trying to do this more often, for example in this 2011 article on the philosophy of Bayesian statistics, where I’m pretty explicit about what I’m disagreeing with (for example, the first section of this paper is entitled, “The Standard View of the Philosophy of Statistics, and Its Malign Influence on Statistical Practice”).

Still, I think the necessity of clearing-away-the-old creates an additional degree of difficulty in much of academic writing.

What I found exciting in Pinker’s note above was his connection of this vague idea, which I arrived at by introspection, to research in cognition.
** share txt files linux to Windows
text files created in Ubuntu are so difficult to understand (read) when opened in Windows' Notepad. No matter how many lines have been used, all the lines appear in the same one line.
To set the buffer coding to DOS style issue Meta + x :
M-x set-buffer-file-coding-system utf-8-dos

** 2015-11-12-adopting-a-bullet-point-style
#+name:adopting-a-bullet-point-style-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-11-12-adopting-a-bullet-point-style.md :exports none :eval no :padline no
---
name: adopting-a-bullet-point-style
layout: post
title: Adopting a bullet point style
date: 2015-11-12
categories:
- disentangle
tags:
- writing 
---

With respect to bullet points:

- there are a range of styles accepted
- you just have to be consistent
- I have decided I like the bullet point style from
[http://www.monash.edu/about/editorialstyle/editing/punctuation](http://www.monash.edu/about/editorialstyle/editing/punctuation).

```
End the introductory phrase preceding a list of bullet points in a
colon. If the individual bullet points are sentence fragments, don't
use a full stop, comma or semi-colon. Leave it bare until the last
bullet point, and then use a full stop. Don't use capitals. Use full
stops if each bullet point is a complete sentence.
```
#+end_src

* Blogging
** COMMENT blog-code
#+name:blog
#+begin_src sh :session *shell* :tangle no :exports none :eval yes
#### name:blog ####
cd ~/projects/ivanhanigan.github.com.raw
jekyll b
cp -r ~/projects/ivanhanigan.github.com.raw/_site/* ~/projects/ivanhanigan.github.com
cd ~/projects/ivanhanigan.github.com
jekyll serve
#cd ~/
#./.bash_profile
#bb

#+end_src

#+RESULTS: blog
|                                                              |                                                              |                     |                                                                    |            |           |         |                         |      |     |             |
| ivan_hanigan@jadehawk:~/projects/ivanhanigan.github.com.raw$ | Configuration                                                | file:               | /home/ivan_hanigan/projects/ivanhanigan.github.com.raw/_config.yml |            |           |         |                         |      |     |             |
| Source:                                                      | /home/ivan_hanigan/projects/ivanhanigan.github.com.raw       |                     |                                                                    |            |           |         |                         |      |     |             |
| Destination:                                                 | /home/ivan_hanigan/projects/ivanhanigan.github.com.raw/_site |                     |                                                                    |            |           |         |                         |      |     |             |
| Generating...                                                |                                                              |                     |                                                                    |            |           |         |                         |      |     |             |
| [33m                                                       | Build                                                        | Warning:            | Layout                                                             | 'nil'      | requested | in      | atom.xml                | does | not | exist.[0m |
| [33m                                                       | Build                                                        | Warning:            | Layout                                                             | 'nil'      | requested | in      | entries.xml             | does | not | exist.[0m |
| [33m                                                       | Build                                                        | Warning:            | Layout                                                             | 'rss-feed' | requested | in      | feed.xml                | does | not | exist.[0m |
| [33m                                                       | Build                                                        | Warning:            | Layout                                                             | 'nil'      | requested | in      | feed/index.xml          | does | not | exist.[0m |
| done.                                                        |                                                              |                     |                                                                    |            |           |         |                         |      |     |             |
| Auto-regeneration:                                           | disabled.                                                    | Use                 | --watch                                                            | to         | enable.   |         |                         |      |     |             |
| ivan_hanigan@jadehawk:~/projects/ivanhanigan.github.com.raw$ | ivan_hanigan@jadehawk:~/projects/ivanhanigan.github.com$     | [33mConfiguration | file:                                                              | none[0m  |           |         |                         |      |     |             |
| Source:                                                      | /home/ivan_hanigan/projects/ivanhanigan.github.com           |                     |                                                                    |            |           |         |                         |      |     |             |
| Destination:                                                 | /home/ivan_hanigan/projects/ivanhanigan.github.com/_site     |                     |                                                                    |            |           |         |                         |      |     |             |
| Generating...                                                |                                                              |                     |                                                                    |            |           |         |                         |      |     |             |
| [33m                                                       | Build                                                        | Warning:            | Layout                                                             | 'default'  | requested | in      | data-doco.md            | does | not | exist.[0m |
| [33m                                                       | Build                                                        | Warning:            | Layout                                                             | 'default'  | requested | in      | energymark.md           | does | not | exist.[0m |
| [33m                                                       | Build                                                        | Warning:            | Layout                                                             | 'default'  | requested | in      | pumilio-bushfm-index.md | does | not | exist.[0m |
| done.                                                        |                                                              |                     |                                                                    |            |           |         |                         |      |     |             |
| Auto-regeneration:                                           | enabled                                                      | for                 | '/home/ivan_hanigan/projects/ivanhanigan.github.com'               |            |           |         |                         |      |     |             |
| [33mConfiguration                                          | file:                                                        | none[0m           |                                                                    |            |           |         |                         |      |     |             |
| jekyll                                                       | 2.5.2                                                        |                     |                                                                    | Error:     | Address   | already | in                      | use  | 0   | bind(2)     |

** 2015-09-22-open-notebook-science-jekyll-blogs-github-and-jerry-seinfelds-secret-to-productivity
#+name:open-notebook-science-jekyll-blogs-github-and-jerry-seinfelds-secret-to-productivity-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-09-22-open-notebook-science-jekyll-blogs-github-and-jerry-seinfelds-secret-to-productivity.md :exports none :eval no :padline no
---
name: open-notebook-science-jekyll-blogs-github-and-jerry-seinfelds-secret-to-productivity
layout: post
title: Open Notebook Science, Jekyll Blogs, Github and Jerry Seinfeld's Secret to Productivity
date: 2015-09-22
categories:
- disentangle
tags:
- blogging
---
  
The other day I reported that I've implemented a new open science task management regime [http://ivanhanigan.github.io/2015/09/task-management-like-an-open-science-hacker/](http://ivanhanigan.github.io/2015/09/task-management-like-an-open-science-hacker/).

This was instigated by my renewed enthusiasm for open science after a few high profile papers have come out in the last few months imploring scientists to take action on shonky statistics and the "morass of poorly conducted data analyses, with errors ranging from trivial and strange to devastating" (Peng 2015) [http://dx.doi.org/10.1111/j.1740-9713.2015.00827.x](http://dx.doi.org/10.1111/j.1740-9713.2015.00827.x).

I believe that making ones electronic notebook open is one of the most obvious and easily achieved things to do toward that ambition.  I also think that keeping the TODO-list in the forefront of ones mind and continuously checking things off the list is a great boost for productivity and keeping on track.  This culminates in the advice to keep momentum by doing something toward the plan on a daily basis, no matter how trivial.  This is sometimes called Jerry Seinfeld's secret to productivity: Just keep at it. Don't break the streak. [http://dirk.eddelbuettel.com/blog/2014/10/12/](http://dirk.eddelbuettel.com/blog/2014/10/12/).

So what was holding me back from a really useful daily publication of my open notebook?  I showed last post how I manage tasks in Emac Orgmode (a task organiser and calendar/agenda rolled up with code execution for running R scripts etc).  I also write my blog posts in orgmode.

The only problem with that set up was that I was still using the code from Charlie Park [http://charliepark.org/jekyll-with-plugins/](http://charliepark.org/jekyll-with-plugins/) which adds the inadequate commit description '`Latest build`' every time.  What I needed was a way to actually log a summary of work each day, so I can look back over the history and know I actually did something everyday and was not just gaming the system by committing random little non-work additions (I want to balance this by doing _some_ work every day, but also take time off to read, exercise, socialize, and generally have fun).

So anyway, the point of this post is to describe my revision to Charlie Park's code for building a jekyll blog:

#### Code: put in ~/.bash_profile
    function bb() {
      cd ~/projects/ivanhanigan.github.com.raw && jekyll b && cp -r    
      ~/projects/ivanhanigan.github.com.raw/_site/* ~/projects/ivanhanigan.github.com && 
      cd ~/projects/ivanhanigan.github.com && git add . -A  && 
      git commit -m "$*" && 
      git push
    }
    
<p></p>


- That bit about `$*` was a bit difficult for me to get working as this is the first time I have written a bash script in anger.  The alternative was to use `$1` and require the git commit message to be passed within quotes, which also makes sense but I did not do that.
- I also needed to change the terminal settings so that it always loads the bash_profile

#### Bash terminal
    Edit > Profile preferences
    Title and Command > Run command as a login shell 

<p></p>

- And so now I just have to deposit a markdown blog post into the jekyll `_posts` folder and then

#### Bash
    bb Add a meaningful commit message about todays progress
      
    
<p></p>

There you have it, a meaningful message regarding what I have been doing towards my scientific output every day.

![/images/seinfeld-streak-day9.png](/images/seinfeld-streak-day9-1.png)

![/images/seinfeld-streak-day9.png](/images/seinfeld-streak-day9.png)

  


#### References
    Peng, R. (2015). The reproducibility crisis in science: 
    A statistical counterattack. Significance, 12(3), 30–32. 
    doi:10.1111/j.1740-9713.2015.00827.x
    
    
<p></p>
      
#+end_src

** 2015-10-04-open-notebook-blogging-vs-twitter-or-can-i-do-both
#+name:open-notebook-blogging-vs-twitter-or-can-i-do-both-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-10-04-open-notebook-blogging-vs-twitter-or-can-i-do-both.md :exports none :eval no :padline no
---
name: open-notebook-blogging-vs-twitter-or-can-i-do-both
layout: post
title: Open Notebook Blogging Vs Twitter? Or Can I Do Both?
date: 2015-10-04
categories:
- disentangle
tags:
- blogging
---

- I was tooling around someone's blog and noticed a link to this interesting talk "Citation & Productivity Benefits from Open Science" by [https://github.com/BillMills](https://github.com/BillMills)
- the slides markdown is here [https://github.com/BillMills/practicalOpenScience/blob/gh-pages/outline.md](https://github.com/BillMills/practicalOpenScience/blob/gh-pages/outline.md)
- the slide on "Open Communication" interested me, as I have turned up my open notebook blog to 11 recently with a couple of posts a week for the last few weeks 
- the recommendation to "Blog Early And Blog Often" resonated here!
- I was intrigued by the comments

```
  
used well, twitter can be a useful tool for frequent communication
paradigms as a distributor and aggregator of links to the content in
the other three bullet points. It can be really tough to stay on top
of everyone's blog, everyone's issue tracker and everything else; by
pushing links to our followers every time we have a new RFC out and
vice versa, we greatly simplify this process. See this example.
  
```
<p></p>
- the other three being: GitHub Issues/Working Open, Blogging/Journals of Brief Ideas and
Study Preregistration/'publication bias'
- RFC means 'Request For Comments'
- and so I clicked on the link to this example [https://twitter.com/MozillaScience/status/628990222651428864](https://twitter.com/MozillaScience/status/628990222651428864)

![/images/twits.png](/images/twits.png)

## Warning, Danger

- This seemed like a fascinating discussion, one that I have been thinking about a lot 
- and writing up here [https://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets](https://ivanhanigan.github.com/2015/09/reproducible-research-and-managing-digital-assets) in my thesis BUT...
- I also recently received this warning from a blogger I admire: [http://charliepark.org/stepping-away-from-twitter/](http://charliepark.org/stepping-away-from-twitter/)
- He warns strongly that people with my temperament for seeking feedback and approval may be hampered by using twitter too much:

```
Stepping Away From Twitter
June 12, 2015
I recently noticed an interesting pattern.
On days when I’m on Twitter, my ability to focus and 
get good work done falls off a cliff.

It’s not just “when you read Twitter first-thing in the day”, 
something I’ve heard people discuss. 
It was that I was on Twitter at all.
```
<p></p>

## But there are clear benefits right?

- So I had to take a step back from my recent explorations with linking up my social media and blogging interests
- Looking over what I am trying to achieve by keeping regular posts of my work, along with the need I feel to make sure people who are interested can _find_ my work, I began to despair
- but then I kept digging around and see a lot of people I admire linking up twitter and more scientific communications

## So what?

- I guess the real point of this post is to admit that I don't know how to use twitter
- I'm happy to report I feel comfortable now (a couple of years in) making regular open notebook entries to communicate what is going on in my 'Lonely Analyst' lab [http://simplystatistics.org/2013/08/09/embarrassing-typos-reveal-the-dangers-of-the-lonely-data-analyst/](http://simplystatistics.org/2013/08/09/embarrassing-typos-reveal-the-dangers-of-the-lonely-data-analyst/)
- but a couple of recent emails asking me for data that I published years ago [like this Australian Postcode Area Weather Estimates stuff](https://gislibrary-extreme-weather.anu.edu.au/poa_weather) made me think I really need to get my stuff exposed so it is **LOT** more visible.
    
#+end_src


** TODO embedding pdfs into webpages
http://stackoverflow.com/a/14081168


You could consider using PDFObject by Philip Hutchison.

Alternatively, if you're looking for a non-Javascript solution, you could use markup like this:

<object data="myfile.pdf" type="application/pdf" width="100%" height="100%">
  <p>Alternative text - include a link <a href="myfile.pdf">to the PDF!</a></p>
</object>
** 2015-11-26-visualisation-tools-for-communicating-data-management-concepts
#+name:visualisation-tools-for-communicating-data-management-concepts-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-11-26-visualisation-tools-for-communicating-data-management-concepts.md :exports none :eval no :padline no
---
name: visualisation-tools-for-communicating-data-management-concepts
layout: post
title: Visualisation tools for communicating data management concepts
date: 2015-11-26
categories:
- disentangle 
---

# Hyperlinked table of contents that looks like a filing system

This looks like it might be useful to display information about filing systems, with a clickable toc that looks like a filing system!

Source:

- [http://tex.stackexchange.com/a/36185](http://tex.stackexchange.com/a/36185)

<object data="/images/tikz_hlink.pdf" type="application/pdf" width="600" height="800">
  <p>Alternative text - include a link <a href="images/tikz_hlink.pdf">to the PDF!</a></p>
</object>

    
#+end_src


* bibliometrics and literature reviewing
*** COMMENT gscraper-func-code
#+name:gscraper-func
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:gscraper-func ####
  # File-Name: GScholarScraper_3.1.R
  # Date: 2012-08-22
  # Author: Kay Cichini
  # Email: kay.cichini@gmail.com
  # Purpose: Scrape Google Scholar search result
  # Packages used: XML
  # Licence: CC BY-SA-NC
  #
  # Arguments:
  # (1) input:
  # A search string as used in Google Scholar search dialog
  #
  # (2) write:
  # Logical, should a table be writen to user default directory?
  # if TRUE ("T") a CSV-file with hyperlinks to the publications will be created.
  #
  # Difference to version 3:
  # (3) added "since" argument - define year since when publications should be returned..
  # defaults to 1900..
  #
  # (4) added "citation" argument - logical, if "0" citations are included
  # defaults to "1" and no citations will be included..
  # added field "YEAR" to output 
  #
  # Caveat: if a submitted search string gives more than 1000 hits there seem
  # to be some problems (I guess I'm being stopped by Google for roboting the site..)
  #
  # And, there is an issue with this error message:
  # > Error in htmlParse(URL): 
  # > error in creating parser for http://scholar.google.com/scholar?q
  # I haven't figured out his one yet.. most likely also a Google blocking mechanism..
  # Reconnecting / new IP-address helps..
  
  
  GScholar_Scraper <- function(input, since = 1900, write = F, citation = 1) {
  
      require(XML)
  
      # putting together the search-URL:
      URL <- paste("http://scholar.google.com/scholar?q=", input, "&as_sdt=1,5&as_vis=", 
                   citation, "&as_ylo=", since, sep = "")
      cat("\nThe URL used is: ", "\n----\n", paste("* ", "http://scholar.google.com/scholar?q=", input, "&as_sdt=1,5&as_vis=", 
                   citation, "&as_ylo=", since, " *", sep = ""))
      
      # get content and parse it:
      doc <- htmlParse(URL)
      
      # number of hits:
      h1 <- xpathSApply(doc, "//div[@id='gs_ab_md']", xmlValue)
      h2 <- strsplit(h1, " ")[[1]][2] 
      num <- as.integer(sub("[[:punct:]]", "", h2))
      cat("\n\nNumber of hits: ", num, "\n----\n", "If this number is far from the returned results\nsomething might have gone wrong..\n\n", sep = "")
      
      # If there are no results, stop and throw an error message:
      if (num == 0 | is.na(num)) {
          stop("\n\n...There is no result for the submitted search string!")
      }
      
      pages.max <- ceiling(num/100)
      
      # 'start' as used in URL:
      start <- 100 * 1:pages.max - 100
      
      # Collect URLs as list:
      URLs <- paste("http://scholar.google.com/scholar?start=", start, "&q=", input, 
                    "&num=100&as_sdt=1,5&as_vis=", citation, "&as_ylo=", since, sep = "")
      
      scraper_internal <- function(x) {
          
          doc <- htmlParse(x, encoding="UTF-8")
          
          # titles:
          tit <- xpathSApply(doc, "//h3[@class='gs_rt']", xmlValue)
          
          # publication:
          pub <- xpathSApply(doc, "//div[@class='gs_a']", xmlValue)
          
          # links:
          lin <- xpathSApply(doc, "//h3[@class='gs_rt']/a", xmlAttrs)
          
          # summaries are truncated, and thus wont be used..  
          # abst <- xpathSApply(doc, '//div[@class='gs_rs']', xmlValue)
          # ..to be extended for individual needs
          options(warn=(-1))
          dat <- data.frame(TITLES = tit, PUBLICATION = pub, 
                            YEAR = as.integer(gsub(".*\\s(\\d{4})\\s.*", "\\1", pub)),
                            LINKS = lin)
          options(warn=0)
          return(dat)
      }
  
      result <- do.call("rbind", lapply(URLs, scraper_internal))
      if (write == T) {
        result$LINKS <- paste("=Hyperlink(","\"", result$LINKS, "\"", ")", sep = "")
        write.table(result, "GScholar_Output.CSV", sep = ";", 
                    row.names = F, quote = F)
        shell.exec("GScholar_Output.CSV") 
        } else {
        return(result)
      }
  }
#+end_src

#+RESULTS: gscraper-func

*** COMMENT gscraper-code
#+name:gscraper
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:gscraper ####
  
  # EXAMPLES:
  # 1:
  input <- "intitle:metapopulation"
  df <- GScholar_Scraper(input, since = 1980, citation = 1)
  nrow(df)
  hist(df$YEAR, xlab = "Year", 
       main = "Frequency of Publications with\n\"METAPOPULATION\" in Title")
  # 2:
  input <- "Suicide and Drought"
  rm(df)
  df  <-  GScholar_Scraper(input, since = 2006, citation = 1)
  nrow(df)
  str(df)
  df[[1]][1]
  # 3:
  input <- "allintitle:ziggy stardust"
  GScholar_Scraper(input, write = T)
  # 4: ERROR with message:
  input <- "allintitle:crazyshit"
  GScholar_Scraper(input)
  # 5: CAVEAT, Google blocks automated requests at about the 1000th hit:
  input <- "metapopulation"
  df <- GScholar_Scraper(input, since = 1980)
  nrow(df)
  
  # 6: this also leads to this error for example no. 1,
  # because when including citations (.., citation = 0) 1000 hits are exceeded, 
  # Google blocks and dataframe generation is not working..
  input <- "intitle:metapopulation"
  df <- GScholar_Scraper(input, since = 1980, citation = 0)
  
  
#+end_src

** 2015-11-13-judging-the-evidence-using-a-literature-review-database
*** judging-the-evidence-using-a-literature-review-database-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-11-13-judging-the-evidence-using-a-literature-review-database.md :exports none :eval no :padline no
---
name: judging-the-evidence-using-a-literature-review-database
layout: post
title: Judging the evidence using a literature review database
date: 2015-11-13
categories:
- disentangle
tags:
- bibliometrics and literature reviewing
---

I recently read through a lecture slide deck called 'Judging the Evidence' by  Adrian Sleigh for a course PUBH7001 Introduction to Epidemiology, April 30, 2001.

It had a lot of great material in it but I especially liked the section 'CRITIQUE OF AN EPIDEMIOLOGIC STUDY' and slide 11 'Quantity of data, duplication' which says:

```
Set clear criteria for admission of studies to your ‘judgement of evidence’
Devise ways to tabulate the information
‘Evidence tables’ show key features of design 
  (source, sample and study pop, N)
  exposures-outcomes measured
  observation methods
  confounding
  key results
```    
<p></p>

I thought this was a great idea, to build a database for keeping 'evidence tables' for each study I read.

I then read through all the slides.  There is a lot of great information here, but it was spread out across the narrative.  I realised I wanted to collate these into a 'evidence table'. I also compared this with my understanding of the Ecological Metadata Language (EML) schema and the 'ANU Data Analysis Plan Template' and have put together a bit of a 'cross-walk' that lets me combine all this info and create a evidence table (database).  

I have started to use the database I built which uses EML concepts heavily and I include some these other ideas into my free `data_inventory` application for a web2py database [https://github.com/ivanhanigan/data_inventory](https://github.com/ivanhanigan/data_inventory).

It is a webform style data entry interface, and I think good for these 'evidence tables'.
In the first instance I piggy back a lot of the elements into single EML tags, especially the abstract.
This may make it hard to parse.  The simple solution is to try to keep each element on a seperate line of the absract.

## The key info for an evidence table entry per study

<table border=1>
<tr> <th> EML </th> <th> ANU </th> <th> Adrian_Sleigh </th>  </tr>
  <tr> <td> dataset/title </td> <td>  Study name </td> <td>   </td> </tr>
  <tr> <td> dataset/creator </td> <td>  Person conducting analysis </td> <td>   </td> </tr>
  <tr> <td> project/personnel/[data_owner or orginator] </td> <td>  Chief investigator </td> <td>   </td> </tr>
  <tr> <td> dataset/abstract </td> <td>  Background to the study </td> <td>  Purpose of Study </td> </tr>
  <tr> <td>          </td> <td>   Study research question  </td> <td>  </td> </tr>
  <tr> <td>          </td> <td>   Specific hypothesis under study </td> <td>   </td> </tr>
  <tr> <td>          </td> <td>  outcomes of interest/ Exposure variables /Covariates </td> <td>  exposures-outcomes measured </td> </tr>
  <tr> <td>          </td> <td>   </td> <td>  key results </td> </tr>
  <tr> <td> dataset/studyextent </td> <td>  Study population </td> <td>  Study Setting </td> </tr>
  <tr> <td>                     </td> <td>                   </td> <td>  source / sample and study pop </td> </tr>
  <tr> <td> dataset/temporalcoverage </td> <td>  Duration of study </td> <td>   </td> </tr>
  <tr> <td> dataset/methods_protocol </td> <td>  Study Type </td> <td>  Type of study </td> </tr>
  <tr> <td> dataset/sampling_desc </td> <td>    </td> <td>  Subject Selection </td> </tr>
  <tr> <td> dataset/methods_steps </td> <td>  analytical strategy </td> <td>  Statistical procedures </td> </tr>
  <tr> <td>                       </td> <td>  exposures/ potential confounders or effect modifiers </td> <td>  Confounding </td> </tr>
  <tr> <td> entity/numberOfRecords </td> <td>  Number study subjects </td> <td>  N </td> </tr>
  <tr> <td> dataset/distribution_methods </td> <td>  dissemination strategy </td> <td>    </td> </tr>
   </table>


<p></p>

## Here is a screen shot of my data inventory data entry form

![/images/datinv_entry.png](/images/datinv_entry.png)

#+end_src
*** COMMENT table
#+name:table
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:table ####
  # sources
  #ANU
  # /home/ivan_hanigan/Dropbox/projects/DataManagementPolicy/References/NCEPH-Handbook
  # Adrian /home/ivan_hanigan/Dropbox/references/Sleigh-epi-lectures/Judging2001.ppt
  # EML ~/tools/web2py/applications/data_inventory/
  dat <- read.csv(textConnection("EML, ANU, Adrian_Sleigh
  dataset/title, Study name, 
  dataset/creator, Person conducting analysis, 
  project/personnel/[data_owner or orginator], Chief investigator, 
  dataset/abstract, Background to the study, Purpose of Study
          ,  Study research question ,
          ,  Specific hypothesis under study, 
          , outcomes of interest/ Exposure variables /Covariates, exposures-outcomes measured
          , , key results
  dataset/studyextent, Study population, Study Setting
                     ,                 , source / sample and study pop
  dataset/temporalcoverage, Duration of study, 
  dataset/methods_protocol, Study Type, Type of study
  dataset/sampling_desc,  , Subject Selection
  dataset/methods_steps, analytical strategy, Statistical procedures
                       , exposures/ potential confounders or effect modifiers, Confounding
  entity/numberOfRecords, Number study subjects, N
  dataset/distribution_methods, dissemination strategy,  
  "))
  print(xtable::xtable(dat), type = "html", include.rownames = F)
#+end_src

#+RESULTS: table
| <!-- html table generated in R 3.2.2 by xtable 1.7-4 package -->   |
| <!-- Sat Nov 14 10:12:49 2015 -->                                  |
| <table border=1>                                                   |
| <tr> <th> data_inventory_field </th> <th> description </th>  </tr> |
| <tr> <td> Citation </td> <td>  </td> </tr>                         |
| <tr> <td> Key results </td> <td>  </td> </tr>                      |
| <tr> <td> Background to the study </td> <td>  </td> </tr>          |
| <tr> <td> Research question </td> <td>  </td> </tr>                |
| <tr> <td> Study extent </td> <td>  </td> </tr>                     |
| <tr> <td> Outcomes </td> <td>  </td> </tr>                         |
| <tr> <td> Exposures </td> <td>  </td> </tr>                        |
| <tr> <td> Covariates </td> <td>  </td> </tr>                       |
| <tr> <td> Method protocol </td> <td>  </td> </tr>                  |
| <tr> <td> General Comments </td> <td>  </td> </tr>                 |
| </table>                                                           |
|                                                                    |

** 2015-11-14-developing-a-lit-review-database
#+name:developing-a-lit-review-database-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-11-14-developing-a-lit-review-database.md :exports none :eval no :padline no
---
name: developing-a-lit-review-database
layout: post
title: Developing a lit review database
date: 2015-11-14
categories:
- disentangle
tags:
- bibliometrics and literature reviewing
---


My work yesterday on implementing [a lit review section of my data inventory database](/2015/11/judging-the-evidence-using-a-literature-review-database) 
went pretty well, and I tested this while collating information on the papers I compile into my thesis.

However, as I worked through the information for each paper I realised that attaching this stuff at the EML/dataset level is not always going to work.
In particular I have projects in which several papers are part of the one dataset.  To deal with this I have invented a non-EML relationship module for publications.
So in my new schema the following is possible

```
eml/project/
           /dataset1/
                    /publication1
                    /publication2
           /dataset2/
                    /etc  
```

<p></p>

In this scenario a single dataset (ie bushfire smoke, temperature and mortality) can be used to write one paper focused on smoke, controlling for temperature.  Another paper on heatwaves, controlling for smoke. Indeed we might use time-series Poisson models for one and case-crossover design for the other (this is similar to what I did with Johnston and Morgan).

So the simple thing to do is input these 'evidence tables' fields at the publication level, rather than the dataset as I did yesterday.

This also partially solves my problem about aggregating these non-EML tags inside the text of the abstract, and worrying about parsing that to extract the elements of information.

## The fields

<!-- html table generated in R 3.2.2 by xtable 1.7-4 package -->
<!-- Sat Nov 14 10:12:49 2015 -->
<table border=1>
<tr> <th> data_inventory_field </th> <th> description </th>  </tr>
  <tr> <td> Citation </td> <td> At a minimum author-date-journal, perhaps DOI? </td> </tr>
  <tr> <td> Key results </td> <td> Include both effect estimates and uncertainty </td> </tr>
  <tr> <td> Background to the study </td> <td>  </td> </tr>
  <tr> <td> Research question </td> <td>  </td> </tr>
  <tr> <td> Study extent </td> <td>  </td> </tr>
  <tr> <td> Outcomes </td> <td>  </td> </tr>
  <tr> <td> Exposures </td> <td>  </td> </tr>
  <tr> <td> Covariates </td> <td> Include covariates, effect modifiers, confounders and subgroups </td> </tr>
  <tr> <td> Method protocol </td> <td>  </td> </tr>
  <tr> <td> General Comments </td> <td>  </td> </tr>
   </table>
<p></p>

## An example

![/images/datinv_pub.png](/images/datinv_pub.png)
    
#+end_src



- Citation: Powers, J. R., Dobson, A. J., Berry, H. L., Graves, A. M., Hanigan, I. C., and Loxton, D. (2015). Lack of association between drought and mental health in a cohort of 45-61 year old rural Australian women. ANZJPH. http://dx.doi.org/doi:10.1111/1753-6405.12369
- Key results: Drought was not associated with depression in mid-aged women in general, or in groups of potentially vulnerable women
- Background to the study: Quantitative evidence on the effect of drought on mental health is scant and inconclusive.
- Research question: Is drought associated with poorer mental health in rural women?
- Study extent: Large nationally representative sample of 45 to 61 year old women, data were collected prospectively over 12 years (96/07)
- Outcomes: Medical Outcomes Study Short Form 36 (SF36), where score less than 53 = depression
- Exposures: Hutchinson Drought Index
- Covariates: Looked for effect modification by isolation, poverty, education, co-morbidities etc. 
- Method protocol: Linear mixed models of longitudinal data, taking into account the correlation within individuals
- General Comments: Given the strong suicide link in men shown by Hanigan et al 2012, this result implies a complicated relationship between drought and depression modified by gender.

*** COMMENT table
#+name:table
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:table ####
  # sources
  #ANU
  # /home/ivan_hanigan/Dropbox/projects/DataManagementPolicy/References/NCEPH-Handbook
  # Adrian /home/ivan_hanigan/Dropbox/references/Sleigh-epi-lectures/Judging2001.ppt
  # EML ~/tools/web2py/applications/data_inventory/
  dat <- read.csv(textConnection("data_inventory_field, description
  Citation,
  Key results,
  Background to the study,
  Research question,
  Study extent, 
  Outcomes,
  Exposures,
  Covariates,
  Method protocol,       
  General Comments,
  "))
  print(xtable::xtable(dat), type = "html", include.rownames = F)
#+end_src


** 2015-11-15-putting-bibtex-key-into-my-lit-review-database-needs-sanitized-latex-characters
#+name:putting-bibtex-key-into-my-lit-review-database-needs-sanitized-latex-characters-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-11-15-putting-bibtex-key-into-my-lit-review-database-needs-sanitized-latex-characters.md :exports none :eval no :padline no
---
name: putting-bibtex-key-into-my-lit-review-database-needs-sanitized-latex-characters
layout: post
title: Putting bibtex key into my lit review database needs sanitized latex characters
date: 2015-11-15
categories:
- disentangle
---

As I compile my papers for the thesis I am keeping notes for my presentation to discuss the results and scope of each study.  The work on the 'evidence tables' I described in the last two posts has proved useful here.  I allowed my self a breif distraction to dig out some code I had developed for a database of case studies demonstrating eco-social tipping points from historical evidence, and show here how to include bibtex info.  The bibtex key (from Mendeley in my case) is added to the database, and then the following code:


```
library(xtable)
library(rpostgrestools) # my own work
if(!exists("ch")) ch <- connect2postgres2("data_inventory_hanigan_dev4")

dat <- dbGetQuery(ch,
"SELECT id, dataset_id, bibtex_key, title,  key_results,background_to_study, 
       research_question, study_extent, outcomes, exposures,  covariates, method_protocol,
        general_comments
  FROM publication
  where key_results is not null and key_results != '';
")
names(dat) <- gsub("_", " ", names(dat))
tabcode <- xtable(dat[,1:8])
align(tabcode) <-  c( 'l', 'p{.7in}','p{.8in}','p{1.7in}', 'p{1.7in}', 'p{1.7in}','p{1.8in}','p{1.7in}', 'p{1.7in}')
print(tabcode,  include.rownames = F, table.placement = '!ht',
      floating.environment='sidewaystable',
      sanitize.text.function = function(x) x)

```

<p></p>

Produces the below table:

![/images/bibtable.png](/images/bibtable.png)


#+end_src

*** COMMENT aa
#+name:aa
#+begin_src R :session *R* :tangle sanitize_bib_table.Rmd :exports none :eval no
  ---
  title: "Using bibtex key in tables "
  author: Ivan C. Hanigan
  header-includes:
    - \usepackage{rotating}
  output: 
    pdf_document:
      keep_tex: true
  ---
  
  ```{r, results = 'asis'}
  library(xtable)
  library(rpostgrestools) # my own work
  if(!exists("ch")) ch <- connect2postgres2("data_inventory_hanigan_dev4")
  
  dat <- dbGetQuery(ch,
  "SELECT id, dataset_id, bibtex_key, title,  key_results,background_to_study, 
         research_question, study_extent, outcomes, exposures,  covariates, method_protocol,
          general_comments
    FROM publication
    where key_results is not null and key_results != '';
  ")
  names(dat) <- gsub("_", " ", names(dat))
  tabcode <- xtable(dat[,1:8])
  align(tabcode) <-  c( 'l', 'p{.7in}','p{.8in}','p{1.7in}', 'p{1.7in}', 'p{1.7in}','p{1.8in}','p{1.7in}', 'p{1.7in}')
  print(tabcode,  include.rownames = F, table.placement = '!ht',
        floating.environment='sidewaystable',
        sanitize.text.function = function(x) x)
  
  ```
#+end_src
*** COMMENT go
#+name:go
#+begin_src sh :session *shell* :tangle no :exports none :eval no
#### name:go ####

#+end_src

** USyd framework for environmental health risk assessment
Geoff Morgan.

Introduction/ Issue identification
(approx. 200 words)
1. identify the key issues 
2. identify if there are multiple interacting hazards or an isolated hazard
3. discuss the level of public anxiety related to the issue

Hazard identification
5 marks (approx. 200 words)
1. describe in general terms the toxicology and epidemiological evidence of health effects related to exposures of interest (in this assignment exposure can be associated with risks AND/OR benefits)

Dose response assessment
10 marks  (approx. 450 words)
1. quantify (if possible) the magnitude of health risks due to exposure from available research
2. quantify (if possible) the magnitude of any health benefits related to the activity from available research
3. identify standards / guidelines based on the exposure response relationship such as effect levels, thresholds, margins of safety.
4. discuss uncertainties and limitations

Exposure assessment 
5 marks (approx. 250 words)
1. Identify exposed populations (to risks AND/OR benefits)
2. describe exposure pathways
3. quantify typical population exposures (to risks AND/OR benefits) related to the issue 
4. describe uncertainties and assumptions

Risk characterization
10 marks (approx. 450 words)
1. quantify (if possible) the magnitude of health risks AND/OR benefits in exposed populations and make statements (quantitative and/ qualitative) about the magnitude / relative importance of health risk and benefit in exposed populations
2. describe any sensitive subgroups (eg adults, children…)
3. describe uncertainties and assumptions

Risk management / Conclusions / Recommendations
10 marks (approx. 450 words)
1. summarise the risks AND/OR benefits including any ways in which risks can be effectively managed and any benefits maximized – consider policies, procedures, programs, different types of controls that can be implemented
2. discuss the many different factors that influence decision making about risk management for the hazard– eg: health, economic, social, political
3. make recommendations based on data/evidence presented throughout your assignment

* Graphical User Interfaces
** web2py


*** TODO using the appadmin interface

query can be db.dataset.contact.like('Lach%')
* Web2py
** boot strap an app: sounds players
# download web2py
wget http://web2py.com/examples/static/web2py_src.zip
unzip web2py_src.zip
cd web2py

# make the app using only core API
mkdir applications/audio
cp -r applications/welcome/* applications/audio
echo "
db.define_table('music',
                Field('name',required=True),
                Field('filename','upload',required=True),
                auth.signature)
" > applications/audio/models/db_audio.py
echo "
response.menu = [('Home',0,URL('index'))]

# allows to browser, search, and upload new music (if logged in)
def index():
    # tell web2py that links to music files are to be
    # represented as HTML5 audio embedded players
    from gluon.contrib.autolinks import expand_one
    db.music.filename.represent = lambda v,r: \
        XML(expand_one(URL('download',args=v),{}))
    return dict(grid = SQLFORM.grid(db.music))

# perform login/logout/registration/etc.
def user():
    return dict(form=auth())

# allow streaming of all files, including partial content of music files
@cache.action()
def download():
    return response.download(request, db)
" > applications/audio/controllers/default.py

* XML
** COMMENT review-xml-code
#+name:review-xml
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:review-xml####
  require(XML)
  # first read one without a public access
  lnk <- "http://www.ltern.org.au/knb/metacat/ltern2.42.44/xml"
  xmlfile=xmlParse(lnk)
  xmltop = xmlRoot(xmlfile) 
  length(names(xmltop[[1]]))
  
  names(xmltop[[2]])
  names(xmltop[[2]]['project']['project'])
  names(xmltop[[2]]['project'][['project']]) == 'personnel'
  names(xmltop[[2]]['project'][[1]][[4]])
  xmltop[[2]]['project'][['project']][[4]]
  xmltop[[2]]['project'][['project']][[4]][['individualName']]
  xmltop[[2]]['project'][['project']][[4]][['role']]
  
  # the first
  xmltop[[2]]['project'][['project']][['personnel']][['role']]
  
  xmltop[[2]]['contact'][['contact']][['organizationName']]
  
  
  # lnk <- "http://dev.ltern.org.au/knb/metacat/datalibrarian.67/xml"
   lnk <- "http://dev.ltern.org.au/knb/metacat/ltern2.42/xml"
  xmlfile=xmlParse(lnk)
  xpath = xmlRoot(xmlfile)
  
  names(xpath)
  xpath[[1]]
  names(xpath[[2]])
  
  #xpath_dataset <- xpath[['dataset']]
  xmlValue(xpath[['dataset']][['title']])
  xpath[['dataset']]['creator']
  lapply(
    xpath[['dataset']]['creator']
    , xmlValue)
  
  # http://stackoverflow.com/a/22626191
  flatten_xml <- function(x) {
    if (length(xmlChildren(x)) == 0) structure(list(xmlValue(x)), .Names = xmlName(xmlParent(x)))
    else Reduce(append, lapply(xmlChildren(x), flatten_xml))
  }
  
  dfs <- lapply(getNodeSet(xpath,"//creator"), function(x) data.frame(flatten_xml(x)))
  allnames <- unique(c(lapply(dfs, colnames), recursive = TRUE))
  df <- do.call(rbind, lapply(dfs, function(df) { df[, setdiff(allnames,colnames(df))] <- NA; df }))
  head(df)
  
  
  
  xpath[['dataset']][['project']]
  names(xpath[['dataset']][['project']])
  xpath[['dataset']][['project']][['title']]
  lapply(xpath[['dataset']][['project']]['personnel'], xmlValue)
  dfs <- lapply(getNodeSet(xpath,"//personnel"), function(x) data.frame(flatten_xml(x)))
  allnames <- unique(c(lapply(dfs, colnames), recursive = TRUE))
  df <- do.call(rbind, lapply(dfs, function(df) { df[, setdiff(allnames,colnames(df))] <- NA; df }))
  head(df)
  
  
  xmlValue(xpath[['dataset']][['project']][['funding']])
  
#+end_src

