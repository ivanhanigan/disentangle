#+TITLE:Disentangle Things (overflow)
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----
* DESCRIPTION-code
#+name:DESCRIPTION
#+begin_src R :session *R* :tangle DESCRIPTION :exports none :eval no :padline no
Package: disentangle
Type: Package
Title: disentangle
Version: 1.4.4
Date: 2015-06-10
Author: ivanhanigan
Maintainer: <ivan.hanigan@gmail.com>
Suggests: stringr, ggmap, maps, maptools, rgdal
Description:  Functions I modified or created.
License: GPL (>= 2)
#+end_src


* Data Documentation
** 2015-03-19-r-eml-to-mitigate-risks-in-morpho-metacat-data-publishing


#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-03-19-r-eml-to-mitigate-risks-in-morpho-metacat-data-publishing.md :exports none :eval no :padline no
---
name: r-eml-to-mitigate-risks-in-morpho-metacat-data-publishing
layout: post
title: Using the R EML software to mitigate risks in Morpho and Metacat data publishing
date: 2015-03-19
categories:
- morpho
- data documentation
---

# Introduction

- Over the last few months I have used software called Metacat as a Data Portal and Repository.  Metacat is server software which has been
developed by the Knowledge Network for Biocomplexity (KNB). 
- Metacat
conforms to the Ecological Metadata Language (EML) Standard ([https://knb.ecoinformatics.org/#external//emlparser/docs/index.html](https://knb.ecoinformatics.org/#external//emlparser/docs/index.html)).  
- KNB also develop another software package called Morpho to be used by Ecologists to document their data ([https://knb.ecoinformatics.org/#tools/morpho](https://knb.ecoinformatics.org/#tools)).
- Morpho can be used to send the data and metadata documents to be published on a Metacat portal. 
- KNBâ€™s software is used internationally by the Data
Observation Network for Earth (DataONE) nodes, the United States Long
Term Ecological Research (US LTER) network and the International Long
Term Ecological Research (ILTER) network. 
- Additionally, the Australian Long Term Ecological Research Network 
Data Portal ([www.ltern.org.au/knb/](http://www.ltern.org.au/knb/)), Australian SuperSites Network and
Australian Centre for Ecological Analysis and Synthesis used
the same underlying technology to publish data packages.
- The Metacat system is great for a data repository but unfortunately (in my experience) the Morpho software package has repeatedly hampered data processing and increased risks of inadvertently publishing data with errors. 
- My colleagues and I workaround these problems using a lot of different 'fixes' for the different problems.
- Fortunately there is an alternative to Morpho in the R statistical software environment called the R-EML package ([https://github.com/ropensci/EML](https://github.com/ropensci/EML)).  This provides a library of functions used in the R language to generate and parse EML files.
- This new workflow mitigates some of the risks of the Morpho software by ensuring the data related steps of the workflow are conducted in the R environment for statical computing.
- However, some Issues remain in that this requires a fairly specialised computing environment with various Linux libraries configured appropriately


# Results
 
- I generate EML metadata using REML in the workflow shown in the figure below.

![altext](/images/workflow-rmd-md.png)

<p></p>

- Image adapted from [http://kieranhealy.org/blog/archives/2014/01/23/plain-text/](http://kieranhealy.org/blog/archives/2014/01/23/plain-text/)


#+end_src

** TODO xxx-adding-value-labels-to-reml-boilerplate
*** adding-value-labels-to-reml-boilerplate-header
#+begin_src R :tangle no :exports none :eval no :padline no
  ---
  name: adding-value-labels-to-reml-boilerplate
  layout: post
  title: adding-value-labels-to-reml-boilerplate
  date: 2014-08-22
  categories:
  -
  ---
  
  #### Code:adding-value-labels-to-reml-boilerplate
  #For sake of argument, imagine a data.frame looking something like this:
  
  
  dat = data.frame(river=c("SAC", "SAC", "AM"),
                   spp = c("king", "king", "ccho"),
                   stg = c("smolt", "parr", "smolt"),
                   ct =  c(293, 410, 210L))
  
  xtable::xtable(dat)
  
  
  
  #In case our column header abbreviations are not obvious to others (or our future selves), we take a moment to define them:
  
  
  col_metadata = c(river = "http://dbpedia.org/ontology/River",
                   spp = "http://dbpedia.org/ontology/Species",
                   stg = "Life history stage",
                   ct = "count")
  
  
  
  # Define the units used in each column.  In the case of factors, we define the abbreviations in a named string.
  
  
  unit_metadata =
    list(river = c(SAC = "The Sacramento River", AM = "The American River"),
         spp = c(king = "King Salmon", ccho = "Coho Salmon"),
         stg = c(parr = "third life stage", smolt = "fourth life stage"),
         ct = "number")
  
  # automated?
  #dat <- dat[,-4]
  dat
  
#+end_src

*** COMMENT reml_boilerplate1-code
#+name:reml_boilerplate
# begin_src R :session *R* :tangle R/reml_boilerplate.r :exports reports :eval no
#+begin_src R :session *R* :tangle no :exports reports :eval no

  ################################################################
  # name:reml_boilerplate
   
  # func
  ## if(!require(EML)) {
  ##   require(devtools)
  ##   install_github("EML", "ropensci")
  ##   } 
  ## require(EML)
  
  reml_boilerplate <- function(data_set, outfile = NA, created_by = "Ivan Hanigan <ivanhanigan@gmail.com>", data_dir = getwd(), titl = NA)
  {
  
    # next create a list from the data
    unit_defs <- list()
    for(i in 1:ncol(data_set))
      {
        # i = 4
        if(is.numeric(data_set[,i])){
          unit_defs[[i]] <- "number"
        } else {
          unit_defs[[i]] <- names(data_set)[i]
        }
      }
  
  # print helpful comments
  cat(
  sprintf('
  # you just got a cheater\'s unit_defs
  # we can get the col names easily
  col_defs <- names(dat)
  # then create a dataset with metadata
  ds <- data.set(dat,
                 col.defs = col_defs,
                 unit.defs = unit_defs
                 )
  # now write EML metadata file
  eml_config(creator="%s")
  eml_write(ds,
            file = "%s",
            title = "%s"
            )
  
  # now your metadata has been created
  # if you want to add this to morpho and metacat it will needs something like
  </dataFormat>
    <distribution scope="document">
      <online>
        <url function="download">ecogrid://knb/hanigan.34.1</url>
      </online>
    </distribution>
  </physical>', created_by, outfile, titl)
  )
  
  
    return(unit_defs)
  
   }
#+end_src

*** COMMENT reml_boilerplate2-code
#+name:get_vals
# begin_src R :session *R* :tangle R/reml_boilerplate.r :exports none :eval no
#+begin_src R :session *R* :tangle no :exports none :eval no
  reml_boilerplate <- function(.dataframe){
  strng <- list()
  for(i in 1:ncol(.dataframe)){
    # i = 6
    .variable <- names(.dataframe)[i]
    #.dataframe[,.variable]
      if(is.character(.dataframe[ ,.variable])){
        .dataframe[,.variable]  <- factor(.dataframe[,.variable])
      }
  
    if(is.factor(.dataframe[,.variable])){
      x <- .dataframe[,.variable]
      vals <-  names(table(x))
      # symbols may pollute the string to parse
      vals <- make.names(vals)
      vals <- tolower(vals)  
      vals <- gsub("\\.","_",vals)
      vals <- gsub("_+","_",vals)    
      v <- .variable
      #v
      strng[[.variable]] <- paste(
      v, ' = c(',
      paste(vals, sep = '', collapse = ' = "TBA",')
      ,' = "TBA")', sep = '')
    } else if(is.numeric(.dataframe[,.variable])){
      v <- .variable
      strng[[.variable]] <- paste(v,' = "number"',sep='')
  #    strng[[.variable]] <- '"number"'
      
    } else if(
      !all(is.na(as.Date(as.character(na.omit(.dataframe[,.variable])), origin = "1970-01-01")))
      ){
      v <- .variable    
      strng[[.variable]] <- paste(v,' = "YYYY-MM-DD"',sep='')
  #    strng[[.variable]] <- '"YYYY-MM-DD"'
  
    } else if (all(is.na(.dataframe[ ,.variable]))){
      v <- .variable        
      strng[[.variable]] <- paste(v,' = "', names(.dataframe)[i], '"', sep='')
    }
  }  
  #strng
  strng2 <- ""
  for(n in 1:(length(strng)-1)){
    strng2 <- paste(strng2, strng[[n]], ",\n")
  }
  strng2 <- paste(strng2, strng[[length(strng)]], "\n")
  #cat(strng2)
  strng3 <- paste("
  unit_metadata =
    list(",strng2,")", sep = "")
  #cat(strng3)
  eval(parse(text = strng3))
  #unit_metadata
  return(unit_metadata)
  }
  
  #u1 <- get_vals(analyte)
  #u1
      
#+end_src
*** COMMENT reml_boilerplate3-code
#+name:get_vals
#+begin_src R :session *R* :tangle R/reml_boilerplate.r :exports none :eval no
  
  reml_boilerplate <- function(.dataframe, enumerated = NA){
  strng <- list()
  for(i in 1:ncol(.dataframe)){
  # i = 1
    .variable <- names(.dataframe)[i]
    #.dataframe[,.variable]
      if(is.character(.dataframe[ ,.variable])){
        .dataframe[,.variable]  <- factor(.dataframe[,.variable])
      }
  
    if(is.factor(.dataframe[,.variable])  & i %in% enumerated){
      x <- .dataframe[,.variable]
      vals <-  names(table(x))
      # symbols may pollute the string to parse
      vals <- make.names(vals)
      vals <- tolower(vals)  
      vals <- gsub("\\.","_",vals)
      vals <- gsub("_+","_",vals)    
      v <- .variable
      #v
      strng[[.variable]] <- paste(
      v, ' = c(',
      paste(vals, sep = '', collapse = ' = "TBA",')
      ,' = "TBA")', sep = '')
    } else if(is.factor(.dataframe[,.variable])){
      
      strng[[.variable]] <- paste(
        .variable, ' = "TBA"', sep = ''
        )
  
    } else if(is.numeric(.dataframe[,.variable])){
      v <- .variable
      strng[[.variable]] <- paste(v,' = "number"',sep='')
  #    strng[[.variable]] <- '"number"'
      
    } else if(
      !all(is.na(as.Date(as.character(na.omit(.dataframe[,.variable])), origin = "1970-01-01")))
      ){
      v <- .variable    
      strng[[.variable]] <- paste(v,' = "YYYY-MM-DD"',sep='')
  #    strng[[.variable]] <- '"YYYY-MM-DD"'
  
    } else if (all(is.na(.dataframe[ ,.variable]))){
      v <- .variable        
      strng[[.variable]] <- paste(v,' = "', names(.dataframe)[i], '"', sep='')
    }
  }  
  #strng
  strng2 <- ""
  for(n in 1:(length(strng)-1)){
    strng2 <- paste(strng2, strng[[n]], ",\n")
  }
  strng2 <- paste(strng2, strng[[length(strng)]], "\n")
  #cat(strng2)
  strng3 <- paste("
  unit_metadata =
    list(",strng2,")", sep = "")
  #cat(strng3)
  eval(parse(text = strng3))
  #unit_metadata
  return(unit_metadata)
  }
  
  #u1 <- get_vals(analyte)
  #u1
      
#+end_src


*** reml_boilerplate-test-code
#+name:reml_boilerplate-test
#+begin_src R :session *R* :tangle no :exports reports :eval no
  ################################################################
  # name:reml_boilerplate-test
  library(EML)
  require(devtools)
  load_all()
  #require(disentangle)
  fpath <- system.file("extdata/civst_gend_sector_additional_variables.csv", package = "disentangle")
  analyte <- read.csv(fpath, colClasses=c(ordinal_variable = "character"))
  analyte <- read.csv(fpath, stringsAsFactor = F)
  
  str(analyte)
  analyte$datevar <- as.Date(analyte$datevar)
  analyte$ordinal_variable <- as.character(analyte$ordinal_variable)
  analyte$fractions <- sample(rnorm(1000,0.1,0.1), nrow(analyte))
  
  str(analyte)
  head(analyte)
  unit_defs <- reml_boilerplate(
    data_set = analyte,
    created_by = "Ivan Hanigan <ivanhanigan@gmail.com>",
    titl = "civst_gend_sector_full",
    outfile = 'testing1234'
    )
  unit_defs
  analyte2 <- analyte
  names(analyte)
  #analyte <- analyte2[,-c(6)]
  unit_defs <- reml_boilerplate2(analyte)
  unit_defs
  
  unit_defs <- reml_boilerplate3(analyte, enumerated = c(1,2,3,8))
  unit_defs
  
  
  
  
    # we can get the col names easily
    col_defs <- names(analyte)
    # then create a dataset with metadata
    ds <- data.set(analyte,
                   col.defs = col_defs,
                   unit.defs = unit_defs
                   )
    # now write EML metadata file
    eml_config(creator="TBA <fakeaddress@gmail.com>")
  outfile <- "testingAbcd12234.csv"
  str(ds)
    ## eml_write(ds,
    ##           file = gsub(".csv", "_eml_skeleton.xml", outfile),
    ##           title = gsub(".csv", "", outfile)
    ##           )
  ## [1] "testingAbcd12234_eml_skeleton.xml"
  ## Warning message:
  ## In `[[<-.data.frame`(`*tmp*`, i, value = c("2013-09-09/01/13", "2013-09-09/01/13",  :
  ##   Setting class(x) to NULL;   result will no longer be an S4 object
  ## >
  eml_write(analyte,
            col.defs = col_defs,
            unit.defs = unit_defs,
            creator="TBA <fakeaddress@gmail.com>",
            file = gsub(".csv", "_eml_skeleton.xml", outfile)
            )
  
  tempfile <- dir(pattern="^data_table_")
  tempfile
  file.rename(tempfile, outfile)
  # rename the CSV file.
  
  # dir("data")
#+end_src

*** COMMENT reml_boilerplate new test-code
#+name:reml_boilerplate new test
#+begin_src R :session *R* :tangle no :exports none :eval no
#### name:reml_boilerplate new test####
install.packages("dlnm")
library(dlnm)
library(disentangle)
library(EML)

data(chicagoNMMAPS)
str(chicagoNMMAPS)
?chicagoNMMAPS
data_dictionary(chicagoNMMAPS)
unit_defs <- reml_boilerplate(chicagoNMMAPS)
col_defs <- names(chicagoNMMAPS)
ds <- eml_dataTable(chicagoNMMAPS,
              col.defs = col_defs,
              unit.defs = unit_defs,
              description = "Metadata documentation for chic.csv", 
              filename = "chic.csv")
# now write EML metadata file
eml_config(creator="Antonio Gasparrini
<antonio.gasparrini@lshtm.ac.uk>")
eml_write(ds,
          file = "chic.xml",
          title = "chicagoNMMAPS"
)

# now your metadata has been created

#+end_src

** R-data_dictionary
*** R-R-data_dict
#+name:R-data_dictionary
#+begin_src R :session *R* :tangle R/data_dict.r :exports none :eval no
  # name:data_dict
  data_dict <- function(.dataframe, .variable, .show_levels = -1)
  {
  
  summary2 <- function(x){
    summa <- summary(x, digits = nchar(max(x))+3)
    return(summa)
  }
  
    if(is.character(.dataframe[ ,.variable])){
      .dataframe[,.variable]  <- factor(.dataframe[,.variable])
    }
    if(all(is.na(.dataframe[ ,.variable]))){
      summa <- summary2(.dataframe[,.variable])
      summa <- as.data.frame(t(summa[2]))
      summa[,1]  <- as.numeric(as.character(summa[,1]))
    } else {
      summa <- summary2(.dataframe[,.variable])
    }
  # if there are some missing obs in a date var you get a malformed
  # summa with less names than levels
    if(length(as.character(summa)) != length(names(summa))){
      summa <- as.character(summa)
    }
  
    
    summa <- as.data.frame(
      cbind(
        c(.variable, rep("", length(summa) - 1)),
        names(summa)
        ,
        as.vector(summa)
        )
      )
    summa[,1]  <- as.character(summa[,1])
    summa[,2]  <- as.character(summa[,2])
    # summa
  
    # if char (factor)
    if(is.factor(.dataframe[,.variable])){
    summa$type <- c("character", rep("", nrow(summa) - 1))
    summa$summa  <- as.numeric(as.character(summa$V3))  
    summa$summa2 <- rep(NA, nrow(summa))
        # as.numeric(as.character(summa$V2)) ?
    summa$pct  <- round((summa$summa / sum(summa$summa)) * 100, 2)
    summa <- summa[,c(1,4,2,6,5,7)]
    if(.show_levels > 0){
      if(nrow(summa) > .show_levels){
        summa <- summa[1:.show_levels,]  
        summa <- rbind(summa, c("", "",
                                sprintf("more than %s levels. list truncated.", .show_levels),
                                "","", "")
                       )
        }
      }
    # summa
    } else if (
      is.numeric(.dataframe[,.variable])
      ){
    summa$type <- c("number", rep("", nrow(summa) - 1))
    summa$cnt <- NA
    summa$pct  <- NA
    summa <- summa[,c(1,4,2,3,5,6)]
    } else if (
     !all(
        is.na(as.Date(as.character(na.omit(.dataframe[,.variable])), origin = "1970-01-01"))
        )
      ){
    # if date
    ## datevar <- as.Date(as.character(
    ##   .dataframe[,.variable]
    ##   ), origin = "1970-01-01")
    # http://stackoverflow.com/questions/18178451/is-there-a-way-to-check-if-a-column-is-a-date-in-r
    # as.Date(as.character(.dataframe[,.variable]),format="%Y-%m-%d")
    ## if(!all(is.na(datevar))){
    ##   summa[,3] <- as.character(datevar)
    ## }
      
    summa$type <- c("date", rep("", nrow(summa) - 1))
    summa$cnt <- NA
    summa$pct  <- NA
    # summa
    if(
      length(which(is.na(.dataframe[,.variable]))) > 0
      ){
      summa$V3[-which(summa$V2 == "NA's")] <- as.character(
          as.Date(as.character(
          summa$V3[-which(summa$V2 == "NA's")]
          ), origin = "1970-01-01")
          )    
    } else {
      summa$V3 <- as.character(as.Date(as.numeric(as.character(summa$V3)), origin = "1970-01-01"))    
    }
    summa <- summa[,c(1,4,2,3,5,6)]
    } else if (all(is.na(.dataframe[ ,.variable]))){
      
    summa$type <- c("missing", rep("", nrow(summa) - 1))
    summa$fill  <- NA
    summa$pct  <- 100
    summa <- summa[,c(1,4,2,5,3,6)]
    
    } else {
        stop(sprintf("variable '%s' type is not character, factor, date, numeric or missing", .variable))
    }
    names(summa)  <- c("Variable","Type","Attributes", "Value", "Count", "Percent")
    # summa
    return(summa)
  }
  
#+end_src

#+RESULTS: R-data_dictionary

*** data_dictionary-code
#+name:data_dictionary
#+begin_src R :session *R* :tangle R/data_dictionary.r :exports none :eval no
  ################################################################
  data_dictionary <- function(dataframe, show_levels = -1){
    out <- matrix(NA, nrow = 0, ncol = 3)
    for(i in 1:ncol(dataframe)){
    #  i = 1
    #  print(i)
    out2 <- data_dict(
      .dataframe = dataframe
      ,
      .variable = names(dataframe)[i]
      ,
      .show_levels = show_levels
      )
    out <- rbind(out, out2)
    }
    row.names(out) <- NULL
    return(out)
  }
#+end_src

*** test-R-data_dictionary
#+name:R-data_dictionary
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:R-data_dictionary
  require(devtools)
  install_github("ivanhanigan/disentangle")
  # or
  load_all()
  require(disentangle)
  fpath <- system.file("extdata/civst_gend_sector.csv", package = "disentangle")
  fpath
  civst_gend_sector <- read.csv(fpath)
  civst_gend_sector$datevar <- as.Date(round(rnorm(nrow(civst_gend_sector), Sys.Date(),10)), origin = "1970-01-01")
  civst_gend_sector$missing_variable  <- NA
  civst_gend_sector$Survey.year  <- as.integer(rep(2012, nrow(civst_gend_sector)))
  str(civst_gend_sector)
  #data_dict
  data_dict(civst_gend_sector, "civil_status")
  class(civst_gend_sector$datevar)
  data_dict(civst_gend_sector, "datevar")
  data_dict(civst_gend_sector, "number_of_cases")
  data_dict(civst_gend_sector, "missing_variable")
  data_dict(civst_gend_sector, "Survey.year")
  dataDictionary <- data_dictionary(civst_gend_sector,
                                    show_levels = -1)
  
  dataDictionary
  #write.csv(dataDictionary, "~/dataDictionary.csv", row.names = F)
  
  # with randoms in date variable
  civst_gend_sector$datevar[sample(1:nrow(civst_gend_sector), 5)] <- NA
  # and civil status
  civst_gend_sector$civil_status[sample(1:nrow(civst_gend_sector), 7)] <- NA
  # and number of cases
  civst_gend_sector$number_of_cases[sample(1:nrow(civst_gend_sector), 2)] <- NA
  civst_gend_sector$Survey.year[sample(1:nrow(civst_gend_sector), 4)] <- NA
  # some integer codes
  civst_gend_sector$ordinal_variable <-  as.character(round(sample(rnorm(10000,2.5,1), nrow(civst_gend_sector))))
  str(civst_gend_sector)
  #debug(data_dict)
  data_dict(
            .dataframe=civst_gend_sector
            ,
            .variable="datevar"
            ,
            .show_levels = -1
            )
  #undebug(data_dict)
  
  dataDictionary <- data_dictionary(civst_gend_sector,
                                    show_levels = -1)
  
  dataDictionary
  write.csv(civst_gend_sector,
            file.path("inst/extdata", gsub(".csv","_additional_variables.csv", basename(fpath)))
            , row.names = F)
#+end_src
*** man-R-data_dictionary
#+name:R-data_dictionary
#+begin_src R :session *R* :tangle no :exports none :eval no
################################################################
# name:R-data_dictionary

#+end_src


** variable_names_and_labels
*** R-variable_names_and_labels
#+name:variable_names_and_labels
#+begin_src R :session *R* :tangle R/variable_names_and_labels.r :exports none :eval no
  #' @title variable_names_and_labels
  #' @name variable names and labels
  #' @param infile the full pathname
  #' @param datadict a dd object (optional will be created if needed)
  #' @param insert_labels T/F if the output should summarise the value labels
  
  variable_names_and_labels <- function(infile, datadict = NULL, insert_labels = FALSE){
  # NB dont allow mismatch between orig and dd
  if(!is.null(datadict)){  
  variable_names <- read.csv(infile, nrows = 1, header = F, stringsAsFactors = F)
  } else {
  dat <- read.csv(infile, stringsAsFactors =  F)
  variable_names  <- names(dat)
  datadict <- data_dictionary(dat)
  }
  # variable_names
  
  #### now get variable names as they appear in the dd ####
  datadict$ordering <- 1:nrow(datadict)
  col_defs <- sqldf::sqldf("
  select Variable, Type
  from datadict
  group by Variable, Type
  order by ordering
  ", drv = "SQLite")
  col_defs <- col_defs[col_defs$Variable != "",]
  col_defs
  
  vl <- as.data.frame(cbind(col_defs,t(variable_names[1,])))
  names(vl) <- c("variable_name", "simple_type", "original_name")
  vl$description <- ""
  vl$nominal_ordinal_interval_ratio_date_time <- ""
  vl$unit_of_measurement <- ""
  vl$value_labels <- ""
  vl$issue_description_and_suggested_change <- ""
  vl$depositor_response <- ""
  # it is easy in a spreadsheet to add the value labels but an
  # automation approach is here
  if(insert_labels){
    if(!exists('dat'))   dat <- read.csv(infile, stringsAsFactors =  F)
    lablist  <- reml_boilerplate(dat, enumerated = 1:3)
    lablist <- lapply(lablist, names)
    lablist <- lapply(lablist, paste, sep = "", collapse = " = ?; ")
    lablist  <- do.call(rbind.data.frame, lablist)
    vl$value_labels <- lablist[,1]
  }
    
  #vl
  return(vl)
  }
#+end_src
*** test-variable_names_and_labels
#+name:variable_names_and_labels
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:variable_names_and_labels
  require(devtools)
  #install_github("ivanhanigan/disentangle")
  require(disentangle)
  # or
  load_all()
  
  
  fpath <- system.file("extdata/civst_gend_sector_additional_variables.csv", package = "disentangle")
  fpath
  civst_gend_sector <- read.csv(fpath)
  civst_gend_sector$datevar <- as.Date(civst_gend_sector$datevar)
  # changing names is something that we want to track
  names(civst_gend_sector) <- gsub("_", " ", names(civst_gend_sector))
  names(civst_gend_sector)  <- sapply(names(civst_gend_sector), upcase_string)
  str(civst_gend_sector)
  civst_dd <- data_dictionary(civst_gend_sector)
  civst_dd
  fpath
  vl <- variable_names_and_labels(
    infile = fpath
    ,
    datadict = civst_dd
    ,
    insert_labels = T
    )
  vl
#+end_src


* Data Operation
** R-spatial
*** TODO xy2shp
#+name:xy2shp
#+begin_src R :session *R* :tangle no :exports none :eval no
  # func
  
  if(!require(ggmap)) install.packages('ggmap'); require(ggmap)
  if (!require(rgdal)) install.packages('rgdal'); require(rgdal)
  epsg <- make_EPSG()
  # load
  latlong <- read.table(tc <- textConnection(
  "ID  POINT_Y   POINT_X
  1  150.5556 -35.09305
  2  150.6851 -35.01535
  3  150.6710 -35.06412
  4  150.6534 -35.08666
  "), header = TRUE); close(tc)
  # do
  for(i in 1:nrow(latlong)){
    coords <- as.numeric(latlong[i,c('POINT_Y', 'POINT_X')])
    e <- as.data.frame(cbind(i, t(coords), revgeocode(coords)))
    write.table(e, "test.csv", sep = ',', append = i > 1, col.names = i == 1, row.names = F)
  }
  d <- read.csv('test.csv')
  head(d)
  ## Treat data frame as spatial points
  pts <- SpatialPointsDataFrame(cbind(d$V2,d$V3),d,
    proj4string=CRS(epsg$prj4[epsg$code %in% '4283']))
  writeOGR(pts, 'test.shp', 'test', driver='ESRI Shapefile')
  
#+end_src

*** COMMENT sptransform-code
#+name:sptransform
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:sptransform####
  require(disentangle)
  require(rgdal)
  epsg <- make_EPSG()
  # load
  d <- read.table(tc <- textConnection(
  "ID  POINT_Y   POINT_X
  1  150.5556 -35.09305
  2  150.6851 -35.01535
  3  150.6710 -35.06412
  4  150.6534 -35.08666
  "), header = TRUE); close(tc)
  # do
  str(epsg)
  epsg[grep("UTM", epsg$note),"note"]
  ## Treat data frame as spatial points
  pts <- SpatialPointsDataFrame(cbind(d$V2,d$V3),d,
  proj4string=CRS(epsg$prj4[epsg$code %in% '4283']))
    # this alternative was from a website  #CRS("+init=epsg:4283"))
  # defin inputs
  datadir <- "data_provided/pr_bg_data_20140612/Data_tables/site_data/"
  flist<-dir(datadir)
  #flist
  #change this
  fname <- "Site_data.txt"
  
  # define outputs
  outdir <- "ljbb_plot_details"
  outfile <- "ljbb_plot_details.shp"
  
  # load
  dat <- read.csv(
    file.path(datadir, fname)
    , header=TRUE)
  
  # clean
  dat$Date.of.last.fire <-  as.Date(dat$Date.of.last.fire, "%d/%m/%Y")
  #head(dat)
  names(dat) <- tolower(gsub("\\.", "_", names(dat)))
  # to match up with the site_number from the spotlighting data
  dat$site_number <- paste("site_", dat$site_number, sep = "")
  
  # TODO check coordinate system code at spatialreference.org
  # EPSG:20256: AGD66 / AMG zone 56
  srid <- CRS(epsg$prj4[epsg$code %in% '20256'])
  #srid
  pts <- SpatialPointsDataFrame(cbind(dat$easting,dat$northing),dat,
    proj4string=srid)
  setwd(outdir)
  writeOGR(pts, dsn = outfile, layer = gsub(".shp","", outfile), driver = "ESRI Shapefile")
  setwd("..")
  
  #### and the csv
  write.csv(pts@data, file.path(outdir, gsub(".shp", ".csv", outfile)), row.names = F)
  
  #### Bounding Box in lat/long ####
  # work out the lat longs
  latlong  <-  spTransform(pts, CRS("+init=epsg:4283"))
  #summary(latlong@data[,c("easting", "northing")])
  #str(latlong)
  
  bb <- morpho_bounding_box(latlong)
  print(xtable(bb), type = "html")
    
#+end_src


*** TODO gIntersection
#+name:gIntersection
#+begin_src R :session *shell* :tangle no :exports none :eval no
  #### name:gIntersection ####
  #?gIntersection
  subset(ma_grid10@data, Id == 125)
  str(mash_ch@data)
  mash_ch2 <- gUnaryUnion(mash_ch)
  str(mash_ch2@data)
  summary(mash_ch2)
  int <- gIntersection(ma_grid10, mash_ch2, byid = T, id = as.character(ma_grid10@data$Id))
  plot(int)
  summary(int)
  str(int)
  n<-names(int)
  ## n  <- as.character(n)
  ## n
  ## n<-data.frame(t(data.frame(
  ##   strsplit(n," ",fixed=TRUE)
  ##   )))
  n <- as.data.frame(n)
  head(n)
  tail(n)
  row.names(n) <- NULL
  colnames(n) <- "Id"
  #[1:2]<-c("Id","chull")
  
  
  n$area<-sapply(int@polygons, function(x) x@area)
  summary(n)
  head(n)
  str(n)
  subset(n, Id == 125)
  #a<- sapply(slot(ma_grid10, "polygons"), function(x) sapply(slot(x, "Polygons"), slot, "area"))
  #str(a)
  
  #df<-merge(n,a,all.x=TRUE)
  #head(df)
  #df$share.area<-df$area/df$total.area*100
  #subset(df, grid == 100)
  #table(df$grid)
  ## str(ma_grid10@data)
  ## summary(ma_grid10@data)
  ## df <- n
  ## df$Id <- as.numeric(as.character(df$Id))
  ## summary(df)
  #ma_grid10 <- ma_grid10
  #by <- "Id"
  # http://stackoverflow.com/questions/3650636/how-to-attach-a-simple-data-frame-to-a-spatialpolygondataframe-in-r
  ma_grid10@data = data.frame(ma_grid10@data, n[match(ma_grid10@data[,"Id"], n[,"Id"]),])
  head(ma_grid10@data)
  tail(ma_grid10@data)
  summary(ma_grid10)
  plot(ma_grid10, col = ma_grid10$area)
  #plot(chull, add = T)
  #setwd("data")
  writeOGR(ma_grid10, "temp20.shp", "temp20", driver = "ESRI Shapefile")
  crs <- ma_grid10@proj4string
  spp <- SpatialPolygonsDataFrame(int,data=as.data.frame(n[,1]),match.ID=F)
  writeOGR(spp, "temp3.shp", "temp3", driver = "ESRI Shapefile")
  
  setwd("..")
  
#+end_src

*** COMMENT shapefile_attributes_join-code
#+name:shapefile_attributes_join
#+begin_src R :session *shell* :tangle no :exports none :eval no
#### name:shapefile_attributes_join ####
see ramnath

sp@data = data.frame(sp@data, df[match(sp@data[,by], df[,by]),])

#+end_src
*** COMMENT shapefile_colour_ramp-code
#+name:shapefile_colour_ramp
#+begin_src R :session *shell* :tangle no :exports none :eval no
#### name:shapefile_colour_ramp ####
  # If we wanted to do a colour ramp 
  ## cls <- -.9 +  (occupancy_grids@data$area.1/(10000*10000))
  ## cls <- abs(ifelse(is.na(cls), -1, cls))
  ## cls <- gray(cls)
  # but we don't, this time just in or out
  cls<-occupancy_grids@data$area/(10000*10000)
  cls
  cls <- ifelse(cls >= 0.01, "lightgrey", "darkgrey")
  cls <- ifelse(is.na(cls), 'white', cls)
  cls
  png("results/B2_occupancy.png")
  plot(occupancy_grids, col = cls)
  plot(chull, add = T)
  plot(ecosystem_current, add = T, border = T, col = 'darkgrey')
  dev.off()

OR
http://www.geog.uoregon.edu/GeogR/examples/maps_examples01.htm
library(RColorBrewer) # creates nice color schemes
library(classInt)     # finds class intervals for continuous variables

 # equal-frequency class intervals
plotvar <- orcounty.shp@data$POP1990
nclr <- 8
plotclr <- brewer.pal(nclr,"BuPu")
class <- classIntervals(plotvar, nclr, style="quantile")
colcode <- findColours(class, plotclr)

plot(orcounty.shp, xlim=c(-124.5, -115), ylim=c(42,47))
plot(orcounty.shp, col=colcode, add=T)
title(main="Population 1990",
    sub="Quantile (Equal-Frequency) Class Intervals")
legend(-117, 44, legend=names(attr(colcode, "table")),
    fill=attr(colcode, "palette"), cex=0.6, bty="n")

#+end_src

** COMMENT aggregate-list-of-dfs
#+name:aggregate
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:aggregate####
  # stoopid hack for lu
  # desc
  "I have a do.call question that perhaps you can answer in 5 seconds... [my brain is leaking... I can't see the solution].
  
  I have a list of data.frames imagine I wanted to sum them up, like d1+d2+... N. If I do that manually the output is a data.frame, but when I use do.call I get one number, and I need a data.frame instead.
  
  Another issues is that some columns have a factor, so I'm planning to wrap my operation into a function with a condition to do the sum if the cell contains a number. Other to skip it.
  so I though of:
  
  foo = function (x) { ifelse(is.numeric(x), sum(x), NA)} #but it doesn't work
  
  This example addressed the issues.
  
  d1 = mtcars
  d2 = d1*2
  
  str(mtcars)
  l1 = list(d1,d2)
  str(l1)
  
  d3 = do.call('sum', l1)
  #> d3
  #[1] 41826.61 # I don't want one number, but a data.frame with the sums.
  
  foo = function (x) { ifelse(is.numeric(x), sum(x), NA)}
  d4 = do.call('foo', l1)
  
  Any help or tip will be welcome!"
  
  # I feel like it should be simple because R can add dataframes
  # but maybe these need to be matrices (because a data.frame is really
  # a list)
  d1 = as.matrix(mtcars)
  d1
  d2 = as.matrix(d1*2)
  d2
  d1 + d2
  # lets make more dfs
  d3 = as.matrix(d2*3)
  d4 = as.matrix(d3*2)
  d1+d2+d3+d4
  # I note that do.call can add two df fine
  l1  <- list(d1, d2)
  out  <- do.call("+", l1)
  str(out)
  out
  # but with multiple no good
  l1 = list(d1,d2, d3, d4)
  str(l1)
  out  <- do.call("+", l1)
  # when these are dataframes the error is
  ## Error in `+`(list(mpg = c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4,  :
  ##   unused arguments (list(mpg = c(... blah blah
  # when matrices the error is
  ## Error in `+`(c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2,  :
  ##   operator needs one or two arguments
  
  
  # But the lists content are able to be combined if done individually
  out1 <- l1[[1]] + l1[[2]] + l1[[3]] + l1[[4]]
  str(out1)
  out1
  
  # make things interesting
  l1 <- list()
  for(i in 1:1000){
    #i = 1
    di  <- iris
    di[sapply(iris, is.numeric)]  <- iris[sapply(iris, is.numeric)] * sample(rnorm(100,1,2), 1)
    l1[[i]]  <- di
  }
  str(l1)
  # so we could figure out a way to iterate over the dataframes with a
  # loop like
  ## for(i in 1:length(l1)){
  ##   if(i == 1){
  ##     out2 <- l1[[i]]
  ##     next
  ##   } else {
  ##     out2  <- out2 + l1[[i]]
  ##   }
  ## }
  # doesn't like factor
  #out2
  # or 
  ## d1 <- iris
  ## str(d1) # Note pesky factor column
  ## d2 <- d1
  ## # Only multiply cells that are numeric
  ## str(d2[sapply(d2, is.numeric)])
  ## d2[sapply(d2, is.numeric)] <- d2[sapply(d2, is.numeric)] * 2
  
  ## # Add another data.frame to make it interesting
  ## d3 <- d1
  ## d3[sapply(d3, is.numeric)] <- d3[sapply(d3, is.numeric)] * 3
  
  
  ## l1 = list(d1,d2, d3)
  str(l1)
  
  # Inititalise output dataframe to zero
  
  # Sum data frames
  summarise_list_dfs3 <- function(listed){
    d.results <- d1
    d.results[sapply(d.results, is.numeric)] <- 0 
    for (i in seq_along(l1)){
      d.results[sapply(d.results, is.numeric)] <-
        d.results[sapply(d.results, is.numeric)] +
        l1[[i]][sapply(l1[[i]], is.numeric)]
    }
    return(d.results)
  }
  system.time(d.results  <- summarise_list_dfs3(l1))
  str(d.results)
  
  # but we want an elegant solution that will be
  # able to give any number of dataframes, and also have
  # the issue of some variables being factor so not to be used
  
  # let's write some func
  # I like SQL for it's clarity
  library(sqldf)
  # we are basically grouping the values of each row in each df, so add
  # an id
  
  nam <- function(x){
    x$row_names <- 1:nrow(x)
    return(x)
  }
  l_df2 <- lapply(l1, nam)
  str(l_df2)
  # now construct some sql and run it.  let's make it flexible for
  # different summarising functions like sum, mean, median, stdev etc
  summarise_list_of_dfs <- function(
    list_of_dfs = l1
    ,
    summarise_fun = 'sum'
    ,
    id = 'row_names'
    ){
    if(!is.data.frame(list_of_dfs[[1]])) list_of_dfs <- lapply(list_of_dfs, as.data.frame)
    l_df2 <- lapply(list_of_dfs, nam)
    x = do.call('rbind.data.frame', l_df2)
  names(x)<-gsub("\\.", "_", names(x))
    todo <- sapply(x, 'is.numeric')
    todo <- names(x)[todo]
    todo <- todo[-which(todo == "row_names")]
    oper  <- sprintf('), %s(',summarise_fun)
    sql<-paste("sum(",paste(todo, sep = '', collapse = oper),")", sep = "")
  #sql
    out<-sqldf(
    #cat(
    sprintf("select row_names, %s
    from x
    group by row_names
    order by row_names
    ", sql)
    )
    names(x)<-gsub("_", "\\.", names(x))
    return(out)
  }
  
  # do
  #l1 <- lapply(l1, as.data.frame)
  
  system.time(qc  <- summarise_list_of_dfs(l1))
  
  str(qc)
  str(out1)
  out1  <- as.data.frame(out1)
  names(qc)<-names(out1)
  # same?
  identical(qc, out1)
  # not identical
  all(qc == out1)
  # all values are equal tho
  
  # compare to base r
  summarise_list_dfs2 <- function(listed){
    listed  <- l1
    d5=do.call('rbind',listed)
  #  str(d5)
    d5.rnames =rep(rownames(listed[[1]]),times=length(listed))
    d5$reg=d5.rnames
    d6 = aggregate(d5[sapply(d5, is.numeric)], by=list(d5$reg), FUN=sum) # I should use an
                                            # ifelse, to do this only to
                                            # numeric columns.
    return(d6)
  }
  system.time(d6 <- summarise_list_dfs2(l1))
  
  str(d6)
  row.names(d6) <- d6$Group.1
  # same?
  identical(d6, out1[sort(row.names(out1)),])
  # not identical
  all(
    d6[,-c(1)] == out1[sort(row.names(out1)),]
    )
  # looks like the integers are equal but not the doubles.
  # but a diff of the two looks same
  
  # now does it work for  mean?
  qc  <- summarise_list_of_dfs(l1, 'mean')
  # whoops, sql uses 'avg'
  qc  <- summarise_list_of_dfs(l1, 'avg')
  str(qc)
  # and stdev
  qc  <- summarise_list_of_dfs(l1, 'stdev')
  str(qc)
  # this did multiple dataframes, now try with factor variables
  str(l1)
  d1[,3] <- as.factor(d1[,3])
  d2[,3] <- as.factor(d2[,3])
  d3[,3] <- as.factor(d3[,3])
  d4[,3] <- as.factor(d4[,3])
  
  l1 = list(d1,d2, d3, d4)
  str(l1)
  qc  <- summarise_list_of_dfs(l1)
  str(qc)
  # it has just skipped that variable so this should be good to go
  # NB if the combination of all the dataframes is too big for RAM then
  # R will fail. in this case I would
  # put into a PostgreSQL database as this will use the disk rather than
  # RAM
  # this can also be used to add indexes and clustering functions to
  # speed up the calculations.
  # HTH, let me know if there is a more efficient R solution?
  
  install.packages("data.table")
  library(data.table)
  sum_ldf4 <- function(
    listed=l1
    ,
    summarise_fun  = 'sum'
                       ){
    d5=do.call('rbind',listed)
    d5.rnames =rep(rownames(listed[[1]]),times=length(listed))
    d5 <- d5[sapply(d5, is.numeric)]
    d5$reg=d5.rnames
    oper  <- sprintf('), %s(',summarise_fun)
    todo <- sapply(d5, 'is.numeric')
    todo <- names(d5)[todo] 
    sql<-paste("sum(",paste(todo, sep = '', collapse = oper),")", sep = "")
    #sql
      
    DT = data.table(d5)
    
    out <-  eval(parse(
        text = sprintf("DT[,list(%s),by=list(d5$reg)]", sql)
        ))
      
  
    return(out)
  }
  
  
  # sqldf
  o1 <- system.time(qc  <- summarise_list_of_dfs(l1))
  str(qc)
  # aggregate
  o2 <- system.time(d6 <- summarise_list_dfs2(l1))
  str(d6)
  # loop
  o3 <- system.time(d.results  <- summarise_list_dfs3(l1))
  str(d.results)
  # data.table
  o4 <- system.time(datab  <- sum_ldf4(l1))
  str(datab)
  
  # http://zvfak.blogspot.com.au/2011/03/applying-functions-on-groups-sqldf-plyr.html
  library(gplots)
  x<-c(o1[3],o2[3],o3[3],o4[3])
  balloonplot( rep("time.elapsed",5),c("sqldf","aggregate","loop", "data.table"),round(x,1), ylab ="Method", xlab="",sorted=F,dotcolor=rev(heat.colors(5)),main="time.elapsed for different methods") 
#+end_src

** COMMENT aggregate-list-of-dfs-asRNW-code
#+name:aggregate-list-of-dfs-asRNW
#+begin_src R :session *R* :tangle no :exports none :eval no
#### name:aggregate-list-of-dfs-asRNW####
---
title: "aggregate list of dfs"
author: "Ivan C Hanigan"
date: "7/2/2015"
output: html_document
---

```{r}
#### name:aggregate####
# stoopid hack for lu
# desc
"I have a do.call question that perhaps you can answer in 5 seconds... [my brain is leaking... I can't see the solution].

I have a list of data.frames imagine I wanted to sum them up, like d1+d2+... N. If I do that manually the output is a data.frame, but when I use do.call I get one number, and I need a data.frame instead.

Another issues is that some columns have a factor, so I'm planning to wrap my operation into a function with a condition to do the sum if the cell contains a number. Other to skip it.
so I though of:

foo = function (x) { ifelse(is.numeric(x), sum(x), NA)} #but it doesn't work

This example addressed the issues.

d1 = mtcars
d2 = d1*2

str(mtcars)
l1 = list(d1,d2)
str(l1)

d3 = do.call('sum', l1)
#> d3
#[1] 41826.61 # I don't want one number, but a data.frame with the sums.

foo = function (x) { ifelse(is.numeric(x), sum(x), NA)}
d4 = do.call('foo', l1)

Any help or tip will be welcome!"

# I feel like it should be simple because R can add dataframes
# but maybe these need to be matrices (because a data.frame is really
# a list)
d1 = as.matrix(mtcars)
d1
d2 = as.matrix(d1*2)
d2
d1 + d2
# lets make more dfs
d3 = as.matrix(d2*3)
d4 = as.matrix(d3*2)
d1+d2+d3+d4
# I note that do.call can add two df fine
l1  <- list(d1, d2)
out  <- do.call("+", l1)
str(out)
out
# but with multiple no good
l1 = list(d1,d2, d3, d4)
str(l1)
#out  <- do.call("+", l1)
# when these are dataframes the error is
## Error in `+`(list(mpg = c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4,  :
##   unused arguments (list(mpg = c(... blah blah
# when matrices the error is
## Error in `+`(c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2,  :
##   operator needs one or two arguments


# But the lists content are able to be combined if done individually
out1 <- l1[[1]] + l1[[2]] + l1[[3]] + l1[[4]]
str(out1)
out1

# make things interesting
l1 <- list()
for(i in 1:1000){
  #i = 1
  di  <- iris
  di[sapply(iris, is.numeric)]  <- iris[sapply(iris, is.numeric)] * sample(rnorm(100,1,2), 1)
  l1[[i]]  <- di
}
str(l1)
# so we could figure out a way to iterate over the dataframes with a
# loop like
## for(i in 1:length(l1)){
##   if(i == 1){
##     out2 <- l1[[i]]
##     next
##   } else {
##     out2  <- out2 + l1[[i]]
##   }
## }
# doesn't like factor
#out2
# or 
## d1 <- iris
## str(d1) # Note pesky factor column
## d2 <- d1
## # Only multiply cells that are numeric
## str(d2[sapply(d2, is.numeric)])
## d2[sapply(d2, is.numeric)] <- d2[sapply(d2, is.numeric)] * 2

## # Add another data.frame to make it interesting
## d3 <- d1
## d3[sapply(d3, is.numeric)] <- d3[sapply(d3, is.numeric)] * 3


## l1 = list(d1,d2, d3)
str(l1)

# Inititalise output dataframe to zero

# Sum data frames
summarise_list_dfs3 <- function(
  listed=l1
  ){
  d.results <- listed[[1]]
  d.results[sapply(d.results, is.numeric)] <- 0 
  for (i in seq_along(listed)){
    d.results[sapply(d.results, is.numeric)] <-
      d.results[sapply(d.results, is.numeric)] +
      listed[[i]][sapply(listed[[i]], is.numeric)]
  }
  return(d.results)
}
system.time(d.results  <- summarise_list_dfs3(l1))
str(d.results)

# but we want an elegant solution that will be
# able to give any number of dataframes, and also have
# the issue of some variables being factor so not to be used

# let's write some func
# I like SQL for it's clarity
library(sqldf)
# we are basically grouping the values of each row in each df, so add
# an id

nam <- function(x){
  x$row_names <- 1:nrow(x)
  return(x)
}
l_df2 <- lapply(l1, nam)
str(l_df2)
# now construct some sql and run it.  let's make it flexible for
# different summarising functions like sum, mean, median, stdev etc
summarise_list_of_dfs <- function(
  list_of_dfs = l1
  ,
  summarise_fun = 'sum'
  ,
  id = 'row_names'
  ){
  if(!is.data.frame(list_of_dfs[[1]])) list_of_dfs <- lapply(list_of_dfs, as.data.frame)
  l_df2 <- lapply(list_of_dfs, nam)
  x = do.call('rbind.data.frame', l_df2)
names(x)<-gsub("\\.", "_", names(x))
  todo <- sapply(x, 'is.numeric')
  todo <- names(x)[todo]
  todo <- todo[-which(todo == "row_names")]
  oper  <- sprintf('), %s(',summarise_fun)
  sql<-paste("sum(",paste(todo, sep = '', collapse = oper),")", sep = "")
#sql
  out<-sqldf(
  #cat(
  sprintf("select row_names, %s
  from x
  group by row_names
  order by row_names
  ", sql)
  )
  names(x)<-gsub("_", "\\.", names(x))
  return(out)
}

# do
#l1 <- lapply(l1, as.data.frame)

system.time(qc  <- summarise_list_of_dfs(l1))

str(qc)
str(out1)
out1  <- as.data.frame(out1)
#names(qc)<-names(out1)
# same?
#identical(qc, out1)
# not identical
#all(qc == out1)
# all values are equal tho

# compare to base r
summarise_list_dfs2 <- function(listed){
  #listed  <- l1
  d5=do.call('rbind',listed)
#  str(d5)
  d5.rnames =rep(rownames(listed[[1]]),times=length(listed))
  d5$reg=d5.rnames
  d6 = aggregate(d5[sapply(d5, is.numeric)], by=list(d5$reg), FUN=sum) # I should use an
                                          # ifelse, to do this only to
                                          # numeric columns.
  return(d6)
}
system.time(d6 <- summarise_list_dfs2(l1))

str(d6)
row.names(d6) <- d6$Group.1
# same?
#identical(d6, out1[sort(row.names(out1)),])
# not identical
#all(
#  d6[,-c(1)] == out1[sort(row.names(out1)),]
#  )
# looks like the integers are equal but not the doubles.
# but a diff of the two looks same

# now does it work for  mean?
#qc  <- summarise_list_of_dfs(l1, 'mean')
# whoops, sql uses 'avg'
qc  <- summarise_list_of_dfs(l1, 'avg')
str(qc)
# and stdev
qc  <- summarise_list_of_dfs(l1, 'stdev')
str(qc)
# this did multiple dataframes, now try with factor variables
str(l1)
d1[,3] <- as.factor(d1[,3])
d2[,3] <- as.factor(d2[,3])
d3[,3] <- as.factor(d3[,3])
d4[,3] <- as.factor(d4[,3])

l1 = list(d1,d2, d3, d4)
str(l1)
qc  <- summarise_list_of_dfs(l1)
str(qc)
# it has just skipped that variable so this should be good to go
# NB if the combination of all the dataframes is too big for RAM then
# R will fail. in this case I would
# put into a PostgreSQL database as this will use the disk rather than
# RAM
# this can also be used to add indexes and clustering functions to
# speed up the calculations.
# HTH, let me know if there is a more efficient R solution?

#install.packages("data.table")
library(data.table)
sum_ldf4 <- function(
  listed=l1
  ,
  summarise_fun  = 'sum'
                     ){
  d5=do.call('rbind',listed)
  d5.rnames =rep(rownames(listed[[1]]),times=length(listed))
  d5 <- d5[sapply(d5, is.numeric)]
  d5$reg=d5.rnames
  oper  <- sprintf('), %s(',summarise_fun)
  todo <- sapply(d5, 'is.numeric')
  todo <- names(d5)[todo] 
  sql<-paste("sum(",paste(todo, sep = '', collapse = oper),")", sep = "")
  #sql
    
  DT = data.table(d5)
  
  out <-  eval(parse(
      text = sprintf("DT[,list(%s),by=list(d5$reg)]", sql)
      ))
    

  return(out)
}


# sqldf
o1 <- system.time(qc  <- summarise_list_of_dfs(l1))
str(qc)
# aggregate
o2 <- system.time(d6 <- summarise_list_dfs2(l1))
str(d6)
# loop
o3 <- system.time(d.results  <- summarise_list_dfs3(l1))
str(d.results)
# data.table
o4 <- system.time(datab  <- sum_ldf4(l1))
str(datab)

# http://zvfak.blogspot.com.au/2011/03/applying-functions-on-groups-sqldf-plyr.html
library(gplots)
x<-c(o1[3],o2[3],o3[3],o4[3])
balloonplot( rep("time.elapsed",5),c("sqldf","aggregate","loop", "data.table"),round(x,1), ylab ="Method", xlab="",sorted=F,dotcolor=rev(heat.colors(5)),main="time.elapsed for different methods") 

sessionInfo()
```



#+end_src

** COMMENT text2columns-code
#+name:text2columns
#+begin_src R :session *shell* :tangle no :exports none :eval no
#### name:text2columns ####
setwd("/home/ivan_hanigan/Dropbox/projects/air_pollution_ucrh")
dat <- read.csv("air_pollution_ucrh_data_inventory_files_20150724.csv", stringsAsFactor = F)
str(dat)
qc <- dat[dat$v5 == "arcgis mxd", ]
qclist <- names(table(qc$entity_name))
qc2  <- strsplit(qclist, "_")
str(qc2)
qc2[[3]]
qc3 <- t(sapply(qc2, '[', 1:max(sapply(qc2, length)))) 
qc3 <- as.data.frame(qc3)
names(qc3) <-  paste("V", 1:13, sep = "")

#+end_src

** COMMENT climate-grids-thredds-code
#+name:climate-grids-thredds

#+begin_src R :session *R* :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-07-27-climate-grids-and-thredds-server-experimenting-update.md  :exports none :eval no :padline no
  ---
  name: climate-grids-and-thredds-server-experimenting-update
  layout: post
  title: Web Data, Climate Grids and THREDDS UPDATE
  date: 2015-07-27
  categories:
  - extreme weather events
  tags:
  - software
  ---
  
  # Update  
  - This is an update to my previous post:  [http://ivanhanigan.github.io/2014/12/climate-grids-and-thredds-server-experimenting/](http://ivanhanigan.github.io/2014/12/climate-grids-and-thredds-server-experimenting/)
  - I was worried about the available metadata with those netcdf data I was working with so contacted Dr Brad Evans at EMAST who set me straight:
  
  #### QUOTE:
      "you need to use ncdf4  - not ncdf  ...
      because they are netcdf4 files and not netcdf3 files. 
      This is poorly explained in the R community.
      All of the netcdf files from Australian providers (AusCover, BoM, etc...) 
      have been in netcdf4 for a couple of years now."
  
  # So here is the revised code using ncdf4 library:
  
  #### CODE
      # sudo apt-get install r-cran-ncdf4 
      library(ncdf4)
      library(raster)
      strt <-'2012-01-01'
      end <- '2012-01-04'
      dates <- seq(as.Date(strt),as.Date(end),1)          
      dates
      par(mfrow = c(2,2))
      for(i in 1:length(dates)){
       # i=1
        date_i <- dates[i]
        infile <- sprintf("http://dapds00.nci.org.au/thredds/dodsC/rr9/Climate/eMAST/ANUClimate/0_01deg/v1m0_aus/day/land/tmin/e_01/2012/eMAST_ANUClimate_day_tmin_v1m0_%s.nc", gsub("-", "", date_i))
       
        nc <- nc_open(infile)
        str(nc)
        print(nc)
        vals <- ncvar_get(nc, varid="air_temperature")
        str(vals)
        nc.att <-    nc$var$air_temperature
        xmin <- min(nc.att$dim[[1]]$vals)
        xmax <- max(nc.att$dim[[1]]$vals)
        ymin <- min(nc.att$dim[[2]]$vals)
        ymax <- max(nc.att$dim[[2]]$vals)
       
        print(c(xmin,xmax))
        print(c(ymin,ymax))
       
        r <- raster(t(vals),
                    xmn=xmin, xmx=xmax,
                    ymn=ymin, ymx=ymax)
        #str(r)
        plot(r)
        nc_close(nc)
      }
  
  
  <p></p>
  
  # RESULTS 1: METADATA
  
  - the result is I now have a lot more metdata returned to my R workspace
    
  #### EXERPT
      [...]
      [1] "        licence_copyright: Copyright 2009-2013 ANU. Rights owned by The Australian National University (ANU). Rights licensed subject to TERN  Attribution (TERN-BY)."
      [1] "        short_desc: Australian coverage, ANUClimate 1.0, 0.01 degree grid, 1970-2012"
      [1] "        summary: Minimum daily temperature, for the Australian continent between 1970-2012. Daily temperature regulates rates of plant growth and determines critical conditions such as frost on flowering and fruiting. Modelled by expressing each daily value as a difference anomaly with respect to the gridded 1976-2005 mean daily minimum temperature for each month as provided by eMAST_ANUClimate_mmn_tmin_v1m0_1976_2005. The daily anomalies were interpolated by trivariate thin plate smoothing spline functions of longitude, latitude and vertically exaggerated elevation using ANUSPLIN Version 4.5. There was an average of 671 Bureau of Meteorology data points available for each day between 1970 and 2012. Automated quality assessment rejected on average 3 data values per day with extreme studentised residuals. These were commonly associated with days following missing observations. The root mean square of all individual cross validation residuals provided by the spline analysis is 1.5 degrees Celsius. A comprehensive assessment of the analysis and the factors contributing to the quality of the final interpolated daily minimum temperature grids is in preparation."
      [1] "        long_name: Daily minimum temperature"
      [1] "        contact: Michael Hutchinson, Professor of spatial and temporal analysis, 3.23A, Fenner School of Environment & Society, College of Medicine, Biology & Environment, Frank Fenner Building 141, Australian National University, Canberra, Australian Capital Territory, 200, Australia, (+61) 2 6125 4783, Michael.Hutchinson@anu.edu.au, http://orcid.org/0000-0001-8205-6689"
      [1] "        references: 1. Hutchinson, M.F., Mckenney, D.W., Lawrence, K., Pedlar, J., Hopkinson, R., Milewska, E. and Papadopol, P. 2009. Development and testing of Canada-wide interpolated spatial models of daily minimum/maximum temperature and precipitation for 1961-2003. Journal of Applied Meteorology and Climatology 48: 725ï¿½741. http://dx.doi.org/10.1175/2008JAMC1979.1 2. Hutchinson, M.F. and Xu, T. 2013. ANUSPLIN version 4.4 User Guide. Fenner School of Environment and Society, Australian National University, Canberra http://fennerschool.anu.edu.au/files/anusplin44.pdf"
      [1] "        source: ANUClimate 1.0"
      [1] "        keywords: EARTH SCIENCE > ATMOSPHERE > ATMOSPHERIC TEMPERATURE > MAXIMUM/MINIMUM TEMPERATURE"
      [1] "        Conventions: CF-1.6"
      [1] "        institution: Australian National University"
      [1] "        geospatial_lat_min: -43.74"
      [1] "        geospatial_lat_max: -9"
      [1] "        geospatial_lat_units: degrees_north"
      [1] "        geospatial_lat_resolution: -0.01"
      [1] "        geospatial_lon_min: 112.9"
      [1] "        geospatial_lon_max: 154"
      [1] "        geospatial_lon_units: degrees_east"
      [1] "        geospatial_lon_resolution: 0.01"
      [1] "        keywords_vocabulary: Global Change Master Directory (http://gcmd.nasa.gov)"
      [1] "        metadata_link: http://datamgt.nci.org.au:8080/geonetwork"
      [1] "        standard_name_vocabulary: Climate and Forecast(CF) convention standard names (http://cf-pcmdi.llnl.gov/documents/cf-standard-names)"
      [1] "        id: eMAST_ANUClimate_day_tmin_v1m0_1970_2012"
      [1] "        DOI: To be added"
      [1] "        cdm_data_type: grid"
      [1] "        contributor_name: Michael Hutchinson, Jennnifer Kesteven, Tingbao Xu"
      [1] "        contributor_role: principalInvestigator, author, author"
      [1] "        creator_email: eMAST.data@mq.edu.au"
      [1] "        creator_name: eMAST data manager"
      [1] "        creator_url: http://www.emast.org.au/"
      [1] "        Metadata_Conventions: Unidata Dataset Discovery v1.0"
      [1] "        publisher_name: Ecosystem Modelling and Scaling Infrastructure (eMAST) Facility: Macquarie University"
      [1] "        publisher_email: eMAST.data@mq.edu.au"
      [1] "        publisher_url: http://www.emast.org.au/"
  
    
  # RESULTS 2: Grid data  
  
  - I still get lots of good data
  
  ![images/thredds2.png](images/thredds2.png)
  
  <p></p>
  
  # NOTE I still need that weird transpose
  
  - I note that the weird hacky transpose is still required 
  
  #### CODE
      # NB weird hacky transpose still required or else you get this
      r <- raster(vals,
                  xmn=xmin, xmx=xmax,
                  ymn=ymin, ymx=ymax)
       
      #str(r)
      plot(r)
  
  <p></p>
  
  ![images/thredds2raw.png](images/thredds2raw.png)
  
  
  
#+end_src

* Graphical User Interfaces
** web2py


*** TODO using the appadmin interface

query can be db.dataset.contact.like('Lach%')

* Project Management
** 2015-03-15-tuftes-gantt-alternative-for-detail-within-context

*** blog
  
  #### Blocker:
      property which allows you to state that a task depends on either
      a previous sibling ("previous-sibling") or
      any other task by stating the task_id property of the predecessor

#+begin_src R :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2015-03-15-tuftes-gantt-alternative-for-detail-within-context.md :exports none :eval no :padline no
  ---
  name: tuftes-gantt-alternative-for-detail-within-context 
  layout: post
  title: tuftes-gantt-alternative-for-detail-within-context 
  date: 2015-03-15
  categories:
  - project management
  ---
  
  - During the end of 2014 I found that the Gantt Chart by TaskJuggler was a struggle to really achieve any decent task management with (fine for higher level overviews though).
  -   I had been following the approach described at [this link](http://orgmode.org/worg/org-tutorials/org-taskjuggler.html)
  - I decided to code up an alternative based on the theory explained on [this link](http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=000076) 
  
  
  #### Project Management Graphics (or Gantt Charts), by Edward Tufte
      Computer screens are generally too small for an overview of big
      serious projects. Horizontal and vertical scrolling are necessary to
      see more than about 40 horizontal time lines for a reasonable period
      of time. Thus, for large projects, print out the sequence on a big
      roll of paper and put it up on a wall.
       
      The chart might be retrospective as well as prospective. That is, the
      chart should show actualdates of achieved goals, evidence which will
      continuously reinforce a reality principle on the mythical future
      dates of goal achievement.
       
      Most of the Gantt charts are analytically thin, too simple, and lack
      substantive detail. The charts should be more intense. At a minimum,
      the charts should be annotated--for example, with to-do lists at
      particular points on the grid. Costs might also be included in
      appropriate cells of the table.
       
      About half the charts show their thin data in heavy grid prisons. For
      these charts the main visual statement is the administrative grid
      prison, not the actual tasks contained by the grid. No explicitly
      expressed grid is necessary--or use the ghost-grid graph
      paper. Degrid!
  
  #### The Results:
  
  I used the example for a fictional Journal Paper submission from my favourite reference for anything to do with Project Management:

      Aragon, T., Mier, H. M., Payauys, T., & Siador, C. (2012). 
      Project Management for Health Professionals.
    [http://www.academia.edu/1746564/Project_Management_for_Health_Professionals](http://www.academia.edu/1746564/Project_Management_for_Health_Professionals)    


  <p></p>

  With the following results (PS SVG format allows you to zoom in).

  ![alttext2](/images/gantt_tufte_test.svg)
  
  #### The codes: 
      library(disentangle)
      library(sqldf)
      library(lubridate)
      
      datin  <- read.csv(
      textConnection("
      container_task_title  , task_id                      , allocated , fte , blocker               ,       start_date , effort , status , notes 
      01 Start              , Start                        , ivan      ,   1 , NA                    ,       2015-03-15 ,     1d , DONE   , NA    
      02 Update Lit Review  , Repeat MEDLINE search        , ivan      ,   1 , Start                 ,       2015-03-16 ,     5d , DONE   , NA    
      02 Update Lit Review  , Retrieve articles            , ivan      ,   1 , Repeat MEDLINE search ,               NA ,     5d , DONE   , NA    
      02 Update Lit Review  , Read articles                , ivan      ,   1 ,                       ,       2015-03-26 ,    11d , DONE   ,       
      02 Update Lit Review  , Summarize articles           , ivan      ,   1 ,                       ,       2015-04-06 ,     9d , TODO   ,       
      03 Write Draft        , Write introduction           , ivan      ,   1 ,                       ,       2015-04-09 ,     6d , TODO   ,       
      03 Write Draft        , Write methods                , ivan      ,   1 , Start                 ,                  ,    15d , TODO   ,       
      03 Write Draft        , Write results                , ivan      ,   1 ,                       ,       2015-03-30 ,    10d , TODO   ,       
      03 Write Draft        , Write discussion             , ivan      ,   1 ,                       ,       2015-04-15 ,    10d , TODO   ,       
      04 Internal Review    , Send to co-author for review , ivan      ,   1 , Write discussion      ,                  ,     2d , TODO   ,        
      04 Internal Review    , Revise draft 1               , ivan      ,   1 ,                       ,       2015-04-19 ,    10d , TODO   ,       
      05 Peer Review        , Submit article 1             , ivan      ,   1 , Revise draft 1        ,                  ,     5d , TODO   ,       
      06 Revise and Resubmit, Revise draft 2               , ivan      ,   1 ,                       ,       2015-04-30 ,    10d , TODO   ,       
      06 Revise and Resubmit, Submit article 2             , ivan      ,   1 , Revise draft 2        ,                  ,     5d , TODO   ,       
      07 End                , Accepted                     , ivan      ,   1 ,                       ,       2015-05-15 ,     1d , TODO   ,       
      "),
      stringsAsFactor = F, strip.white = T)
      # or 
      # datin <- get_gantt_data("gantt_todo", test_data = T) # need to
      # adjust min_context_xrange to 2015-01-01 or something
      datin$start_date  <- as.Date(datin$start_date)
      str(datin)
      datin
      
      dat_out <- gantt_data_prep(dat_in = datin)
      str(dat_out)
      dat_out
      svg("tests/gantt_tufte_test.svg",height=10,width=8)
      gantt_tufte(dat_out, focal_date = "2015-04-13", time_box = 3*7,
                  min_context_xrange = "2015-03-16",
                  cex_context_ylab = 0.65, cex_context_xlab = .7,
                  cex_detail_ylab = 0.9,  cex_detail_xlab = .4,
                  show_today = F)
      dev.off()
      
  
  
  
      
#+end_src

** gantt_tufte
*** EXAMPLES
/home/ivan_hanigan/projects/asn-ltern.bitbucket.org/ServerDetails/gantt-chart

*** COMMENT R-gantt_tufte_test_data
#+name:gantt_tufte
#+begin_src R :session *R* :tangle no :exports none :eval yes
  # func
  library(sqldf)
  library(lubridate)
  library(swishdbtools)
  ch <- connect2postgres('localhost','gantt_tufte2', 'w2p_user', p='xpassword')
  pgListTables(ch, "public")
  
  # load
  datin  <- read.csv(textConnection("container_task_title, task_id, allocated, fte, blocker, start_date, effort
  Container 1, task 0, jim,   1,   ,     2014-12-01, 1m
  Container 1, task 1, jim,   1,   ,     2014-12-20, 1m
  Container 1, task 2, bob,   1, task 1,           , 10d 
  Container 2, task 3, sue,   1,   ,     2014-12-01, 2w
  Container 2, task 4, jim,   1, task 3,           , 2d
  Container 3, task 5, jimmy, 1, task 3,           , 10d
  Container 3, task 6, jimmy, 1,       , 2015-01-01, 10d
  Container 4, task 7, jimmy, 1, task 3,           , 10d
  "),
  stringsAsFactor = F, strip.white = T)
  datin$start_date  <- as.Date(datin$start_date)
  str(datin)
  datin
  
  cnt  <- sqldf("select container_task from datin group by container_task", drv = "SQLite")
  cnt$key_contact  <- NA
  cnt$abstract  <- NA
  cnt
  dbWriteTable(ch, "container_task", cnt, append = T)
  cnt  <- dbReadTable(ch, "container_task")
  cnt
  
  paste(  names(datin), sep = "", collapse = ", ")
  datin2  <- sqldf("select id as container_id, task_id, allocated, fte, blocker, start_date, effort
  from cnt
  join datin
  on cnt.container_task_title = datin.container_task", drv = "SQLite")
  datin2
  datin2$notes_issues  <- NA
  dbWriteTable(ch, "work_package", datin2, append = T)
  
  # psql got munteded, so revert to sqlite, tried swapping to sqlite, noto
  
  ## drv <- dbDriver("SQLite")
  ## tfile <- tempfile()
  ## con <- dbConnect(drv, dbname = "~/tools/web2py/applications/gantt_tufte/databases/storage.sqlite")
  ## dbListTables(con)
  ## datin2 <- dbGetQuery(con , "select * from work_package")
  ## dbWriteTable(ch, "work_package", datin2, append = T)
  
  
  # ended up deleteing from the applications folder
  
   
#+end_src

#+RESULTS: gantt_tufte
=1
==1
==1
==2
==2
==2
==2
==2
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==2
==1
==1
==1
==1
==1
==2
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==2
==2
==1
==1
==2
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==2
==1
==1
==2
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==1
==2
==1
==1
==1
==TRUE
==1
==1
==1
==1
==1
==1
==1
==1
==1

*** COMMENT R-gantt_tufte_preprocessing
#+name:gantt_tufte
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################ 
  gantt_tufte_preprocessing  <- function(
    indat = datin
    ){
    # self join to collect the dependencies
    # paste(names(datint), sep = "", collapse = ", ")
    library(sqldf)
    library(lubridate)
    indat
    #indat$indat_id <- paste(indat$container_task, indat$task_id, sep = "_")
    # self join to return dependents
    indat2 <- sqldf("
    select t1.container_task,
    t1.task_id as predecessor,
    t2.task_id, t2.efforti,
    t1.end
    from indat t1
    left join
    indat t2
    on t1.task_id = t2.blocker
    
    ", drv = 'SQLite')
    #where t2.task_id is not null 
    indat2
    # get any other containers... not sure this helps
    indat2_1 <- sqldf("select t1.container_task, t1.predecessor, t2.predecessor as task_id,
    t2.efforti,
    t2.end
    from indat2 t1
    join
    indat2 t2
    where t1.predecessor = t2.task_id")
    indat2_1
    indat2$start  <- indat2$end 
    indat2$end  <- indat2$start + indat2$efforti
    indat2_1$start  <- indat2_1$end 
    indat2_1$end  <- indat2_1$start + indat2_1$efforti
    indat2  <- indat2[!is.na(indat2$start) & !is.na(indat2$end) ,]
    indat2
    indat2_1
    indat2 <- rbind(indat2, indat2_1)
    
    indat2 <- unique(indat2)
    # now you know the start of the dependents
    
    # now get other independent tasks
    indat3 <- sqldf("select container_task,
    task_id as predecessor,
    task_id,
    efforti,
    end, start
    from indat
    where start is not null
    ")
    # TODO at this point need to figure out how to get proper locs
    #indat3$loc <- nrow(indat3):1
    indat3
    indat2 
    # add loc of siblings
    ## indatx <- sqldf("select t1.*, t2.loc
    ## from indat2 t1
    ## left join
    ## indat3 t2
    ## where (t1.predecessor = t2.task_id)
    ## and t1.task_id is not null
    ## ")
    #indatx
    
    indat4 <- rbind(indat2, indat3)
    indat4 <- indat4[order(indat4$start),]
    indat4[order(indat4$container_task),]
    indat4 
    return(indat4)
  }
  datin2 <- indat4
  #datin2 <- gantt_tufte_preprocessing(datin)
  #str(datin2)
    
#+end_src

*** R timebox

#+name:timebox
#+begin_src R :session *R* :tangle R/timebox.R :exports none :eval yes
  #### name:timebox####
  # func to calculate time boxes
  timebox <- function(dat_in){
    # dat_in  <- datin
    if(
      !exists("dat_in$end_date")
      ) dat_in$end_date <- NA
    # str(dat_in)
    nameslist <- names(dat_in)
    dat_in$effortt <- as.numeric(gsub("[^\\d]+", "", dat_in$effort, perl=TRUE))
    dat_in$effortd <- gsub("d", 1, gsub("[[:digit:]]+", "", dat_in$effort, perl=TRUE))
    dat_in$effortd <- gsub("w", 7, dat_in$effortd)
    dat_in$effortd <- gsub("m", 30.5, dat_in$effortd)
    dat_in$effortd <- as.numeric(dat_in$effortd)
    dat_in$efforti <- dat_in$effortt * dat_in$effortd
    dat_in[is.na(dat_in$end_date),"end_date"] <- dat_in[is.na(dat_in$end_date),"start_date"] + dat_in[is.na(dat_in$end_date),"efforti"]
    dat_in$end_date  <- as.Date(dat_in$end_date, '1970-01-01')
    #   str(dat_in)
    dat_in <- dat_in[,c(nameslist, "efforti")]
    return(dat_in)
  }
  
#+end_src

#+RESULTS: timebox

*** get_gantt_data-code ETL
#+name:get_test_data
#+begin_src R :session *R* :tangle R/get_gantt_data.R :exports none :eval no
  
  library(sqldf)
  library(lubridate)
  library(swishdbtools)
  
  
  get_gantt_data <- function(
    dbname = 'gantt_todo'
    ,
    test_data = T
    ){
  if(test_data != TRUE){
  #### name:get_test_data####
  if(exists("ch"))  dbDisconnect(ch)
  ch <- connect2postgres2(dbname)
  
  datin  <- dbGetQuery(ch,
  "
  select t1.container_task_title, 
  t2.*
  from container_task t1
  join work_package t2
  on t1.id = t2.container_id
  where t2.status != 'DONTSHOW'
  order by container_task_title"
  )
  str(datin)
  datin_done  <- dbGetQuery(ch,
  "
  select t1.container_task_title, 
  t2.*
  from container_task_done t1
  join work_package_done t2
  on t1.id = t2.container_id
  where t2.status != 'DONTSHOW'
  "
  )
  str(datin_done)
  datin  <- rbind(datin, datin_done)
  } else {
  # or simpler
  datin  <- read.csv(textConnection("container_task_title, task_id, allocated, fte, blocker, start_date, effort, status, notes
    Container 1, task 0, jim,   1,   ,     2015-01-01, 1m  , DONE,  
    Container 1, task 1, jim,   1,   ,     2015-01-20, 1m  , DONE,  
    Container 1, task 2, bob,   1, task 1,           , 10d , TODO, This is a note 
    Container 2, task 3, sue,   1,   ,     2015-01-01, 2w  , TODO,  
    Container 2, task 4, jim,   1, task 3,           , 2d  , TODO,  
    Container 3, task 5, jimmy, 1, task 3,           , 10d , TODO,  
    Container 3, task 6, jimmy, 1,       , 2015-02-01, 10d , TODO,  
    Container 4, task 7, jimmy, 1, task 0,           , 10d , TODO,  
    Container 5, task 8, sue,   1,       , 2015-01-14, 5d  , TODO,  
    Container 5, task 9, sue,   1, task 8, , 2d            , TODO,  
    Container 5, task 10, sue,   1, task 9, , 2d           , TODO,  
    Container 5, task 11, sue,   1, task 10, , 2d          , TODO,  
    Container 5, task 12, sue,   1, task 11, , 2d          , TODO,  
    Container 5, task 13, sue,   1, task 12, , 2d          , TODO,  
    Container 5, task 14, sue,   1, task 13, , 2d          , TODO,  
    "),
    stringsAsFactor = F, strip.white = T)
    datin$start_date  <- as.Date(datin$start_date)
    str(datin)
    datin[datin$blocker == "","blocker"] <- NA
  # datin
  }
  
  return(datin)
  }

#+end_src

*** gantt_data_prep-code ETL
#+name:get_test_data
#+begin_src R :session *R* :tangle R/gantt_data_prep.R :exports none :eval no
  
  gantt_data_prep <- function(
    dat_in = datin
    ){
    dat_in <- timebox(dat_in)
    dat_in[1:5,c("task_id","start_date","end_date", "efforti")]
    str(dat_in)
    dat_in  <- dat_in[,c('container_task_title','task_id','allocated','fte','start_date','efforti','notes','status','blocker','end_date')]
    t(dat_in[1,])
    #dat_in
    # dbSendQuery(ch, "drop table indat")
    # dbWriteTable(ch, "indat", dat_in)
    
    indat <- dat_in
    dat_in_depends <- sqldf("
    select tab1.container_task_title, tab1.task_id, 
    'depends on ' || tab1.blocker || ' from Container ' || tab2.container_task_title as depends_on,
    tab2.end_date as start_date, 
    tab1.efforti, tab1.status
    from
    (
      select t1.container_task_title,
      t1.task_id, t1.blocker,
      t1.start_date,
      t1.end_date,
      t1.efforti, t1.status
      from indat t1
      where t1.blocker is not null
      ) tab1
    join
    indat tab2
    on tab1.blocker = tab2.task_id
    ", drv = "SQLite")
    # cast(tab2.end_date + (tab1.efforti || ' day')::INTERVAL as date) as
    # end_date
    dat_in_depends[1,]
    #dat_in_depends
    dat_in_depends$end_date  <- dat_in_depends$start_date + dat_in_depends$tab1.efforti
    names(dat_in_depends) <- gsub('tab1.', '', names(dat_in_depends))
    
    dat_in <- sqldf("
      select t1.container_task_title,
      t1.task_id, 
      t1.task_id as depends_on,  
      t1.start_date,
      t1.efforti,
      t1.status,
      t1.end_date
      from indat t1
      where t1.blocker is null or t1.blocker = ''
      order by container_task_title
    ", drv = 'SQLite')
    dat_in[,1]
    dat_in <- rbind(dat_in, dat_in_depends)
    dat_in[1,]
    #dat_in
    loc  <- sqldf("select container_task_title from dat_in group by container_task_title", drv = "SQLite")
    loc$loc  <- nrow(loc):1
    loc
    dat_in <- merge(loc, dat_in)
    str(dat_in)
    loc
    dat_out <- as.data.frame(matrix(NA, nrow = 0, ncol = ncol(dat_in) + 1))
    #names(qc) <- c(names(dat_in),"loc2")
    for(loci in loc$loc){
    # loci = loc$loc[1]
    qc <- dat_in[dat_in$loc == loci,]
    qc <- qc[order(qc$start_date),]
    loc2 <- seq(qc$loc[1]-1, qc$loc[1],  1/(length(qc$loc)))
    qc$loc2  <- loc2[(length(loc2)):2] 
    
    dat_out  <- rbind(dat_out, qc)
    
    }
    str(dat_out)
    return(dat_out)
    }
      
#+end_src

#+RESULTS: get_test_data

*** R-gantt_tufte PLOT
#+name:gantt_tufte
#+begin_src R :session *R* :tangle R/gantt_tufte.r :exports none :eval yes
  ################################################################
  # plot 
  
  gantt_tufte <- function(
    indat = dat_out
    ,
    smidge_lab = .15
    ,
    focal_date = '2015-01-18' # Sys.Date()
    , 
    show_today = TRUE
    ,
    time_box = 7 * 2.5
    ,
    end_task_ticks = F
    ,
    cex_context_ylab = 0.2
    ,
    cex_context_xlab = 0.5
    ,
    cex_context_points = 0.5
    ,
    min_context_xrange =  NA
    , 
    max_context_xrange = NA
    ,
    cex_detail_ylab = 0.7
    ,
    cex_detail_xlab = 1
    ,
    cex_detail_points = 0.7
    ,
    cex_detail_labels = 0.7
    ){
    focal_date <- as.Date(focal_date)
    m <- matrix(c(1,2), 2, 1)
    layout(m, widths=c(1), heights=c(.75,4))
    par(mar = c(3,16,2,1))
    # layout.show(2)
  
  
    yrange <- c((min(indat$loc2) - smidge_lab), (max(indat$loc2) + smidge_lab))
    if(!is.na(min_context_xrange)){
    xmin <- as.Date(min_context_xrange)    
    } else {
    xmin <- min(indat$start_date, na.rm = T)
    }
    if(!is.na(max_context_xrange)){
    xmax <- as.Date(max_context_xrange)    
    } else {
    xmax <- max(indat$start_date, na.rm = T)
    }
  
    xrange  <- c(xmin,xmax)
    
    # xrange
    #### context ####
    
    plot(xrange, yrange, type = 'n', xlab = "", ylab = "", axes = F )
    indat_lab  <- sqldf("select container_task_title, loc from indat group by container_task_title, loc", drv = "SQLite")
    mtext(c(indat_lab$container_task_title), 2, las =1, at = indat_lab$loc, cex = cex_context_ylab)
  
    polygon(c(focal_date, focal_date + time_box, focal_date + time_box, focal_date), c(rep(yrange[1],2), rep(yrange[2],2)), col = 'lightyellow', border = 'lightyellow')
  # DONE is grey
  indat_done <- indat[indat$status == 'DONE',]
    points(indat_done$start_date, indat_done$loc2, pch = 16, cex = cex_context_points, col = 'grey')
    #text(indat_done$start_date, indat_done$loc2 - smidge_lab, labels = indat_done$task_id, pos = 4)
    js <- indat_done$loc2
    for(i in 1:nrow(indat_done)){
    # = 1
      segments(indat_done$start_date[i] , js[i] , indat_done$start_date[i] , max(indat_done$loc2) + 1 , lty = 3, col = 'grey')
      segments(indat_done$start_date[i] , js[i] , indat_done$end_date[i] , js[i], col = 'grey')
    }
  # indat todo is black
  indat_todo <- indat[indat$status == 'TODO',]
    points(indat_todo$start_date, indat_todo$loc2, pch = 16, cex = cex_context_points)
    #text(indat_todo$start_date, indat_todo$loc2 - smidge_lab, labels = indat_todo$task_id, pos = 4)
    js <- indat_todo$loc2
    for(i in 1:nrow(indat_todo)){
    # = 1
      segments(indat_todo$start_date[i] , js[i] , indat_todo$start_date[i] , max(indat_todo$loc2) + 1 , lty = 3)
      segments(indat_todo$start_date[i] , js[i] , indat_todo$end_date[i] , js[i] )
    }  
    #segments(focal_date, yrange[1], focal_date, yrange[2], 'red')
    xstart_date <- ifelse(wday(xrange[1]) != 1, xrange[1] - (wday(xrange[1]) - 2), xrange[1])
    xend <- ifelse(wday(xrange[2]) != 7, xrange[2] + (5-wday(xrange[2])), xrange[2] )
    at_dates  <- seq(xstart_date, xend, 7)
    label_dates  <-
      paste(month(as.Date(at_dates, "1970-01-01"), label = T),
      day(as.Date(at_dates, "1970-01-01")),
      sep = "-")
  
    axis(1, at = at_dates, labels = label_dates, cex.axis = cex_context_xlab)
    #axis(3)
    if(show_today) segments(Sys.Date(), min(js), Sys.Date(), max(js), lty = 2, col = 'blue')
    
    #### detail ####
    js <- indat$loc2
    # todo
    plot(c(focal_date, focal_date + time_box), yrange, type = 'n', xlab = "", ylab = "", axes = F)
         
    mtext(c(indat_lab$container_task_title), 2, las =1, at = indat_lab$loc, cex = cex_detail_ylab)
    points(indat$start_date, indat$loc2, pch = 16, cex = cex_detail_points)
    text(indat$start_date, indat$loc2 - smidge_lab, labels = indat$task_id, pos = 4,
         cex = cex_detail_labels)
    for(i in 1:nrow(indat)){
    # = 1
      segments(indat$start_date[i] , js[i] , indat$start_date[i] , max(indat$loc2) + 1 , lty = 3,
        col = ifelse(indat$status[i] == "DONE", "grey","black"))
      segments(indat$start_date[i] , js[i] , indat$end_date[i] , js[i],
        col = ifelse(indat$status[i] == "DONE", "grey","black"))
    }
    # done
    indat_done  <- indat[indat$status == "DONE",]
    points(indat_done$start_date, indat_done$loc2, pch = 16, cex = cex_detail_points, col = "darkgrey")
    text(indat_done$start_date, indat_done$loc2 - smidge_lab, labels = indat_done$task_id, pos = 4,
         cex = cex_detail_labels, col = "darkgrey")  
    for(i in 1:nrow(indat_done)){
    # = 1
      segments(indat_done$start_date[i] , indat_done$loc2[i] , indat_done$start_date[i] , max(indat_done$loc2) + 1 , lty = 3, col = 'darkgrey')
      segments(indat_done$start_date[i] , indat_done$loc2[i] , indat_done$end_date[i] , indat_done$loc2[i], col = 'darkgrey' )
    }
  
    # continuing
  
    bumped_up <- indat[indat$start_date < focal_date & indat$status != 'DONE',]
    if(nrow(bumped_up) > 0){
    text(focal_date, bumped_up$loc2 - smidge_lab, labels = bumped_up$task_id, pos = 4,
         cex = cex_detail_labels, col = 'darkred')
    }

    bumped_up2 <- indat[indat$start_date < focal_date & indat$status == 'DONE' & indat$end_date >= focal_date,]
    if(nrow(bumped_up2) > 0){
    text(focal_date, bumped_up2$loc2 - smidge_lab, labels = bumped_up2$task_id, pos = 4,
         cex = cex_detail_labels, col = 'grey')
    }
    
    # overdue
    ## bumped_up <- indat[indat$end_date < focal_date & indat$status != 'DONE',]
    ## text(focal_date, bumped_up$loc2 - smidge_lab, labels = bumped_up$task_id, pos = 4,
    ##      cex = cex_detail_labels, col = 'darkorange')
    
    #segments(focal_date, yrange[1], focal_date, yrange[2], 'red')
    xstart_date <- ifelse(wday(focal_date) != 1, focal_date - (wday(focal_date) - 2), focal_date)
    xend <- ifelse(wday(focal_date + time_box) != 7, (focal_date + time_box) + (5-wday(focal_date + time_box)), (focal_date + time_box))
    at_dates  <- seq(xstart_date, xend, 1)
    at_dates2  <- seq(xstart_date, xend, 7)
    
    label_dates  <-
      paste(month(as.Date(at_dates2, "1970-01-01"), label = T),
      day(as.Date(at_dates2, "1970-01-01")),
      sep = "-")
  
    axis(1, at = at_dates, labels = F)
    axis(1, at = at_dates2, labels = label_dates,  cex = cex_detail_xlab)
    #segments(min(xrange), min(yrange) - .09, max(xrange), min(yrange) - .09)
    axis(3, at = at_dates, labels = F)
    axis(3, at = at_dates2, labels = label_dates)
    #segments(min(xrange), max(yrange) + .09, max(xrange), max(yrange) + .09)  
    if(show_today) segments(Sys.Date(), min(js), Sys.Date(), max(js) + 1, lty = 2, col = 'blue')
    
  }
  #ls()
  
#+end_src
*** man-gantt_tufte
#+name:gantt_tufte
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # arguments: gantt_tufte
  # this is a ploting function, depends on timebox and preprocessing 

  # args
  ## indat = datin4
  ## smidge_lab = .15
  ## focal_date = Sys.Date()
  ## time_box = 21
  ## end_task_ticks = F # this is the little tick marking the end of the tasks

#+end_src


      
*** test go
**** COMMENT test1-code

#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test2####
  source("R/timebox.R")
  source("R/get_gantt_data.R")
  source("R/gantt_data_prep.R")
  source("R/gantt_tufte.r")
  dat_in <- get_gantt_data(test_data = F)
  dat_out <- gantt_data_prep(dat_in)
  str(dat_out)
    svg("tests/AAPL.svg",height=26,width=18)
    gantt_tufte(dat_out, focal_date = "2015-01-10", cex_context_ylab = 0.45,
     min_context_xrange = "2015-01-01", time_box = 7 * 3.5, cex_detail_xlab = .4)
    dev.off()
  
  #### name:tat####
  #library(devtools)
  #install_github("ivanhanigan/disentangle")
  setwd("tests")
  require(knitr)
  require(markdown)
  opts_chunk$set(fig.align=â€leftâ€)
  knit2html("gantt_tufte_test.Rmd", options = c("toc", markdown::markdownHTMLOptions(TRUE)), stylesheet = "custom.css")
  setwd("..")
#+end_src

#+RESULTS:
: /home/ivan_hanigan/tools/disentangle/tests

: 
*** COMMENT test RMD
<section>
    <img style="float: left" src="AAPL.svg">
  </section>

#+name:make_html
#+begin_src R :session *R* :tangle tests/gantt_tufte_test.Rmd :exports none :eval yes
  Overview of Gantt Chart
  ===
  
  ivan.hanigan@anu.edu.au
  
  ```{r echo = F, eval=F, results="hide"}
  setwd("tests")
  require(knitr)
  require(markdown)
  opts_chunk$set(fig.align=â€leftâ€)
  knit2html("gantt_tufte_test.Rmd", options = c("toc", markdown::markdownHTMLOptions(TRUE)), stylesheet = "custom.css")
  ```
  
  ```{r}
  print(Sys.Date())
  ```
  
  Introduction
  ---
  
  This is a report of the TODO list broken down by LTERN Data Team member.
  
  
  ![aa](AAPL.svg)  
  
  ```{r}
  print(cat("\n"))
  ```
  
    
  ```{r echo = F, results = "hide", eval = T}
  #### name:test2####
  source("../R/timebox.R")
  source("../R/get_gantt_data.R")
  source("../R/gantt_tufte.r")
  dat <- get_gantt_data(test_data = F)
  # str(dat)
  
  
  datin  <- dbGetQuery(ch,
  "
  select t1.container_task_title, 
  t2.*
  from container_task t1
  join work_package t2
  on t1.id = t2.container_id
  where t2.status != 'DONTSHOW'
  order by container_task_title"
  )
  #str(datin)
  datin_done  <- dbGetQuery(ch,
  "
  select t1.container_task_title, 
  t2.*
  from container_task_done t1
  join work_package_done t2
  on t1.id = t2.container_id
  where t2.status != 'DONTSHOW'
  "
  )
  #str(datin_done)
  datin  <- rbind(datin, datin_done)
   str(datin)
  str(dat)
  as.data.frame(table(datin$name))
  
  
  ```
  
  ```{r echo = F, results = "asis", eval = T}
  library("xtable")
  
  for(i in names(table(datin$name))){
   #i <-names(table(datin$name))[1]
  
   cat(i)
   cat("\n")
   cat("===\nTODO\n---\n")
   # str(dat)
   xdat <- sqldf(
   sprintf("select name, t1.container_task_narrow as task_group , t1.task_id,
   t2.start_date, t2.end_date, t2.efforti as effort_days, t1.notes, t2.status,
   case when t2.depends_on = t1.task_id then '' else t2.depends_on end as depends_on
   from datin t1
   left join
   dat t2
   on t1.task_id = t2.task_id
   where t1.name = '%s'
   order by t2.start_date
   ", i),
   drv = "SQLite")
    xdat$start_date <- as.character(format(xdat$start_date, "%A, %d-%b-%Y"))
    xdat$end_date <- as.character(format(xdat$end_date, "%A, %d-%b-%Y"))
   xdat1 <- subset(xdat, status == 'TODO')
   print(xtable(xdat1), type = "html", include.rownames = F)
  
   xdat2 <- subset(xdat, status == 'DONE')
   xdat2 <- xdat2[which(as.Date(xdat2$end_date, format = "%A, %d-%b-%Y") > Sys.Date() - 7),]
   # xdat2
   if(nrow(xdat2) > 0){
     cat("DONE\n---\n")  
     cat("\n")
     print(xtable(xdat2), type = "html", include.rownames = F)
     }
   }
  
  ```
  
    
#+end_src

#+RESULTS: make_html
*** COMMENT test2-code

| container_task_title | task_id                      | allocated | fte | blocker               |       start_date | effort | status | notes |
| no 1                 | Start                        | ivan      |   1 | NA                    |       2015-03-15 |     1d | DONE   | NA    |
| no 2                 | Repeat MEDLINE search        | ivan      |   1 | Start                 |       2015-03-16 |     5d | TODO   | NA    |
| no 3                 | Retrieve articles            | ivan      |   1 | Repeat MEDLINE search |               NA |     5d | TODO   | NA    |
| no 4                 | Read articles                | ivan      |   1 |                       |       2015-03-26 |    10d | TODO   |       |
| no 5                 | Summarize articles           | ivan      |   1 |                       |       2015-04-06 |     5d | TODO   |       |
| no 6                 | Write introduction           | ivan      |   1 |                       |       2015-04-11 |     5d | TODO   |       |
| no 7                 | Write methods                | ivan      |   1 | Start                 |                  |    10d | TODO   |       |
| no 8                 | Write results                | ivan      |   1 | Start                 |                  |    10d | TODO   |       |
| no 9                 | Write discussion             | ivan      |   1 | Write results         |                  |    10d | TODO   |       |
| no 10                | Send to co-author for review | ivan      |   1 | Write discussion      |                  |     2d | TODO   |       |
| no 11                | Revise draft 1               | ivan      |   1 |                       |       2015-04-30 |    10d | TODO   |       |
| no 12                | Submit article 1             | ivan      |   1 |                       |   Revise draft 1 |     5d | TODO   |       |
| no 13                | Revise draft 2               | ivan      |   1 |                       |       2015-05-30 |    10d | TODO   |       |
| no 14                | Submit article 2             | ivan      |   1 |                       |   Revise Draft 2 |     5d | TODO   |       |
| no 15                | Accepted                     | ivan      |   1 |                       | Submit article 2 |     1d | TODO   |       |

*** code name:test2
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test2####
  library(disentangle)
  library(sqldf)
  library(lubridate)
  
  datin  <- read.csv(
  textConnection("
  container_task_title  , task_id                      , allocated , fte , blocker               ,       start_date , effort , status , notes 
  01 Start              , Start                        , ivan      ,   1 , NA                    ,       2015-03-15 ,     1d , DONE   , NA    
  02 Update Lit Review  , Repeat MEDLINE search        , ivan      ,   1 , Start                 ,       2015-03-16 ,     5d , DONE   , NA    
  02 Update Lit Review  , Retrieve articles            , ivan      ,   1 , Repeat MEDLINE search ,               NA ,     5d , DONE   , NA    
  02 Update Lit Review  , Read articles                , ivan      ,   1 ,                       ,       2015-03-26 ,    11d , DONE   ,       
  02 Update Lit Review  , Summarize articles           , ivan      ,   1 ,                       ,       2015-04-06 ,     9d , TODO   ,       
  03 Write Draft        , Write introduction           , ivan      ,   1 ,                       ,       2015-04-09 ,     6d , TODO   ,       
  03 Write Draft        , Write methods                , ivan      ,   1 , Start                 ,                  ,    15d , TODO   ,       
  03 Write Draft        , Write results                , ivan      ,   1 ,                       ,       2015-03-30 ,    10d , TODO   ,       
  03 Write Draft        , Write discussion             , ivan      ,   1 ,                       ,       2015-04-15 ,    10d , TODO   ,       
  04 Internal Review    , Send to co-author for review , ivan      ,   1 , Write discussion      ,                  ,     2d , TODO   ,        
  04 Internal Review    , Revise draft 1               , ivan      ,   1 ,                       ,       2015-04-19 ,    10d , TODO   ,       
  05 Peer Review        , Submit article 1             , ivan      ,   1 , Revise draft 1        ,                  ,     5d , TODO   ,       
  06 Revise and Resubmit, Revise draft 2               , ivan      ,   1 ,                       ,       2015-04-30 ,    10d , TODO   ,       
  06 Revise and Resubmit, Submit article 2             , ivan      ,   1 , Revise draft 2        ,                  ,     5d , TODO   ,       
  07 End                , Accepted                     , ivan      ,   1 ,                       ,       2015-05-15 ,     1d , TODO   ,       
  "),
  stringsAsFactor = F, strip.white = T)
  # or 
  # datin <- get_gantt_data("gantt_todo", test_data = T) # need to
  # adjust min_context_xrange to 2015-01-01 or something
  datin$start_date  <- as.Date(datin$start_date)
  str(datin)
  datin
  
  dat_out <- gantt_data_prep(dat_in = datin)
  str(dat_out)
  dat_out
  svg("tests/gantt_tufte_test.svg",height=10,width=8)
  gantt_tufte(dat_out, focal_date = "2015-04-13", time_box = 3*7,
              min_context_xrange = "2015-03-16",
              cex_context_ylab = 0.65, cex_context_xlab = .7,
              cex_detail_ylab = 0.9,  cex_detail_xlab = .4,
              show_today = F)
  dev.off()
  
#+end_src

#+RESULTS: test2
: 1

* Workflow Tools


** R-newnode
*** COMMENT R-newnode
#+name:newnode
#+begin_src R :session *R* :tangle R/newnode.r :exports none :eval yes
  ################################################################
  # name:newnode
  newnode<-function(
    name = "name_of_step"
    ,
    inputs= c("input_to_step", "input2", "in3", "in4")
    ,
    outputs= c("output_from_step", "out2", "out3") # character(0)
    ,
    desc = "some (potentially) long descriptive text saying what this step is about and why and how"
    ,
    graph = 'nodes'
    , newgraph=F, notes=F, code=NA, ttype=NA, plot = T,
    rgraphviz = F,
    nchar_to_snip = 40
    ){
     if(rgraphviz == F){
  
    if(nchar(name) > 140) print("that's a long name. consider shortening this")
    if(nchar(desc) > nchar_to_snip) desc <- paste(substr(desc, 1, nchar_to_snip), "[...]")
    name2paste <- paste('"', name, '"', sep = "")
    inputs <- paste('"', inputs, '"', sep = "")
    inputs_listed <- paste(inputs, name2paste, sep = ' -> ', collapse = "\n")
    #cat(inputs_listed)
    outputs <- paste('"', outputs, '"', sep = "")  
    outputs_listed <- paste(name2paste, outputs, sep = ' -> ', collapse = "\n")
    #cat(outputs_listed)
    strng <- sprintf('%s
  %s  [ shape=record, label="{{ { Name | Description } | { %s | %s } }}"] 
  %s\n\n', inputs_listed, name2paste, name, desc, outputs_listed
    )
    if(newgraph == F) eval(parse(text =
                                   sprintf('strng <- paste(%s, strng, "\n")', graph, graph)
                             ))
    # cat(strng)
    return(strng)
  
    } else {
    # USAGE
    # nodes <- newnode(  # adds to a graph called nodes
    # name = 'aquire the raw data'  # the name of the node being added 
    # inputs = REQUIRED c('external sources','collected by researcher') # single or multiple inputs to it
    # outputs = OPTIONAL c('file server','metadata','cleaning') # single or multiple outputs from it
    # append=F # append to existing graph?  if False remove old graph of that name and start new
    # TODO 
    # nodes <- addEdge(from='analyse using stats package',
    # to='new data in database server',graph=nodes,weights=1)
    # INIT
    # source('http://bioconductor.org/biocLite.R')
    # biocLite("Rgraphviz")
    # or may be needed for eg under ubuntu
    # biocLite("Rgraphviz", configure.args=c("--with-graphviz=/usr"))
    require(Rgraphviz)
    # FURTHER INFO
    # see the Rgraphviz examples
    # example(layoutGraph)
    # require(biocGraph) # for imageMap
    # TODO change names in following
    dsc <- name
    i <- inputs
    o <- outputs
    #   if(!exists('nodes')) {
    if(newgraph==T) {    
      nodes <- new("graphNEL", nodes=c(dsc),
                 edgemode="directed")
      # nodes <- addEdge(from=i, to=dsc, graph=nodes, 1)    
    } else {
      if(length(grep(dsc,nodes@nodes)) == 0) nodes <- addNode(node=dsc,object=nodes)
    }  
    if(sum(i %in% nodes@nodes) != length(i)) {
      inew <- i[!i %in% nodes@nodes]
      nodes <- addNode(node=inew,object=nodes)   
    }
    nodes <- addEdge(i, dsc, nodes, 1)
    #}
    if(length(o) > 0){
    if(sum(o %in% nodes@nodes) != length(o)) {
      onew <- o[!o %in% nodes@nodes]
      nodes <- addNode(node=onew,object=nodes)   
    }
    nodes <- addEdge(from=dsc, to=o, graph=nodes, 1)  
    }
    if(plot == T){
      try(silent=T,dev.off())
      plot(nodes,attrs=list(node=list(label="foo", fillcolor="grey",shape="ellipse", fixedsize=FALSE), edge=list(color="black")))
    }
    return(nodes)
    }
  }
  
#+end_src

#+RESULTS: newnode

*** test-newnode
#+name:newnode
#+begin_src R :session *R* :tangle tests/test-newnode.r :exports reports :eval no
  ################################################################
  # name:newnode
  # REQUIRES GRAPHVIZ, AND TO INSTALL RGRAPHVIZ
  # source('http://bioconductor.org/biocLite.R')
  # biocLite("Rgraphviz")
  # or may be needed for eg under ubuntu
  # biocLite("Rgraphviz", configure.args=c("--with-graphviz=/usr"))
  # FURTHER INFO
  # see the Rgraphviz examples
  # example(layoutGraph)
  # require(biocGraph) # for imageMap
  
  # source("R/newnode.r")
  require(devtools)
  #install_github("disentangle", "ivanhanigan")
  load_all()
  #require(disentangle)
  newnode(
    name = "NAME"
    ,
    inputs="INPUT"
    ,
    outputs = "OUTPUT"
    ,
    graph = 'nodes'
    ,
    newgraph=T
    ,
    notes=F
    ,
    code=NA
    ,
    ttype=NA
    ,
    plot = T, rgraphviz = F
    )
  
  nodes <- newnode("merge", c("d1", "d2", "d3"), c("EDA"),
                   newgraph =T)
  nodes <- newnode("qc", c("data1", "data2", "data3"), c("d1", "d2", "d3"))
  nodes <- newnode("modelling", "EDA")
  nodes <- newnode("model checking", "modelling", c("data checking", "reporting"))
#+end_src
*** COMMENT test-df-input-code
#+name:test-df-input
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:test-df-input####
  library(devtools)
  load_all()
  #require(disentangle)
  # either edit a spreadsheet with filenames, inputs and outputs 
  # filesList <- read.csv("exampleFilesList.csv", stringsAsFactors = F)
  # or 
  filesList <- read.csv(textConnection('
  CLUSTER ,  FILE         , INPUTS                    , OUTPUTS                                , DESCRIPTION                      
  A       ,  siteIDs      , GPS                       ,                                        , latitude and longitude of sites  
  A       ,  weather      , BoM                       ,                                        , weather data from BoM            
  B       ,  trapped      , siteIDs                   ,                                        , counts of species caught in trap 
  B       ,  biomass      , siteIDs                   ,                                        ,                                  
  B       ,  correlations , "weather,trapped,biomass" , report1                                , A study we published             
  C       ,  paper1       , report1                   , "open access repository, data package" ,                                  
  '), stringsAsFactors = F, strip.white = T)
  str(filesList)
  filesList
  ## newnode_df <- function(indat, names_col, in_col, out_col, desc_col, clusters_col){
  ## # start the graph
  i <- 1
  nodes <- newnode(name = indat[i,names_col],
                   inputs = strsplit(indat[,in_col], ",")[[i]],
                   outputs =
                   strsplit(indat[,out_col], ",")[[i]]
                   ,
                   newgraph=T)
   
  for(i in 2:nrow(indat))
  {
    # i <- 2
    if(length(strsplit(indat[,out_col], ",")[[i]]) == 0)
    {
      nodes <- newnode(name = indat[i,names_col],
                       inputs = strsplit(indat[,in_col], ",")[[i]]
      )    
    } else {
      nodes <- newnode(name = indat[i,names_col],
                       inputs = strsplit(indat[,in_col], ",")[[i]],
                       outputs = strsplit(indat[,out_col], ",")[[i]]
      )
    }
  }
  
  help_txt <- c('
  sink("fileTransformations.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  ')
  cat(help_txt)
  ## return(nodes)
  ## }
  
  
  ## newnode_df <- function(indat, names_col, in_col, out_col, desc_col, clusters_col, graph = 'nodes_list'){
  ## # start the graph
  ## i <- 1
  ## nodes_list <- eval(parse(text=sprintf('newnode(name = indat[i,names_col],
  ##                  inputs = strsplit(indat[,in_col], ",")[[i]],
  ##                  outputs =
  ##                  strsplit(indat[,out_col], ",")[[i]]
  ##                  ,
  ##                  newgraph=T, graph = %s)', graph)))
   
  ## for(i in 2:nrow(indat))
  ## {
  ##   # i <- 2
  ##   if(length(strsplit(indat[,out_col], ",")[[i]]) == 0)
  ##   {
  ##     nodes_list <- eval(parse(text=sprintf('newnode(name = indat[i,names_col],
  ##                      inputs = strsplit(indat[,in_col], ",")[[i]]
  ##     , graph = %s)',graph)))    
  ##   } else {
  ##     nodes_list <- eval(parse(text=sprintf('newnode(name = indat[i,names_col],
  ##                      inputs = strsplit(indat[,in_col], ",")[[i]],
  ##                      outputs = strsplit(indat[,out_col], ",")[[i]]
  ##     , graph = %s)', graph)))
  ##   }
  ## }
  ## cat(nodes_list)
  ## help_txt <- c('
  ## sink("fileTransformations.dot")
  ## cat("digraph G {")
  ## cat(nodes)
  ## cat("}")
  ## sink()
  ## system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  ## ')
  ## cat(help_txt)
  ## return(nodes_list)
  ## }
  
  
  
  
  str(filesList)
  ## undebug(newnode_df)
  ## nodes <- newnode_df(
  ## indat = filesList
  ## ,
  ## names_col = "FILE"
  ## ,
  ## in_col = "INPUTS"
  ## ,
  ## out_col = "OUTPUTS"
  ## ,
  ## desc_col = "DESCRIPTION"
  ## ,
  ## clusters_col = "CLUSTER"
  ## )
  cat(nodes)
  
  sink("fileTransformations.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  
#+end_src
*** COMMENT test-df-input-code2
#+name:test-df-input
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test-df-input####
  library(devtools)
  load_all()
  filesList <- read.csv(textConnection('
  CLUSTER ,  FILE    , INPUTS                    , OUTPUTS                                , DESCRIPTION                      
  A  ,  siteIDs      , GPS                       , spatial                                , latitude and longitude of sites  
  A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
  B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
  B  ,  biomass      , spatial                   , biomass_g                              ,                                  
  B  ,  correlations , "exposures,trapped_no,biomass_g" , report1                         , A study we published             
  C  ,  paper1       , report1                   , "open access repository, data package" ,                                  
  D  ,  biomass revision, new estimates          , biomass_g                              , this came late
  '), stringsAsFactors = F, strip.white = T)
  str(filesList)
  
  indat = filesList
  #,
  names_col = "FILE"
  #,
  in_col = "INPUTS"
  #,
  out_col = "OUTPUTS"
  #,
  desc_col = "DESCRIPTION"
  #,
  clusters_col = "CLUSTER"
  
  ## # start the graph
  rm(nodes)
  
  cluster_ids <- names(table(indat[,clusters_col]))
  cluster_ids
  for(cluster_i in cluster_ids){
    #cluster_i <- cluster_ids[1]
    if(cluster_i == cluster_ids[1]){
    nodes <- sprintf('subgraph cluster_%s {
    label = "%s"
    ', cluster_i, cluster_i)
    
    i <- 1
    nodes <- newnode(name = indat[i,names_col],
                     inputs = strsplit(indat[,in_col], ",")[[i]],
                     outputs =
                     strsplit(indat[,out_col], ",")[[i]]
                     ,
                     newgraph=F)
    } else {
    nodes <- paste(nodes, sprintf('subgraph cluster_%s {
    label = "%s"
    ', cluster_i, cluster_i))
  
    #i <- 1
    if(length(strsplit(indat2[,out_col], ",")[[i]]) == 0){
      nodes <- newnode(name = indat2[i,names_col],
                       inputs = strsplit(indat2[,in_col], ",")[[i]]
      )
    } else {
      nodes <- newnode(name = indat2[i,names_col],
                       inputs = strsplit(indat2[,in_col], ",")[[i]],
                       outputs = strsplit(indat2[,out_col], ",")[[i]]
      )
    }
    }
  #cat(nodes)


  indat2 <- indat[indat[,clusters_col] == cluster_i,]
  indat2
  
  for(i in 2:nrow(indat2)){
    # i <- 2
    if(length(strsplit(indat2[,out_col], ",")[[i]]) == 0){
      nodes <- newnode(name = indat2[i,names_col],
                       inputs = strsplit(indat2[,in_col], ",")[[i]]
      )    
    } else {
      nodes <- newnode(name = indat2[i,names_col],
                       inputs = strsplit(indat2[,in_col], ",")[[i]],
                       outputs = strsplit(indat2[,out_col], ",")[[i]]
      )
    }
  }
  
  nodes <- paste(nodes,"}\n\n")
  }
  cat(nodes)
  help_txt <- c('
  sink("fileTransformations.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  ')
  cat(help_txt)
  cat(nodes)
  
  sink("fileTransformations.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  
#+end_src

#+RESULTS: test-df-input
=0
==0
==0
=*** COMMENT test2-code
#+name:test2
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:test2####
  getwd()
  source("R/newnode.r")
  newnode
  rm(nodes)
  nodes <- newnode("Setup_Data.R", inputs = "data",
                   outputs = c("stag.data.all.years.df",
                     "ash.fire.matrix.df",
                     "stag.data.yyyy.xx.df"), # "Subsets of the data by year and fire type"),
                   newgraph = T)
  
  nodes <- newnode("average number of stags standing per site",
                   inputs = "stag.data.yyyy.xx.df",
                   outputs = "ash.no.stags.yyyy.xx"
                   )
  
  nodes <- newnode("variance in the average number of stags per site",
                   inputs = "stag.data.yyyy.xx.df", 
                   outputs = "ash.var.no.stags.yyyy.xx"
                   )
  nodes <- newnode("compute the number of transitions from form to form",
                   inputs = "stag.data.yyyy.xx.df",
                   outputs = "tpm.98.11.xx.coll.ash"
                   )
  nodes <- newnode("convert to a probability transition matrix",
                   inputs = "tpm.98.11.xx.coll.ash",
                   outputs = "tpm.98.11.xx.coll.ash.per"                 
                   )
  nodes <- newnode("frequencies) by form  in 2011 by fire category ",
                   inputs = "stag.data.2011.xx.df",
                   outputs = "freq.11.xx.coll.ash"
                   )
  sink("test.dot")
  cat("digraph G {")
  cat(nodes)
  cat("}")
  sink()
  system("dot -Tpdf test.dot -o test.pdf")
#+end_src

*** COMMENT TODO man-newnode
#+name:newnode
#+begin_src R :session *R* :tangle no :exports none :eval no
################################################################
# name:newnode

#+end_src

** R-newnode_df
*** COMMENT newnode_df-code
#+name:newnode_df
#+begin_src R :session *R* :tangle R/newnode_df.R :exports none :eval no
  #' @title newnode_df
  #' @name newnode_df
  #' @param indat the input data.frame
  #' @param names_col the name of each edge (the boxes)
  #' @param in_col the name of the nodes that are inputs to each edge with comma seperated vals.  whitespace will be stripped
  #' @param out_col the nodes that are outputs of each edge.
  #' @param desc_col description
  #' @param clusters_col optional column identifying clusters
  #' @param todo_col optional column with TODO status (DONE and WONTDO will be white, others are red)
  #' @return character string object that has the DOT language representatio of the input
  newnode_df <- function(indat = NA,names_col = NA,in_col = NA,out_col = NA,desc_col = NA,clusters_col = NA, todo_col = NA, nchar_to_snip = 40){
  if(any(is.na(indat) |
         is.na(names_col) |
         is.na(in_col) |
         is.na(out_col) |
         is.na(desc_col)
         )) stop("data or arguments are not supplied")
  
  if(!is.na(clusters_col)){
  cluster_ids <- names(table(indat[,clusters_col]))
  #cluster_ids
  
  for(cluster_i in cluster_ids){
    # cluster_i <- cluster_ids[1]
  
    if(cluster_i == cluster_ids[1]){
      nodes_graph <- sprintf('subgraph cluster_%s {
      label = "%s"
      ', cluster_i, cluster_i)
    } else {
      nodes_graph <- paste(nodes_graph, sprintf('subgraph cluster_%s {
      label = "%s"
      ', cluster_i, cluster_i))  
    }
  #  cat(nodes_graph)    
    indat2 <- indat[indat[,clusters_col] == cluster_i,]
  #  indat2
      for(i in 1:nrow(indat2)){
        # i <- 1
        #i
        indat2[i,]
        name <- indat2[i,names_col]
        inputs <- unlist(lapply(strsplit(indat2[i,in_col], ","), str_trim))
        outputs <- unlist(lapply(strsplit(indat2[i,out_col], ","), str_trim))
        desc <- indat2[i,desc_col]
  
  
        if(nchar(name) > 140) print("that's a long name. consider shortening this")
        if(nchar(desc) > nchar_to_snip) desc <- paste(substr(desc, 1, nchar_to_snip), "[...]")
        name2paste <- paste('"', name, '"', sep = "")
        inputs <- paste('"', inputs, '"', sep = "")
        #inputs
        inputs_listed <- paste(inputs, name2paste, sep = ' -> ', collapse = "\n")
        #cat(inputs_listed)
        outputs <- paste('"', outputs, '"', sep = "")  
        outputs_listed <- paste(name2paste, outputs, sep = ' -> ', collapse = "\n")
        #cat(outputs_listed)
  if(!is.na(todo_col)){
        
        status <- indat2[i,todo_col]      
        
  strng <- sprintf('%s\n%s  [ shape=record, label="{{ { Name | Description | Status } | { %s | %s | %s } }}"]\n%s\n\n',
                   inputs_listed, name2paste, name, desc, status, outputs_listed
                   )
        # cat(strng)
        if(!status %in% c("DONE", "WONTDO")){ 
          strng <- gsub("shape=record,", "shape=record, style = \"filled\", color=\"indianred\",", strng)
        }
      } else {
    
  strng <- sprintf('%s\n%s  [ shape=record, label="{{ { Name | Description } | { %s | %s } }}"]\n%s\n\n',
                   inputs_listed, name2paste, name, desc, outputs_listed
                   )
  }
        nodes_graph <- paste(nodes_graph, strng, "\n")
        if(nrow(indat2) == 1) break
      }
  nodes_graph <- paste(nodes_graph, "}\n\n")
  }
  } else {
    indat2 <- indat
    nodes_graph  <- ""
  #  indat2
      for(i in 1:nrow(indat2)){
        # i <- 1
        #i
        indat2[i,]
        name <- indat2[i,names_col]
        inputs <- unlist(lapply(strsplit(indat2[i,in_col], ","), str_trim))
        outputs <- unlist(lapply(strsplit(indat2[i,out_col], ","), str_trim))
        desc <- indat2[i,desc_col]
  
        if(nchar(name) > 140) print("that's a long name. consider shortening this")
        if(nchar(desc) > nchar_to_snip) desc <- paste(substr(desc, 1, nchar_to_snip), "[...]")
        name2paste <- paste('"', name, '"', sep = "")
        inputs <- paste('"', inputs, '"', sep = "")
        #inputs
        inputs_listed <- paste(inputs, name2paste, sep = ' -> ', collapse = "\n")
        #cat(inputs_listed)
        outputs <- paste('"', outputs, '"', sep = "")  
        outputs_listed <- paste(name2paste, outputs, sep = ' -> ', collapse = "\n")
        #cat(outputs_listed)
  if(!is.na(todo_col)){
    status <- indat2[i,todo_col]
         
  strng <- sprintf('%s\n%s  [ shape=record, label="{{ { Name | Description | Status } | { %s | %s | %s } }}"]\n%s\n\n',
                   inputs_listed, name2paste, name, desc, status, outputs_listed
                   )
        # cat(strng)
        if(!status %in% c("DONE", "WONTDO")){ 
          strng <- gsub("shape=record,", "shape=record, style = \"filled\", color=\"indianred\",", strng)
        }
  } else {
    
  strng <- sprintf('%s\n%s  [ shape=record, label="{{ { Name | Description } | { %s | %s } }}"]\n%s\n\n',
                   inputs_listed, name2paste, name, desc, outputs_listed
                   )
  }
        nodes_graph <- paste(nodes_graph, strng, "\n")
  }
  }
  nodes_graph <- paste("digraph transformations {\n\n",
                       nodes_graph,
                       "}\n")
  
  help_text <- c('# to run this graph
  sink("fileTransformations.dot")
  cat(nodes_graph)
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  ')
  cat(help_text)
  return(nodes_graph)
  }
  
#+end_src

*** COMMENT test-df-input-code3
#+name:test-df-input
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test-df-input####
  library(devtools)
  load_all()
  library(stringr)
  ## filesList <- read.csv(textConnection('
  ## CLUSTER ,  FILE    , INPUTS                    , OUTPUTS                                , DESCRIPTION                      
  ## A  ,  siteIDs      , "GPS, helicopter"          , "spatial, site doco"                 , latitude and longitude of sites  
  ## A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
  ## B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
  ## B  ,  biomass      , spatial                   , biomass_g                              ,                                  
  ## B  ,  correlations , "exposures,trapped_no,biomass_g" , report1                         , A study we published             
  ## C  ,  paper1       , report1                   , "open access repository, data package" ,                                  
  ## D  ,  biomass revision, new estimates          , biomass_g                              , this came late
  ## '), stringsAsFactors = F, strip.white = T)
  #write.csv(filesList, "fileTransformations.csv", row.names = F)
  filesList <- read.csv("fileTransformations.csv", stringsAsFactors = F, strip.white = T)
  
  str(filesList)
  # filesList
  
  nodes_graphy <- newnode_df(
    indat = filesList
    ,
    names_col = "STEP"
    ,
    in_col = "INPUTS"
    ,
    out_col = "OUTPUTS"
    ,
    desc_col = "DESCRIPTION"
    ,
    clusters_col = "CLUSTER"
    ,
    todo_col = NA #"STATUS"
    ,
    nchar_to_snip = 40
    )
  
  sink("fileTransformations.dot")
  cat(nodes_graphy)
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  
#+end_src

** R-newnode_rmd
*** COMMENT newnode_df-code
#+name:newnode_rmd
#+begin_src R :session *R* :tangle R/newnode_rmd.R :exports none :eval no
  #' @title newnode_rmd
  #' @name newnode_rmd
  #' @param indat the input data.frame
  #' @param names_col the name of each edge (the boxes)
  #' @param in_col the name of the nodes that are inputs to each edge with comma seperated vals.  whitespace will be stripped
  #' @param out_col the nodes that are outputs of each edge.
  #' @param desc_col description
  #' @param clusters_col optional column identifying clusters
  #' @param todo_col optional column with TODO status (DONE and WONTDO will be white, others are red)
  #' @return character string object that has the DOT language representatio of the input
  newnode_rmd <- function(indat = NA,names_col = NA,in_col = NA,out_col = NA,desc_col = NA,clusters_col = NA, todo_col = NA, nchar_to_snip = 40){
  if(any(is.na(indat) |
         is.na(names_col) |
         is.na(in_col) |
         is.na(out_col) |
         is.na(desc_col)
         )) stop("data or arguments are not supplied")
  
    indat2 <- indat
    nodes_graph  <- ""
  #  indat2
      for(i in 1:nrow(indat2)){
        # i <- 1
        #i
        indat2[i,]
        name <- indat2[i,names_col]
        desc <- indat2[i,desc_col]
  
        #if(nchar(name) > 140) print("that's a long name. consider shortening this")
        #if(nchar(desc) > nchar_to_snip) desc <- paste(substr(desc, 1, nchar_to_snip), "[...]")
        name2paste <- paste('', name, ':', sep = "")
        inputs <- unlist(lapply(strsplit(indat2[i,in_col], ","), str_trim))      
        inputs <- paste('## inputs\n',
                        paste("- ", inputs, '\n',
                                              sep = "", collapse = "")
                        , collapse = "\n", sep = "")
  #      cat(inputs)
  #      inputs_listed <- paste(inputs, name2paste, sep = ' -> ', collapse = "\n")
        #cat(inputs_listed)
        outputs <- unlist(lapply(strsplit(indat2[i,out_col], ","), str_trim))
        outputs <- paste('## outputs\n',
                        paste("- ", outputs, '\n',
                                              sep = "", collapse = "")
                        , collapse = "\n", sep = "")      
  #      outputs <- paste('"', outputs, '"', sep = "")  
  #      outputs_listed <- paste(name2paste, outputs, sep = ' -> ', collapse = "\n")
  #      cat(outputs)
  ## if(!is.na(todo_col)){
  ##   status <- indat2[i,todo_col]
         
  ## strng <- sprintf('%s\n%s  [ shape=record, label="{{ { Name | Description | Status } | { %s | %s | %s } }}"]\n%s\n\n',
  ##                  inputs_listed, name2paste, name, desc, status, outputs_listed
  ##                  )
  ##       # cat(strng)
  ##       if(!status %in% c("DONE", "WONTDO")){ 
  ##         strng <- gsub("shape=record,", "shape=record, style = \"filled\", color=\"indianred\",", strng)
  ##       }
  ## } else {
    
  strng <- sprintf('# %s\n## Description\n\n%s\n\n%s\n\n%s\n\n',
                   name2paste,  desc, inputs, outputs
                   )
  #cat(strng)
  #nodes_graph  <- ""      
        nodes_graph <- paste(nodes_graph, strng, "\n", sep = '')
  #}
  }
  nodes_graph <- paste("---\ntitle: Test report of Workflow
  author: Ivan Hanigan
  output:
    html_document:
      toc: true
      theme: united
      number_sections: yes
  documentclass: article
  classoption: a4paper\n---\n",
                       nodes_graph,
                       "\n", sep = '')
  
  #help_text <- c('# to run this graph
  #sink("fileTransformations.dot")
  #cat(nodes_graph)
  #sink()
  #system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  #')
  #cat(help_text)
  return(nodes_graph)
  }
  
#+end_src

*** COMMENT test-df-input-code3
#+name:test-df-input
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test-df-input####
  library(devtools)
  load_all()
  library(stringr)
  ## filesList <- read.csv(textConnection('
  ## CLUSTER ,  FILE    , INPUTS                    , OUTPUTS                                , DESCRIPTION                      
  ## A  ,  siteIDs      , "GPS, helicopter"          , "spatial, site doco"                 , latitude and longitude of sites  
  ## A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
  ## B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
  ## B  ,  biomass      , spatial                   , biomass_g                              ,                                  
  ## B  ,  correlations , "exposures,trapped_no,biomass_g" , report1                         , A study we published             
  ## C  ,  paper1       , report1                   , "open access repository, data package" ,                                  
  ## D  ,  biomass revision, new estimates          , biomass_g                              , this came late
  ## '), stringsAsFactors = F, strip.white = T)
  #write.csv(filesList, "fileTransformations.csv", row.names = F)
  filesList <- read.csv("fileTransformations.csv", stringsAsFactors = F, strip.white = T)
  
  str(filesList)
  # filesList
  
  nodes_graphy <- newnode_rmd(
    indat = filesList
    ,
    names_col = "STEP"
    ,
    in_col = "INPUTS"
    ,
    out_col = "OUTPUTS"
    ,
    desc_col = "DESCRIPTION"
    ,
    clusters_col = "CLUSTER"
    ,
    todo_col = NA #"STATUS"
    ,
    nchar_to_snip = 40
    )
  
  sink("fileTransformations.Rmd")
  cat(nodes_graphy)
  sink()
  #system("dot -Tpng fileTransformations.dot -o fileTransformations.png")
  
#+end_src

** R-newnode_csv
*** COMMENT newnode_df-code
#+name:newnode_csv
#+begin_src R :session *R* :tangle R/newnode_csv.R :exports none :eval no
  #' @title newnode_csv
  #' @name newnode_csv
  #' @param indat the input data.frame
  #' @param names_col the name of each edge (the boxes)
  #' @param in_col the name of the nodes that are inputs to each edge with comma seperated vals.  whitespace will be stripped
  #' @param out_col the nodes that are outputs of each edge.
  #' @param desc_col description
  #' @param clusters_col optional column identifying clusters
  #' @param todo_col optional column with TODO status (DONE and WONTDO will be white, others are red)
  #' @return character string object that has the DOT language representatio of the input
  newnode_csv <- function(indat = NA,names_col = NA,in_col = NA,out_col = NA,desc_col = NA,clusters_col = NA, todo_col = NA, nchar_to_snip = 40){
  if(any(is.na(indat) |
         is.na(names_col) |
         is.na(in_col) |
         is.na(out_col) |
         is.na(desc_col)
         )) stop("data or arguments are not supplied")
  
    indat2 <- indat
    nodes_graph  <- as.data.frame(matrix(NA, ncol = 3, nrow = 0))
    names(nodes_graph) <- c("name", "in_or_out", "value")
  #  indat2
      for(i in 1:nrow(indat2)){
        # i <- 1
        #i
        indat2[i,]
        name <- indat2[i,names_col]
        desc <- indat2[i,desc_col]
  
        #if(nchar(name) > 140) print("that's a long name. consider shortening this")
        #if(nchar(desc) > nchar_to_snip) desc <- paste(substr(desc, 1, nchar_to_snip), "[...]")
        name2paste <- paste('', name, '', sep = "")
        inputs <- data.frame(in_or_out = "input", value = unlist(lapply(strsplit(indat2[i,in_col], ","), str_trim)))
        
  #      inputs
        
        outputs <- data.frame(in_or_out = "output", value = unlist(lapply(strsplit(indat2[i,out_col], ","), str_trim)))
  #      outputs
        inout <- rbind(inputs, outputs)      
  
  strng <- data.frame(name = name2paste,                    
                      inout = inout
                   )
  strng
        names(strng) <- c("name", "in_or_out", "value")
  #nodes_graph  <- ""      
        nodes_graph <- rbind(nodes_graph, strng)
  #}
  }
  #nodes_graph
  return(nodes_graph)
  }
  
#+end_src

*** COMMENT test-df-input-code3
#+name:test-df-input
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:test-df-input####
  library(devtools)
  load_all()
  library(stringr)
  ## filesList <- read.csv(textConnection('
  ## CLUSTER ,  FILE    , INPUTS                    , OUTPUTS                                , DESCRIPTION                      
  ## A  ,  siteIDs      , "GPS, helicopter"          , "spatial, site doco"                 , latitude and longitude of sites  
  ## A  ,  weather      , BoM                       , exposures                              , weather data from BoM            
  ## B  ,  trapped      , spatial                   , trapped_no                             , counts of species caught in trap 
  ## B  ,  biomass      , spatial                   , biomass_g                              ,                                  
  ## B  ,  correlations , "exposures,trapped_no,biomass_g" , report1                         , A study we published             
  ## C  ,  paper1       , report1                   , "open access repository, data package" ,                                  
  ## D  ,  biomass revision, new estimates          , biomass_g                              , this came late
  ## '), stringsAsFactors = F, strip.white = T)
  #write.csv(filesList, "fileTransformations.csv", row.names = F)
  filesList <- read.csv("fileTransformations.csv", stringsAsFactors = F, strip.white = T)
  
  str(filesList)
  # filesList
  
  nodes_graphy <- newnode_csv(
    indat = filesList
    ,
    names_col = "STEP"
    ,
    in_col = "INPUTS"
    ,
    out_col = "OUTPUTS"
    ,
    desc_col = "DESCRIPTION"
    ,
    clusters_col = "CLUSTER"
    ,
    todo_col = NA #"STATUS"
    ,
    nchar_to_snip = 40
    )
  
  #sink("fileTransformations.Rmd")
  cat(nodes_graphy)
  #sink()
  #system("dot -Tpng fileTransformations.dot -o fileTransformations.png")
  
#+end_src

    
** 2013-11-25-setting-up-a-workflow-script
#+name:project-template-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-11-25-setting-up-a-workflow-with-code-chunks.md :exports none :eval no :padline no
  ---
  name: 2013-11-25-setting-up-a-workflow-with-code-chunks
  layout: post
  title: Setting Up A Workflow Script With Code Chunks
  date: 2013-11-25
  categories:
  - research methods
  ---
  
  This post describes some ideas and techniques I use to set up a "workflow script".  I use this term to refer to the structured combination of code, data and narrative that make an executable Reproducible Research Report (RRR).
  
  A lot of these ideas are inpsired by  a great paper by Kieran Healy called  "Choosing Your Workflow Applications" available at [https://github.com/kjhealy/workflow-paper](https://github.com/kjhealy/workflow-paper) to accompany [his Emacs Starter Kit](http://kieranhealyo.org/resources/emacs-starter-kit.html). My shortened version of his main points are:
  
  - 1 use a good code editor
  - 2 analyse data with scripts
  - 3 store your work simply and document it properly
  - 4 use a version control system
  - 5 Automate back ups 
  - 6 Avoid distracting gadgets
  
  #### Here's my current approach in each of these categories
  - 1 use Emacs with Orgmode (and kjhealy's drop-in set of useful defaults)
  - 2 Scripts that utilise the literate programming technique of mixing Code Chunks in with descriptive prose
  - 3 John Myles White's ProjectTemplate R Package and Josh Riech's LCFD paradigm 
  - 4 git and GitHub for version control
  
  5 Automated Backups and 6 Avoiding Gadgets are still somethings I find challenging
  
  #### 1 Use a good code editor
  I like using Emacs with Orgmode.
  
  - I have previously tried a variety of code editors from Tinn-r, NppToR, Rstudio and Eclipse.  
  - Emacs with Orgmode suits me the most because it has a great number of features especially the linkage with LaTeX or HTML export
  - A key reference to look at for reasons why Emacs is so good for scientific work is Eric Schulte et al ["A Multi-Language Computing Environment for Literate Programming"](www.jstatsoft.org/v46/i03â€Ž) 
  - Here is a [link to a great orgmode description](http://doc.norang.ca/org-mode.html)
  - (this guy spends a lot of time on tweaking his set up)
  
  #### 2 Analyse data with Scripts (stitch together code chunks)
  I use Scripts but prefer to think of them as stitched together Code Chunks with prose into Compendia.
  
  - Compendia are documents that weave together Code and Prose into an executable report
  - The underlying philosophy is called Reproducible Research Reports
  - A very useful tool is a keyboard shortcut to quickly create a chunk for code
  - so you can be writing parts of the report like this: "Blah Blah Blah as shown in Figure X and Table Y"
  - then just hit the correct keys and WHAMM-O there is a new chunk ready for the code that creates Figure X and Table Y to be written.
  - Here is how I use Emacs to achieve this (the other editors I mentioned above also have the abiltiy to do this too).  The IPython Notebook does this stuff too but calls chunks "cells" for some reason.
  
  #### Emacs Code: Put this into the ~/.emacs.d/init.el file
      (define-skeleton chunk-skeleton
        "Info for a code chunk."
        "Title: "
        "*** " str "-code\n"
        "#+name:" str "\n"
        "#+begin_src R :session *R* :tangle src/" str ".r :exports reports :eval no\n"
        "#### name:" str " ####\n"
        "\n"
        "#+end_src\n"
      )
      (global-set-key [?\C-x ?\C-\\] 'chunk-skeleton)
  
  #### Using the Emacs Shortcut
  - now whenever you type Control-x control-\ a new code chunk will appear
  - you'll be typing "blah blah blah" and think I need a figure or table, just hit it.
  - move into the empty section and add some code
  - you can hit C-c ' to enter a org-babel code execution session that will be able to send these line by line to an R session
  - or within the main org buffer if your eval flag is set to yes then you can run the entire chunk (and return tabular output to the doc) using C-c C-c
  - To export the code chunks and create the modular code scripts without the narrative prose use C-c C-v t
  - this is called "tangling" and the chunks will be written out to the file specified in the chunk header ":tangle" flag
  
  #### Compiling the resulting Compendium
  - Emacs uses LaTeX or HTML to produce the Report
  - I find both of these outputs very pleasing
  - to compile to TEX use C-c C-e d
  - for HTML use C-c C-e h (FOR CODE HIGHLIGHTING INSTALL htmlize.el)
  - these commands will also evaluate all the chunks where ":eval" = yes to load the data and calculate the results fresh. 
  - AWESOME!
      
  #### 3 Store your work simply and document it properly
  - I use the [ProjectTemplate R package](http://www.johnmyleswhite.com/notebook/2010/08/26/projecttemplate/) to organise my code sections into modules
  - These modules are organised into the Reichian LCFD paradigm described first [on StackOverflow here](http://stackoverflow.com/a/1434424), and encoded into [the makeProject R package](http://cran.r-project.org/web/packages/makeProject/makeProject.pdf)
  - documentation is within the main orgmode script
  - data documentation is a whole other universe that I will deal with in a separate post
  
  #### 4 use a version control system using git and github
      # once you have the project via R
      R
      require(ProjectTemplate)
      create.project("AwesomeProject", minimal = T)
      q()
      # use the shell to start a git repo
      cd AwesomeProject
      git init
      # and commit the TODO
      git add TODO
      git commit -m "first commit"
      # tada!
  
  - Emacs can now be used to manage the git repo using the C-x g command
  - Rstudio has a really nice GUI for doing this inside it;s Project management interface too.
  
  #### Using Github or another Git Server
  - You can easily set up a Github repo for this now but it will be public
  - Alternatative is to set up your own private Git server.  I followed [these instructions to Running a Simple Git Server Using SSH](http://blog.goosoftware.co.uk/2012/02/07/quick-git-server/)
  - Either way once you have set up your remote git repo you need to set the remote tracking
  
  #### Git Code:
      cd /path/to/local/git/repo
      git remote add origin git@github-or-other-server:myname/myproject.git
      git push origin master
  
  #### 5 Automate back ups AND 6 Avoid distracting gadgets
  - OMG backups stress me out
  - ideally I would follow [this advice because "when it comes to losing your data the universe tends toward maximum irony. Don't push it."](http://www.jwz.org/blog/2007/09/psa-backups/)
  - But I don;t fully comply
  - Instead I generally use Dropbox for  basic project management admin stuff
  - I use github for code projects I am happy to share, I also pay for 10 private repos 
  - I Set up a git server at my workplace for extra projects but this is on a test server that is not backed up, and I am not really happy about this
  - In terms of Distracting Gadgets, I think that with the current tempo of new innovations related to new software tools for this type of work I should keep trying new things but I have pretty much settled into a comfortable zone with the gadgets I described here. 
  
  #### Conclusions
  - This is how I've worked for a couple of years
  - I find it very enjoyable, mostly productive but prone to the distractions of "distractions by gadgets"
  - The main thing I want to point out is the usage of Code Chunks in RRR scripts.
  - These things are awesome.
      
#+end_src

** 2013-12-01-graphviz-automagic-flowcharts
*** workflow_steps-code
#+begin_src R :session *R* :tangle no :exports reports :eval no :padline no
# ~/tools/transformations/workflow_steps.txt
Transformation
	description
		the thing 
	inputs
		the thing before
		another thing
	output
		the next thing
	notes
		the thing is this other thing	this is a really long description 
		blah blah asdfasdfasdfasdfasdfa 

Transformation
	description
		yet another thing
	inputs
		the next thing
	output
		a final thing
	notes
		this is a note

#+end_src

*** post
#+name:graphviz-automagic-flowcharts-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-12-01-graphviz-automagic-flowcharts.md :exports none :eval no :padline no
  ---
  name: 2013-12-01-graphviz-automagic-flowcharts
  layout: post
  title: graphviz-automagic-flowcharts
  date: 2013-12-01
  categories:
  - research methods 
  ---
  
  - Back in 2009 Joseph Guillaume worked with me on a complicated workflow
  - He came up with this python script to help keep track of the steps
  - The simple text file is a list of transformations, inputs and output
  - It is converted to the right format and graphviz creates a html page with pop-outs
  
  #### Simple text list
      Transformation
              description
                      the thing 
              inputs
                      the thing before
                      another thing
              output
                      the next thing
              notes
                      the thing is this other thing   this is a really long description 
                      blah blah asdfasdfasdfasdfasdfa 
       
      Transformation
              description
                      yet another thing
              inputs
                      the next thing
              output
                      a final thing
              notes
                      this is a note
  
  <p></p>
  
  - keep this in your work directory and update it whenever you add a step to the workflow
  - the list can be as big as you like (hundreds of steps), and entered in any order, the inputs/output relationships determine how the graph looks at the end
  - to run the script just do the one line

  #### Python code: run
      python transformations.py workflow_steps.txt index
  
  <p></p>
  - open the html page and click on a square box to bring up the pop-out
  - short text is shown, long text is replaced by an ellipse and only shown in pop-out
  
  #### Conclusions  
  - I've popped the script up as a [Github repo](https://github.com/ivanhanigan/transformations)
  - The example is in the [gh-pages branch](http://ivanhanigan.github.io/transformations/)
  
#+end_src

** 2013-12-24-a-few-best-practices-for-statistical-programming
#+name: a-few-best-practices-for-statistical-programming-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-12-24-a-few-best-practices-for-statistical-programming.md :exports none :eval no :padline no
---
name: 2013-12-24-a-few-best-practices-for-statistical-programming
layout: post
title: a-few-best-practices-for-statistical-programming
date: 2013-12-24
categories:
- research methods
---

- John Myles White invented the ProjectTemplate R Package
- This is a great application that helps streamline the process of creating a data analysis project
- Recently John posted about some tips for [best practices for statistical programming](http://www.johnmyleswhite.com/notebook/2013/01/24/writing-better-statistical-programs-in-r/)


#### Best Practices for Statistical Programming

- Write Out a Directed Acyclic Graph (DAG)
- Vectorize Your Operations
- Profile your code and understand where it spends its time
- Generate Data and Fit Models
- Correctness: always ensure that code infers  parameters of models given simulated data with known parameters.


#### Additional suggestions

- Unit Testing (use testthat)
- Create modular code with discrete chunks
- Write functions as much as possible, put these into a personal 'misc' package
   
#+end_src

* XML
** COMMENT review-xml-code
#+name:review-xml
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:review-xml####
  require(XML)
  # first read one without a public access
  lnk <- "http://www.ltern.org.au/knb/metacat/ltern2.42.44/xml"
  xmlfile=xmlParse(lnk)
  xmltop = xmlRoot(xmlfile) 
  length(names(xmltop[[1]]))
  
  names(xmltop[[2]])
  names(xmltop[[2]]['project']['project'])
  names(xmltop[[2]]['project'][['project']]) == 'personnel'
  names(xmltop[[2]]['project'][[1]][[4]])
  xmltop[[2]]['project'][['project']][[4]]
  xmltop[[2]]['project'][['project']][[4]][['individualName']]
  xmltop[[2]]['project'][['project']][[4]][['role']]
  
  # the first
  xmltop[[2]]['project'][['project']][['personnel']][['role']]
  
  xmltop[[2]]['contact'][['contact']][['organizationName']]
  
  
  # lnk <- "http://dev.ltern.org.au/knb/metacat/datalibrarian.67/xml"
   lnk <- "http://dev.ltern.org.au/knb/metacat/ltern2.42/xml"
  xmlfile=xmlParse(lnk)
  xpath = xmlRoot(xmlfile)
  
  names(xpath)
  xpath[[1]]
  names(xpath[[2]])
  
  #xpath_dataset <- xpath[['dataset']]
  xmlValue(xpath[['dataset']][['title']])
  xpath[['dataset']]['creator']
  lapply(
    xpath[['dataset']]['creator']
    , xmlValue)
  
  # http://stackoverflow.com/a/22626191
  flatten_xml <- function(x) {
    if (length(xmlChildren(x)) == 0) structure(list(xmlValue(x)), .Names = xmlName(xmlParent(x)))
    else Reduce(append, lapply(xmlChildren(x), flatten_xml))
  }
  
  dfs <- lapply(getNodeSet(xpath,"//creator"), function(x) data.frame(flatten_xml(x)))
  allnames <- unique(c(lapply(dfs, colnames), recursive = TRUE))
  df <- do.call(rbind, lapply(dfs, function(df) { df[, setdiff(allnames,colnames(df))] <- NA; df }))
  head(df)
  
  
  
  xpath[['dataset']][['project']]
  names(xpath[['dataset']][['project']])
  xpath[['dataset']][['project']][['title']]
  lapply(xpath[['dataset']][['project']]['personnel'], xmlValue)
  dfs <- lapply(getNodeSet(xpath,"//personnel"), function(x) data.frame(flatten_xml(x)))
  allnames <- unique(c(lapply(dfs, colnames), recursive = TRUE))
  df <- do.call(rbind, lapply(dfs, function(df) { df[, setdiff(allnames,colnames(df))] <- NA; df }))
  head(df)
  
  
  xmlValue(xpath[['dataset']][['project']][['funding']])
  
#+end_src

* Reproducible Research Reports
** headers
#+begin_src R :session *R* :tangle main.Rmd :exports none :eval no :padline no
  ---
  title: "orgmode tests"
  author: Ivan C. Hanigan
  output:
    html_document:
      toc: true
      theme: united
      number_sections: yes    
    pdf_document:
      toc: true
      toc_depth: 3
      highlight: zenburn
      keep_tex: true
      number_sections: no        
  documentclass: article
  classoption: a4paper
  csl: mee.csl
  bibliography: references.bib
  ---
  
  ```{r echo = F, eval=F, results="hide"}
#+end_src
** run-able R
#+begin_src R :session *shell* :tangle main.Rmd :exports none :eval no :padline no

  library(rmarkdown)
  library(knitr)
  library(knitcitations)
  library(bibtex)
  cleanbib()
  cite_options(citation_format = "pandoc", check.entries=FALSE) 
  rmarkdown::render("main.Rmd", "html_document")

#+end_src
** perhaps tangle out pure R
#+begin_src R :session *shell* :tangle main.Rmd :exports none :eval no :padline no
  ```
  ```{r echo = F, eval=F, results="hide"}  
  # to tangle chunks even when eval = F use this (with eval=F)
  library(knitr)
  knit_hooks$set(purl = function(before, options) {
    if (before) return()
    input  = current_input()  # filename of input document
    output = paste(tools::file_path_sans_ext(input), 'R', sep = '.')
    if (knitr:::isFALSE(knitr:::.knitEnv$tangle.start)) {
      assign('tangle.start', TRUE, knitr:::.knitEnv)
      unlink(output)
    }
    cat(options$code, file = output, sep = '\n', append = TRUE)
  })
  
  ```  
#+end_src
** bib
#+begin_src R :session *shell* :tangle main.Rmd :exports none :eval no :padline no
  
  ```{r, echo = F, results = 'hide'}
  # load
  if(!exists("bib")){
  bib <- read.bibtex("~/references/library.bib")
  }
  ```
  
#+end_src

** Introduction
*** intro
#+name:intro
#+begin_src R :session *shell* :tangle main.Rmd :exports none :eval no :padline yes 
# Intro
This is the thing `r citep(     bib[["Hsiang2011"]])`.
This is another thing `r citep( bib[["Hanigan2012"]])`.

# section

#+end_src

** COMMENT bib-code
#+name:bib
#+begin_src R :session *shell* :tangle main.Rmd :exports none :eval no

  **References**

  ```{r, echo=FALSE, message=FALSE, eval = T}
  write.bibtex(file="references.bib")
  ```
  
#+end_src

** footnotes in markdown
http://nancym.tumblr.com/post/59594358553/links-footnotes-and-abbreviations-in-markdown
This is needed provide the trust that a data
  analysis was appropriately conducted and avoided errors of execution,
  or methodogical design [^comerrmeth].

[^comermeth]: the footnote
* Blogging
*** COMMENT blog-code
#+name:blog
#+begin_src sh :session *shell* :tangle no :exports none :eval yes
#### name:blog ####
cd ~/projects/ivanhanigan.github.com.raw
jekyll b
cp -r ~/projects/ivanhanigan.github.com.raw/_site/* ~/projects/ivanhanigan.github.com
cd ~/projects/ivanhanigan.github.com
jekyll serve
#cd ~/
#./.bash_profile
#bb

#+end_src

#+RESULTS: blog
|                                                              |                                                              |                     |                                                                    |            |           |         |                         |      |     |             |
| ivan_hanigan@jadehawk:~/projects/ivanhanigan.github.com.raw$ | Configuration                                                | file:               | /home/ivan_hanigan/projects/ivanhanigan.github.com.raw/_config.yml |            |           |         |                         |      |     |             |
| Source:                                                      | /home/ivan_hanigan/projects/ivanhanigan.github.com.raw       |                     |                                                                    |            |           |         |                         |      |     |             |
| Destination:                                                 | /home/ivan_hanigan/projects/ivanhanigan.github.com.raw/_site |                     |                                                                    |            |           |         |                         |      |     |             |
| Generating...                                                |                                                              |                     |                                                                    |            |           |         |                         |      |     |             |
| [33m                                                       | Build                                                        | Warning:            | Layout                                                             | 'nil'      | requested | in      | atom.xml                | does | not | exist.[0m |
| [33m                                                       | Build                                                        | Warning:            | Layout                                                             | 'nil'      | requested | in      | entries.xml             | does | not | exist.[0m |
| [33m                                                       | Build                                                        | Warning:            | Layout                                                             | 'rss-feed' | requested | in      | feed.xml                | does | not | exist.[0m |
| [33m                                                       | Build                                                        | Warning:            | Layout                                                             | 'nil'      | requested | in      | feed/index.xml          | does | not | exist.[0m |
| done.                                                        |                                                              |                     |                                                                    |            |           |         |                         |      |     |             |
| Auto-regeneration:                                           | disabled.                                                    | Use                 | --watch                                                            | to         | enable.   |         |                         |      |     |             |
| ivan_hanigan@jadehawk:~/projects/ivanhanigan.github.com.raw$ | ivan_hanigan@jadehawk:~/projects/ivanhanigan.github.com$     | [33mConfiguration | file:                                                              | none[0m  |           |         |                         |      |     |             |
| Source:                                                      | /home/ivan_hanigan/projects/ivanhanigan.github.com           |                     |                                                                    |            |           |         |                         |      |     |             |
| Destination:                                                 | /home/ivan_hanigan/projects/ivanhanigan.github.com/_site     |                     |                                                                    |            |           |         |                         |      |     |             |
| Generating...                                                |                                                              |                     |                                                                    |            |           |         |                         |      |     |             |
| [33m                                                       | Build                                                        | Warning:            | Layout                                                             | 'default'  | requested | in      | data-doco.md            | does | not | exist.[0m |
| [33m                                                       | Build                                                        | Warning:            | Layout                                                             | 'default'  | requested | in      | energymark.md           | does | not | exist.[0m |
| [33m                                                       | Build                                                        | Warning:            | Layout                                                             | 'default'  | requested | in      | pumilio-bushfm-index.md | does | not | exist.[0m |
| done.                                                        |                                                              |                     |                                                                    |            |           |         |                         |      |     |             |
| Auto-regeneration:                                           | enabled                                                      | for                 | '/home/ivan_hanigan/projects/ivanhanigan.github.com'               |            |           |         |                         |      |     |             |
| [33mConfiguration                                          | file:                                                        | none[0m           |                                                                    |            |           |         |                         |      |     |             |
| jekyll                                                       | 2.5.2                                                        |                     |                                                                    | Error:     | Address   | already | in                      | use  | 0   | bind(2)     |

* bibliometrics
*** COMMENT gscraper-func-code
#+name:gscraper-func
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:gscraper-func ####
  # File-Name: GScholarScraper_3.1.R
  # Date: 2012-08-22
  # Author: Kay Cichini
  # Email: kay.cichini@gmail.com
  # Purpose: Scrape Google Scholar search result
  # Packages used: XML
  # Licence: CC BY-SA-NC
  #
  # Arguments:
  # (1) input:
  # A search string as used in Google Scholar search dialog
  #
  # (2) write:
  # Logical, should a table be writen to user default directory?
  # if TRUE ("T") a CSV-file with hyperlinks to the publications will be created.
  #
  # Difference to version 3:
  # (3) added "since" argument - define year since when publications should be returned..
  # defaults to 1900..
  #
  # (4) added "citation" argument - logical, if "0" citations are included
  # defaults to "1" and no citations will be included..
  # added field "YEAR" to output 
  #
  # Caveat: if a submitted search string gives more than 1000 hits there seem
  # to be some problems (I guess I'm being stopped by Google for roboting the site..)
  #
  # And, there is an issue with this error message:
  # > Error in htmlParse(URL): 
  # > error in creating parser for http://scholar.google.com/scholar?q
  # I haven't figured out his one yet.. most likely also a Google blocking mechanism..
  # Reconnecting / new IP-address helps..
  
  
  GScholar_Scraper <- function(input, since = 1900, write = F, citation = 1) {
  
      require(XML)
  
      # putting together the search-URL:
      URL <- paste("http://scholar.google.com/scholar?q=", input, "&as_sdt=1,5&as_vis=", 
                   citation, "&as_ylo=", since, sep = "")
      cat("\nThe URL used is: ", "\n----\n", paste("* ", "http://scholar.google.com/scholar?q=", input, "&as_sdt=1,5&as_vis=", 
                   citation, "&as_ylo=", since, " *", sep = ""))
      
      # get content and parse it:
      doc <- htmlParse(URL)
      
      # number of hits:
      h1 <- xpathSApply(doc, "//div[@id='gs_ab_md']", xmlValue)
      h2 <- strsplit(h1, " ")[[1]][2] 
      num <- as.integer(sub("[[:punct:]]", "", h2))
      cat("\n\nNumber of hits: ", num, "\n----\n", "If this number is far from the returned results\nsomething might have gone wrong..\n\n", sep = "")
      
      # If there are no results, stop and throw an error message:
      if (num == 0 | is.na(num)) {
          stop("\n\n...There is no result for the submitted search string!")
      }
      
      pages.max <- ceiling(num/100)
      
      # 'start' as used in URL:
      start <- 100 * 1:pages.max - 100
      
      # Collect URLs as list:
      URLs <- paste("http://scholar.google.com/scholar?start=", start, "&q=", input, 
                    "&num=100&as_sdt=1,5&as_vis=", citation, "&as_ylo=", since, sep = "")
      
      scraper_internal <- function(x) {
          
          doc <- htmlParse(x, encoding="UTF-8")
          
          # titles:
          tit <- xpathSApply(doc, "//h3[@class='gs_rt']", xmlValue)
          
          # publication:
          pub <- xpathSApply(doc, "//div[@class='gs_a']", xmlValue)
          
          # links:
          lin <- xpathSApply(doc, "//h3[@class='gs_rt']/a", xmlAttrs)
          
          # summaries are truncated, and thus wont be used..  
          # abst <- xpathSApply(doc, '//div[@class='gs_rs']', xmlValue)
          # ..to be extended for individual needs
          options(warn=(-1))
          dat <- data.frame(TITLES = tit, PUBLICATION = pub, 
                            YEAR = as.integer(gsub(".*\\s(\\d{4})\\s.*", "\\1", pub)),
                            LINKS = lin)
          options(warn=0)
          return(dat)
      }
  
      result <- do.call("rbind", lapply(URLs, scraper_internal))
      if (write == T) {
        result$LINKS <- paste("=Hyperlink(","\"", result$LINKS, "\"", ")", sep = "")
        write.table(result, "GScholar_Output.CSV", sep = ";", 
                    row.names = F, quote = F)
        shell.exec("GScholar_Output.CSV") 
        } else {
        return(result)
      }
  }
#+end_src

#+RESULTS: gscraper-func

*** COMMENT gscraper-code
#+name:gscraper
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:gscraper ####
  
  # EXAMPLES:
  # 1:
  input <- "intitle:metapopulation"
  df <- GScholar_Scraper(input, since = 1980, citation = 1)
  nrow(df)
  hist(df$YEAR, xlab = "Year", 
       main = "Frequency of Publications with\n\"METAPOPULATION\" in Title")
  # 2:
  input <- "Suicide and Drought"
  rm(df)
  df  <-  GScholar_Scraper(input, since = 2006, citation = 1)
  nrow(df)
  str(df)
  df[[1]][1]
  # 3:
  input <- "allintitle:ziggy stardust"
  GScholar_Scraper(input, write = T)
  # 4: ERROR with message:
  input <- "allintitle:crazyshit"
  GScholar_Scraper(input)
  # 5: CAVEAT, Google blocks automated requests at about the 1000th hit:
  input <- "metapopulation"
  df <- GScholar_Scraper(input, since = 1980)
  nrow(df)
  
  # 6: this also leads to this error for example no. 1,
  # because when including citations (.., citation = 0) 1000 hits are exceeded, 
  # Google blocks and dataframe generation is not working..
  input <- "intitle:metapopulation"
  df <- GScholar_Scraper(input, since = 1980, citation = 0)
  
  
#+end_src
* Writing
** One-pager
focus
down *(mgmnt)
out (clients)

whoami, establish credentials
what
why
Actions
when
what I need
implications

1 issue (one line)
2 Analysis (history, backghround, one paragraph) refer to other docs or conversations)
3 options or dimensions (list of all possible options. alternateives, one paragraph per option
4 suggrestion/request one sentence or short para
5 timing one sentence

name (date)

dots are great
** SI emails
The SI Rules

1    Try to send no more than one email a day. 
2    Emails should be 3 sentences or less. Better if you can get the whole email in the subject line. 
3    If you need information, ask yes or no questions whenever possible. Never ask a question that requires a full sentence response.
4    When something is time sensitive, state the action you will take if you donâ€™t get a response by a time you specify. 
5    Be as specific as you can while conforming to the length requirements. 
6    Bonus: include obvious keywords people can use to search for your email. 

eg
Example 1

Subject: Is my response to reviewer 2 ok with you?

Body: Iâ€™ve attached the paper/responses to referees.

Example 2

Subject: Can you send my letter of recommendation to john.doe@someplace.com?

Body:

Keywords = recommendation, Jeff, John Doe.

Example 3

Subject: I revised the draft to include your suggestions about simulations and language

Revisions attached. Let me know if you have any problems, otherwise Iâ€™ll submit Monday at 2pm. 
